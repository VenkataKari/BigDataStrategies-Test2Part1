{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "93ba3a3d-dfd2-47d1-db5c-7e3f591eea48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=4a6be6e81f426700c59538be75aaad4a9309b3fbe82b7408b08e3d73f81ba5a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "f5bf4f1d-ba79-42de-9c92-d992487dc9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "620b9dac-33a1-49c1-e326-cb1435cd8321"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "f07e5881-f60a-4d28-8b7f-8e152d2af507"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "6eb1277e-6f66-44a6-9184-84d299099024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 800 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "982c7cd0-ee03-453a-d98a-4d3ef1903335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.980199\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.970397\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.950693\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951186\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.931674\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.932358\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.933034\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.933704\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.934367\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.925023\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.925773\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.926515\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.927250\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.927977\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.928698\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.929411\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.930116\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.930815\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.921507\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.912292\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.913169\n",
            "resetting env. episode 26.000000, reward total was -18.000000. running mean: -20.884037\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.885197\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.886345\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.867482\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.868807\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.870119\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.861418\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.862803\n",
            "resetting env. episode 34.000000, reward total was -19.000000. running mean: -20.844175\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.835734\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.837376\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.829003\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.830712\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.832405\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.814081\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.815940\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.807781\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.799703\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.801706\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.803689\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.805652\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.807596\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.799520\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.801525\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.803509\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.805474\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.807420\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.799345\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.801352\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -20.773338\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -20.755605\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.758049\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.760468\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.762864\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.765235\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.767583\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.759907\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.762308\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.764685\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.767038\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.759368\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.761774\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.754156\n",
            "resetting env. episode 69.000000, reward total was -18.000000. running mean: -20.726615\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.719348\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.722155\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.724933\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.717684\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.710507\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.713402\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.716268\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.719105\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.711914\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.714795\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.717647\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.710471\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.703366\n",
            "resetting env. episode 83.000000, reward total was -18.000000. running mean: -20.676332\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.679569\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.682773\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.675946\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.679186\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.672394\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.675670\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.668914\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.672225\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.665502\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.658847\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.652259\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.635736\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.629379\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.623085\n",
            "resetting env. episode 98.000000, reward total was -18.000000. running mean: -20.596854\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.590886\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.574977\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.579227\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.573435\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.577700\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.581923\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.576104\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.560343\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.564740\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.549092\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.533601\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.518265\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.513083\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.507952\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.512872\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.507744\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.512666\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.507540\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.512464\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.507340\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.492266\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.487344\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.492470\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.497545\n",
            "resetting env. episode 123.000000, reward total was -18.000000. running mean: -20.472570\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.467844\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.453166\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.448634\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.454148\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.439606\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.435210\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.430858\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.436550\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.432184\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.417862\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.423684\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.419447\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.415252\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.421100\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.416889\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.422720\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.428493\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.414208\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.420066\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.425865\n",
            "resetting env. episode 144.000000, reward total was -18.000000. running mean: -20.401606\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.407590\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.413514\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.419379\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.405185\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.411134\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.417022\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.412852\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.418724\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.404536\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.410491\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.406386\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.402322\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.398299\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.404316\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.400273\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.406270\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.402207\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.408185\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.414103\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.409962\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.415863\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.411704\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.407587\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.403511\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.399476\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.385481\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.381627\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.387810\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.393932\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.399993\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.405993\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.401933\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.407914\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.413835\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.419696\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.425499\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.421244\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.427032\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.432761\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.438434\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.424050\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.429809\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.435511\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.431156\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.426844\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.432576\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.438250\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.443868\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.439429\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.445035\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.440584\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.446178\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.451717\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.447199\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.442727\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.438300\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.433917\n",
            "resetting env. episode 202.000000, reward total was -18.000000. running mean: -20.409578\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.405482\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.411427\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.417313\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.413140\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.419009\n",
            "resetting env. episode 208.000000, reward total was -17.000000. running mean: -20.384819\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.390970\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.397061\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.393090\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.399159\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.395168\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.401216\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.407204\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.403132\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.409100\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.415009\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.420859\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.416651\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.422484\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.408259\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.404177\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.400135\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.386134\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.392272\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.398350\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.404366\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.400322\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.406319\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.412256\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.398133\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.394152\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.400211\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.386208\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.392346\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.388423\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.394539\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.400593\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.396587\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.402621\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.398595\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.394609\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.390663\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.386757\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.382889\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.369060\n",
            "resetting env. episode 248.000000, reward total was -16.000000. running mean: -20.325370\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.332116\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.338795\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.345407\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.341953\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.338533\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.335148\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.331796\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.328478\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.335194\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.331842\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.338523\n",
            "resetting env. episode 260.000000, reward total was -18.000000. running mean: -20.315138\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.321987\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.318767\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.305579\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.312523\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.309398\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.306304\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.313241\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.300109\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.307108\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.304036\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.310996\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.307886\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.314807\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.311659\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.308543\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.305457\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.302403\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.309379\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.306285\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.303222\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.310190\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.317088\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.313917\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.310778\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.317670\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.324493\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.321248\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.328036\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.334756\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.331408\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.328094\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.334813\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.331465\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.338150\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.334769\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.321421\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.318207\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.315025\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.311874\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.318756\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.325568\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.332312\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.338989\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.345599\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.342143\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.348722\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.355235\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.351682\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.338166\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.344784\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.351336\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.337823\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.344445\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.341000\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.347590\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.354114\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.340573\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.337167\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.333796\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.340458\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.347053\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.343583\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.350147\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.356645\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.363079\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.369448\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.375754\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.371996\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.378276\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.364493\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.360848\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.347240\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.353768\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.360230\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.356628\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.363061\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.369431\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.365736\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.352079\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.358558\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.364973\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.371323\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.367610\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.363934\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.370294\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.376591\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.362825\n",
            "resetting env. episode 348.000000, reward total was -18.000000. running mean: -20.339197\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.345805\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.352347\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.338824\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.345435\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.341981\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.348561\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.355076\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.361525\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.367910\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.364231\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.370588\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.376882\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.363113\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.359482\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.345888\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.352429\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.338904\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.345515\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.352060\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.358540\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.364954\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.361305\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.367692\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.374015\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.380275\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.386472\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.392607\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.398681\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.404694\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.410647\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.416541\n",
            "resetting env. episode 380.000000, reward total was -17.000000. running mean: -20.382375\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.378552\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.374766\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.381018\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.387208\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.393336\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.379403\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.385609\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.391753\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.377835\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.384057\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.390216\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.396314\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.382351\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.388527\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.394642\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.390696\n",
            "resetting env. episode 397.000000, reward total was -18.000000. running mean: -20.366789\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.363121\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.369490\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.355795\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.362237\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.368614\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.374928\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.381179\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.387367\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.383494\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.389659\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.375762\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.362004\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.358384\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.364801\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.361153\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.357541\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.353966\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.350426\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.346922\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.333452\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.330118\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.336817\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.333449\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.340114\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.346713\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.353246\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.359713\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.366116\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.372455\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.378731\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.384943\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.381094\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.387283\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.373410\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.379676\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.385879\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.392020\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.388100\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.394219\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.400277\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.396274\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.402311\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.408288\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.414205\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.410063\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.415963\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.421803\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.417585\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.423409\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.429175\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.434883\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.440535\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.446129\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.451668\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.447151\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.452680\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.458153\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.453571\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.459036\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.464445\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.449801\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.445303\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.440850\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.436441\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.432077\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.427756\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.423479\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.409244\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.405151\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.391100\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.397189\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.393217\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.399285\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.405292\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.411239\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.407127\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.413055\n",
            "resetting env. episode 475.000000, reward total was -18.000000. running mean: -20.388925\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.395036\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.391085\n",
            "resetting env. episode 478.000000, reward total was -19.000000. running mean: -20.377174\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.373403\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.369669\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.375972\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.362212\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.358590\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.365004\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.361354\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.367741\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.374063\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.380323\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.386519\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.372654\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.368928\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.355238\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.361686\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.368069\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.374388\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.370645\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.376938\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.383169\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.389337\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.385444\n",
            "CPU times: user 1h 3min 13s, sys: 12min 25s, total: 1h 15min 39s\n",
            "Wall time: 38min 50s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "d530c472-e4b5-4cff-a7ea-93de0110dfc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.980297\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.980494\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.980689\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.980882\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.971073\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.971363\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.971649\n",
            "resetting env. episode 14.000000, reward total was -18.000000. running mean: -20.941933\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.942513\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.933088\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.933757\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.934420\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -20.915075\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.905925\n",
            "resetting env. episode 21.000000, reward total was -17.000000. running mean: -20.866865\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -20.848197\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.849715\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.851218\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.842705\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.844278\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.835836\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.827477\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.829203\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.830910\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.832601\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.824275\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.806033\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.797972\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.779993\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.772193\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.774471\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.776726\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.758959\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.761369\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.753755\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.746218\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.748756\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.751268\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.753755\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.746218\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.748756\n",
            "resetting env. episode 48.000000, reward total was -18.000000. running mean: -20.721268\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.714056\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.716915\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.719746\n",
            "resetting env. episode 52.000000, reward total was -18.000000. running mean: -20.692548\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.685623\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.688767\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.681879\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.685060\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.688210\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.691327\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.674414\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.667670\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.660993\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.664383\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.667740\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.671062\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.674352\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.667608\n",
            "resetting env. episode 67.000000, reward total was -18.000000. running mean: -20.640932\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.644523\n",
            "resetting env. episode 69.000000, reward total was -18.000000. running mean: -20.618077\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.611897\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -20.595778\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.599820\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.603822\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.607784\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.611706\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.615589\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.619433\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.623238\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.627006\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.630736\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.634429\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.638084\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.631703\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.625386\n",
            "resetting env. episode 85.000000, reward total was -18.000000. running mean: -20.599133\n",
            "resetting env. episode 86.000000, reward total was -18.000000. running mean: -20.573141\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.577410\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.561636\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.566019\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.570359\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.574656\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.568909\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.563220\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.567588\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.571912\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.566193\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.560531\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.554926\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.549376\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.553882\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.548344\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.542860\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.537432\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.542057\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.546637\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.541170\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.545759\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.540301\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.544898\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.549449\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.553955\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.538415\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.533031\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.537701\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.532324\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.527000\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.521730\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.526513\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.531248\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.535935\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.540576\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -20.525170\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.529919\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.534619\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.539273\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.543881\n",
            "resetting env. episode 127.000000, reward total was -18.000000. running mean: -20.518442\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.523257\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.508025\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.492944\n",
            "resetting env. episode 131.000000, reward total was -19.000000. running mean: -20.478015\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.483235\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.478403\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.473618\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.468882\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.474193\n",
            "resetting env. episode 137.000000, reward total was -18.000000. running mean: -20.449452\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.454957\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.460407\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.465803\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.461145\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.466534\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.471869\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.457150\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.462578\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.457953\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.463373\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.458739\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.464152\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.459510\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.464915\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.470266\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.475564\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.480808\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.486000\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.491140\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.496228\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.501266\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.506253\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.511191\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.506079\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.511018\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.505908\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.490849\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.485940\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.491081\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.486170\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.481309\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.476495\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.481731\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.466913\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.472244\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.477522\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.462746\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.468119\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.453438\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.458903\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.464314\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.469671\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.464974\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.470325\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.455622\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.461065\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.446455\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.451990\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.457470\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.462895\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.468267\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.473584\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.468848\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.474160\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.479418\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.484624\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -20.459778\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.465180\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.470528\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.455823\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.451264\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.456752\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.452184\n",
            "resetting env. episode 201.000000, reward total was -18.000000. running mean: -20.427662\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.423386\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.429152\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.414860\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.400712\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.396705\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.402738\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.408710\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.414623\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.420477\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.416272\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.422109\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.427888\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.423609\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.429373\n",
            "resetting env. episode 216.000000, reward total was -18.000000. running mean: -20.405080\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -20.381029\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.387219\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.393346\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.379413\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.385619\n",
            "resetting env. episode 222.000000, reward total was -18.000000. running mean: -20.361763\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.358145\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.364564\n",
            "resetting env. episode 225.000000, reward total was -18.000000. running mean: -20.340918\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.347509\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.354034\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.350493\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.356988\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.363418\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.369784\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.376086\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.372326\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.368602\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.364916\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.371267\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.377554\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.373779\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -20.360041\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.366441\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.362776\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.359149\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.355557\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.362002\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.368381\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.364698\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -20.341051\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.337640\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.324264\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.331021\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.337711\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.344334\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.350890\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.357382\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.363808\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.370170\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.366468\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.372803\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.379075\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.365285\n",
            "resetting env. episode 261.000000, reward total was -18.000000. running mean: -20.341632\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.348215\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.344733\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.351286\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.357773\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.354195\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.340653\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.327247\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.323974\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.310735\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.317627\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.324451\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.331206\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.337894\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.344515\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.331070\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.337760\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.334382\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.331038\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.317728\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.314551\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.311405\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.318291\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.315108\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.311957\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.308837\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.315749\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.322592\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.329366\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.336072\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.332711\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.329384\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.326090\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.312829\n",
            "resetting env. episode 295.000000, reward total was -17.000000. running mean: -20.279701\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.266904\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.264235\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.271593\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.258877\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.246288\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.253825\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.261287\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.258674\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.266087\n",
            "resetting env. episode 305.000000, reward total was -18.000000. running mean: -20.243426\n",
            "resetting env. episode 306.000000, reward total was -17.000000. running mean: -20.210992\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.198882\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.206893\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.204824\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.212776\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.220648\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.218442\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.216258\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.214095\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.211954\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.219834\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.227636\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.235360\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.243006\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.240576\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.228170\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.235889\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.243530\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.241094\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.248684\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.256197\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.263635\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.250998\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.248488\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.246004\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.243543\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.251108\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.258597\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.266011\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.273351\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.270617\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.277911\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.285132\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.282281\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.289458\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.296563\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.283598\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.290762\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.297854\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.284876\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.292027\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.299107\n",
            "resetting env. episode 348.000000, reward total was -18.000000. running mean: -20.276116\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.273354\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.270621\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.277915\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.285135\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.292284\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.289361\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.286468\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.293603\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.300667\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -20.287660\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.294784\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.291836\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.288917\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.276028\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.283268\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.280435\n",
            "resetting env. episode 365.000000, reward total was -18.000000. running mean: -20.257631\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.255055\n",
            "resetting env. episode 367.000000, reward total was -18.000000. running mean: -20.232504\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.240179\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.237777\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.245400\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.252946\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.240416\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.248012\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.255532\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.262976\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.270347\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.277643\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.274867\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.272118\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.279397\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.286603\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.293737\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.300800\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.297792\n",
            "resetting env. episode 385.000000, reward total was -18.000000. running mean: -20.274814\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.262066\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.269445\n",
            "resetting env. episode 388.000000, reward total was -18.000000. running mean: -20.246750\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.244283\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.251840\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.249322\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.246828\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.254360\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.241817\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.239398\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.247004\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.244534\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.252089\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.249568\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.257072\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.264502\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.271857\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.279138\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.286347\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.283483\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.290648\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.297742\n",
            "resetting env. episode 408.000000, reward total was -18.000000. running mean: -20.274765\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.262017\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.269397\n",
            "resetting env. episode 411.000000, reward total was -18.000000. running mean: -20.246703\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.244236\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.241793\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.229375\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.227082\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.234811\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.222463\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.210238\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.218136\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.215954\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.203795\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.201757\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.209739\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.207642\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.195566\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.193610\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.191674\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.199757\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.207759\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.215682\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.223525\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.231290\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.228977\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.236687\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.224320\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.212077\n",
            "resetting env. episode 437.000000, reward total was -18.000000. running mean: -20.189956\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.188057\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.196176\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.204214\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.202172\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.200151\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.208149\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.206068\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.214007\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.221867\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.219648\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.227452\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.225177\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.232925\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.240596\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.238190\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.225808\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.223550\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.221315\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.209102\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.207011\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.204940\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.202891\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.200862\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.198853\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.196865\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.194896\n",
            "resetting env. episode 464.000000, reward total was -18.000000. running mean: -20.172947\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.171218\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.179506\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.167711\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.176034\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.184273\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.182430\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.190606\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.188700\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.196813\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.204845\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.212797\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.210669\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.218562\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.216376\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.204212\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.202170\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.200149\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.198147\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.196166\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.184204\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.192362\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.190438\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.188534\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.196649\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.194682\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.202735\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.200708\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.188701\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.196814\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.194846\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.202897\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.210868\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.218760\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.226572\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.224306\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.222063\n",
            "CPU times: user 1h 7min 22s, sys: 12min 48s, total: 1h 20min 10s\n",
            "Wall time: 41min 3s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "11c39b84-0f2b-41ed-84e8-ee8308c61a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "763f5cf0-3fd3-440e-b52e-e77b691ae6c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.980297\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980494\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980689\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.980882\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.971073\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.971363\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.971649\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.971933\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.962213\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.962591\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.962965\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.963336\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.963702\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.944065\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.944624\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.945178\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.945726\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.936269\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.926907\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.927637\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.918361\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.919177\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.919986\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.910786\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.911678\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.912561\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.913436\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.914301\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.905158\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.906107\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.897046\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.898075\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.899094\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.880103\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.861302\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.862689\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.864062\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.865422\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.866768\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.868100\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.869419\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.870725\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.852018\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.843497\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.835062\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.826712\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.828445\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.820160\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.811959\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.813839\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.815701\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.807544\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -20.789468\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.781573\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.773758\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.776020\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.778260\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -20.760477\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.762873\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.765244\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.757591\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.760015\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.762415\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.764791\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.767143\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.769472\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.771777\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -20.754059\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.746519\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.749054\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.751563\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.744047\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -20.726607\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.719341\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.702147\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.695126\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.698175\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.701193\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.694181\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.677239\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.660467\n",
            "resetting env. episode 86.000000, reward total was -19.000000. running mean: -20.643862\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.637424\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.641049\n",
            "resetting env. episode 89.000000, reward total was -18.000000. running mean: -20.614639\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.618492\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.622308\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.626084\n",
            "resetting env. episode 93.000000, reward total was -18.000000. running mean: -20.599824\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.603825\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.597787\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.601809\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.585791\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.579933\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.584134\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.578293\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.572510\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.566785\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.571117\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.575406\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.579651\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.583855\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.578016\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.562236\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.556614\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.551048\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.555537\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.549982\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.544482\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.539037\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.543647\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.538210\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.542828\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.547400\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.551926\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.556407\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.540843\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.545434\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.549980\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.554480\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.558935\n",
            "resetting env. episode 126.000000, reward total was -18.000000. running mean: -20.533346\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.538013\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.532632\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.527306\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.532033\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.526713\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.521446\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.516231\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.511069\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.495958\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.490999\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.486089\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.491228\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.476315\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.461552\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.466937\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.452267\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.457745\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.463167\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.458536\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.453950\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.459411\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.454817\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.460268\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.465666\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.461009\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.466399\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.461735\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.467118\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.472446\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -20.457722\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.463145\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.458513\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.453928\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.439389\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.434995\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.440645\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.446239\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.441776\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.437358\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.432985\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.438655\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.444268\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.449826\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.455328\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.440774\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.446367\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.451903\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.447384\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.452910\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.458381\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.463797\n",
            "resetting env. episode 178.000000, reward total was -18.000000. running mean: -20.439159\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.444768\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.430320\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.436017\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.431656\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.437340\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.442967\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.448537\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.454051\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.439511\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.445116\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.440665\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.436258\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.421895\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.427677\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.433400\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.429066\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.434775\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.440427\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.436023\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.441663\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.437246\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.432874\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.428545\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.434260\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.419917\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.405718\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.401661\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.407644\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.413568\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.409432\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.415338\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.411184\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.417072\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.422902\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.418673\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.404486\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.410441\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.406337\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.412273\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.418151\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.423969\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.429729\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.425432\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.421178\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.416966\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.412796\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.408668\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.414582\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.420436\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.426231\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.411969\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.407849\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.413771\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.419633\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -20.405437\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.391383\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.397469\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.403494\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.389459\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.385565\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.391709\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.397792\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.393814\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.389876\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.385977\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.372117\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.378396\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.384612\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -20.360766\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.357158\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.353587\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.350051\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.356550\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.362985\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.359355\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.345761\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.352304\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.358781\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.345193\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.331741\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.338424\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.335039\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.321689\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.328472\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.325187\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.331936\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.338616\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.335230\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.341878\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.338459\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.345074\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.351624\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.348107\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.334626\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.341280\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.337867\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.334489\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.341144\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.347732\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.334255\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.340912\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.347503\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.344028\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.350588\n",
            "resetting env. episode 283.000000, reward total was -18.000000. running mean: -20.327082\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.333811\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.340473\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.347068\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.343598\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.340162\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.336760\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.343392\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.339959\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.346559\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.343093\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.339662\n",
            "resetting env. episode 295.000000, reward total was -18.000000. running mean: -20.316266\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.303103\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.290072\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.287171\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.284300\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.291457\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.298542\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.305557\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.312501\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.309376\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.306282\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.303220\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.310187\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.317085\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.313915\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.310775\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.317668\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.324491\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.331246\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.337934\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.344554\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.341109\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.347698\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.354221\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.360679\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.367072\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.373401\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.379667\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.385870\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.392012\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.398092\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.394111\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.390169\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.396268\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.392305\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.398382\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.394398\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.400454\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.386450\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.392585\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.398659\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.394673\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.390726\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.376819\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.383051\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.379220\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.385428\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.381574\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.377758\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.373980\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.370241\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.356538\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.352973\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.349443\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.355949\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.362389\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.368765\n",
            "resetting env. episode 352.000000, reward total was -18.000000. running mean: -20.345078\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.341627\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.338210\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.334828\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.341480\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.338065\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.344685\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.351238\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.347725\n",
            "resetting env. episode 361.000000, reward total was -18.000000. running mean: -20.324248\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.331006\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.337696\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.344319\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.340875\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.327467\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.324192\n",
            "resetting env. episode 368.000000, reward total was -18.000000. running mean: -20.300950\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.287941\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.295061\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.292111\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.299190\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.306198\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.313136\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.320004\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.326804\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.323536\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.310301\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.307198\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.314126\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.320985\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.317775\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.304597\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.301551\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.308536\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.315450\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.322296\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.319073\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.325882\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.312623\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.319497\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.316302\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.323139\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.329908\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.316608\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.303442\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.290408\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.287504\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.294629\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.301683\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.298666\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.305679\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.312622\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.319496\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.306301\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.313238\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.310106\n",
            "resetting env. episode 408.000000, reward total was -18.000000. running mean: -20.287005\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.274135\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.261393\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.248779\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.246292\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.253829\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.251290\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.248777\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.256290\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.253727\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.261189\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.258578\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.265992\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.273332\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.280599\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.287793\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.274915\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.272166\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.269444\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.276749\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.273982\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.261242\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.258630\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.266043\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.263383\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.270749\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.258042\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.255461\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.242907\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.250478\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.257973\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.265393\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.272739\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.270012\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.277312\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.284538\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.291693\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.298776\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.285788\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.282931\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.280101\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.267300\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.264627\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.251981\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.249461\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.256967\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.264397\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -20.241753\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.249335\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.246842\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.244374\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.251930\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.259411\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.266816\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.274148\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.271407\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.268693\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.276006\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.283246\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.280413\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.277609\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.274833\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.272085\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.259364\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.266770\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.254103\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.261562\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.268946\n",
            "resetting env. episode 476.000000, reward total was -17.000000. running mean: -20.236256\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.233894\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.231555\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.239239\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.236847\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.244479\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.252034\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.259513\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.266918\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.264249\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.271607\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.278891\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.276102\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.273341\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.270607\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.277901\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.275122\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.282371\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.289547\n",
            "resetting env. episode 495.000000, reward total was -18.000000. running mean: -20.266652\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.253985\n",
            "resetting env. episode 497.000000, reward total was -18.000000. running mean: -20.231445\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.229131\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.226840\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.234571\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.232225\n",
            "resetting env. episode 502.000000, reward total was -19.000000. running mean: -20.219903\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.217704\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.215527\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.223372\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.221138\n",
            "resetting env. episode 507.000000, reward total was -18.000000. running mean: -20.198927\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.206938\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.214868\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.212719\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.220592\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.218386\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.226202\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.233940\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.241601\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.249185\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.246693\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.254226\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.261684\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.259067\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.266476\n",
            "resetting env. episode 522.000000, reward total was -19.000000. running mean: -20.253812\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -20.241274\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.238861\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.236472\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.234108\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.241766\n",
            "resetting env. episode 528.000000, reward total was -19.000000. running mean: -20.229349\n",
            "resetting env. episode 529.000000, reward total was -19.000000. running mean: -20.217055\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.214885\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.222736\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.230509\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.238203\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.245821\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.253363\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.260830\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.268221\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.265539\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.272884\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.270155\n",
            "resetting env. episode 541.000000, reward total was -19.000000. running mean: -20.257453\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.264879\n",
            "resetting env. episode 543.000000, reward total was -20.000000. running mean: -20.262230\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.269608\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.276912\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -20.264142\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.271501\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.268786\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.276098\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.283337\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.290504\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.287599\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.284723\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.291876\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.288957\n",
            "resetting env. episode 556.000000, reward total was -19.000000. running mean: -20.276067\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.283307\n",
            "resetting env. episode 558.000000, reward total was -19.000000. running mean: -20.270474\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.277769\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.284991\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.282141\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.279320\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.286527\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.293661\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.290725\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.297817\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.304839\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.301791\n",
            "resetting env. episode 569.000000, reward total was -19.000000. running mean: -20.288773\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.295885\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.302926\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.309897\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.316798\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.323630\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.330394\n",
            "resetting env. episode 576.000000, reward total was -19.000000. running mean: -20.317090\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.313919\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.320780\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.317572\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -20.304396\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.311352\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.318239\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.325056\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.321806\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.328588\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.335302\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.341949\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.348529\n",
            "resetting env. episode 589.000000, reward total was -18.000000. running mean: -20.325044\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -20.321794\n",
            "resetting env. episode 591.000000, reward total was -19.000000. running mean: -20.308576\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.315490\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.322335\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.329112\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.335821\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.332462\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -20.319138\n",
            "resetting env. episode 598.000000, reward total was -19.000000. running mean: -20.305946\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.312887\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.319758\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.326561\n",
            "resetting env. episode 602.000000, reward total was -19.000000. running mean: -20.313295\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.310162\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -20.307060\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.313990\n",
            "resetting env. episode 606.000000, reward total was -18.000000. running mean: -20.290850\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.297941\n",
            "resetting env. episode 608.000000, reward total was -19.000000. running mean: -20.284962\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -20.272112\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.269391\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.266697\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.274030\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.281290\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.278477\n",
            "resetting env. episode 615.000000, reward total was -19.000000. running mean: -20.265692\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.263035\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.270405\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.277701\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.274924\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.282175\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.279353\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.286559\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.293694\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.300757\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.307749\n",
            "resetting env. episode 626.000000, reward total was -17.000000. running mean: -20.274672\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.281925\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.289106\n",
            "resetting env. episode 629.000000, reward total was -19.000000. running mean: -20.276215\n",
            "resetting env. episode 630.000000, reward total was -19.000000. running mean: -20.263453\n",
            "resetting env. episode 631.000000, reward total was -19.000000. running mean: -20.250818\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.258310\n",
            "resetting env. episode 633.000000, reward total was -17.000000. running mean: -20.225727\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.233470\n",
            "resetting env. episode 635.000000, reward total was -18.000000. running mean: -20.211135\n",
            "resetting env. episode 636.000000, reward total was -19.000000. running mean: -20.199024\n",
            "resetting env. episode 637.000000, reward total was -19.000000. running mean: -20.187033\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.195163\n",
            "resetting env. episode 639.000000, reward total was -20.000000. running mean: -20.193211\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.201279\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.209266\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.207174\n",
            "resetting env. episode 643.000000, reward total was -19.000000. running mean: -20.195102\n",
            "resetting env. episode 644.000000, reward total was -19.000000. running mean: -20.183151\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.191320\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.189406\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.197512\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.205537\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.203482\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.211447\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.209332\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.207239\n",
            "resetting env. episode 653.000000, reward total was -18.000000. running mean: -20.185167\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.183315\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.191482\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.199567\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.197571\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.205596\n",
            "resetting env. episode 659.000000, reward total was -19.000000. running mean: -20.193540\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.191604\n",
            "resetting env. episode 661.000000, reward total was -17.000000. running mean: -20.159688\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.158091\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.156511\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.164945\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.173296\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.171563\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.179847\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.188049\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.186168\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -20.184307\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.182464\n",
            "resetting env. episode 672.000000, reward total was -18.000000. running mean: -20.160639\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.169033\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.177342\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.175569\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.183813\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.181975\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.190155\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.198254\n",
            "resetting env. episode 680.000000, reward total was -19.000000. running mean: -20.186271\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.194409\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.202464\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.200440\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.198435\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -20.186451\n",
            "resetting env. episode 686.000000, reward total was -18.000000. running mean: -20.164587\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.172941\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.181211\n",
            "resetting env. episode 689.000000, reward total was -19.000000. running mean: -20.169399\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.177705\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.175928\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.174169\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.182427\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.190603\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -20.188697\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.196810\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -20.194842\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.202893\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.210864\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.208756\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.216668\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.224502\n",
            "resetting env. episode 703.000000, reward total was -19.000000. running mean: -20.212257\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.210134\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.208033\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.195952\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.203993\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.211953\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.219833\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.227635\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.225359\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.233105\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.230774\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.238466\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.246082\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.253621\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.261085\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.268474\n",
            "resetting env. episode 719.000000, reward total was -19.000000. running mean: -20.255789\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -20.253231\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.260699\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.268092\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.275411\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.282657\n",
            "resetting env. episode 725.000000, reward total was -19.000000. running mean: -20.269830\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.277132\n",
            "resetting env. episode 727.000000, reward total was -19.000000. running mean: -20.264361\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.271717\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.279000\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.286210\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -20.273348\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.280614\n",
            "resetting env. episode 733.000000, reward total was -19.000000. running mean: -20.267808\n",
            "resetting env. episode 734.000000, reward total was -19.000000. running mean: -20.255130\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.252579\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.260053\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.267452\n",
            "resetting env. episode 738.000000, reward total was -19.000000. running mean: -20.254778\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.252230\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.249708\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.257211\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.254639\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.262092\n",
            "resetting env. episode 744.000000, reward total was -19.000000. running mean: -20.249471\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.256977\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.264407\n",
            "resetting env. episode 747.000000, reward total was -20.000000. running mean: -20.261763\n",
            "resetting env. episode 748.000000, reward total was -18.000000. running mean: -20.239145\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.246754\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.254286\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.251743\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.259226\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.256634\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.264067\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -20.271427\n",
            "resetting env. episode 756.000000, reward total was -19.000000. running mean: -20.258712\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.256125\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -20.253564\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.261028\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.268418\n",
            "resetting env. episode 761.000000, reward total was -19.000000. running mean: -20.255734\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.263176\n",
            "resetting env. episode 763.000000, reward total was -20.000000. running mean: -20.260545\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.267939\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.275260\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.282507\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -20.279682\n",
            "resetting env. episode 768.000000, reward total was -19.000000. running mean: -20.266885\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.274217\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.271474\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -20.258760\n",
            "resetting env. episode 772.000000, reward total was -19.000000. running mean: -20.246172\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.253710\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.261173\n",
            "resetting env. episode 775.000000, reward total was -18.000000. running mean: -20.238561\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.246176\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.243714\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.251277\n",
            "resetting env. episode 779.000000, reward total was -15.000000. running mean: -20.198764\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.196777\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -20.194809\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.192861\n",
            "resetting env. episode 783.000000, reward total was -19.000000. running mean: -20.180932\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.179123\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -20.167332\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.175658\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -20.173902\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.182163\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -20.170341\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.168638\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -20.156951\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.165382\n",
            "resetting env. episode 793.000000, reward total was -20.000000. running mean: -20.163728\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -20.162091\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.170470\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.178765\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.176977\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.185208\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.193356\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.201422\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.199408\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.207414\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.215340\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.213186\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.211054\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.218944\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.216754\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.224587\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.222341\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.220117\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.217916\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.225737\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.223480\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -20.211245\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.209133\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -20.207041\n",
            "resetting env. episode 817.000000, reward total was -19.000000. running mean: -20.194971\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.193021\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.201091\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.199080\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -20.187089\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.195218\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.203266\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.211233\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.219121\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.216930\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.224761\n",
            "resetting env. episode 828.000000, reward total was -19.000000. running mean: -20.212513\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -20.210388\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.218284\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.226101\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.233840\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -20.221502\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.229287\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.236994\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.244624\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.242178\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.249756\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -20.247258\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.254786\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.252238\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.259715\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.267118\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.274447\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.281703\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.288886\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.295997\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.293037\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -20.290106\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.297205\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.304233\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.311191\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.318079\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.314898\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -20.311749\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.308632\n",
            "resetting env. episode 857.000000, reward total was -19.000000. running mean: -20.295546\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.292590\n",
            "resetting env. episode 859.000000, reward total was -19.000000. running mean: -20.279664\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.286868\n",
            "resetting env. episode 861.000000, reward total was -18.000000. running mean: -20.263999\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.261359\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.268745\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.276058\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.283297\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.290464\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -20.287560\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.294684\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.301737\n",
            "resetting env. episode 870.000000, reward total was -19.000000. running mean: -20.288720\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.285833\n",
            "resetting env. episode 872.000000, reward total was -19.000000. running mean: -20.272974\n",
            "resetting env. episode 873.000000, reward total was -20.000000. running mean: -20.270245\n",
            "resetting env. episode 874.000000, reward total was -20.000000. running mean: -20.267542\n",
            "resetting env. episode 875.000000, reward total was -18.000000. running mean: -20.244867\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.242418\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.249994\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -20.237494\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.235119\n",
            "resetting env. episode 880.000000, reward total was -19.000000. running mean: -20.222768\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.230540\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.228235\n",
            "resetting env. episode 883.000000, reward total was -19.000000. running mean: -20.215952\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.223793\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.231555\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.239239\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.246847\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -20.244378\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -20.231935\n",
            "resetting env. episode 890.000000, reward total was -19.000000. running mean: -20.219615\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.227419\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.235145\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.232794\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.240466\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.238061\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.245680\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.253224\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.250691\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -20.248184\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.255703\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.263146\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.270514\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.277809\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.285031\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.292181\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.299259\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.296266\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.303303\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.310270\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.317168\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.323996\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.330756\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.337449\n",
            "resetting env. episode 914.000000, reward total was -17.000000. running mean: -20.304074\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.311033\n",
            "resetting env. episode 916.000000, reward total was -19.000000. running mean: -20.297923\n",
            "resetting env. episode 917.000000, reward total was -19.000000. running mean: -20.284944\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.282094\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -20.279273\n",
            "resetting env. episode 920.000000, reward total was -19.000000. running mean: -20.266481\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.273816\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.271078\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.278367\n",
            "resetting env. episode 924.000000, reward total was -19.000000. running mean: -20.265583\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.272927\n",
            "resetting env. episode 926.000000, reward total was -18.000000. running mean: -20.250198\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -20.247696\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -20.235219\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.242867\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -20.240438\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.238034\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.245654\n",
            "resetting env. episode 933.000000, reward total was -20.000000. running mean: -20.243197\n",
            "resetting env. episode 934.000000, reward total was -18.000000. running mean: -20.220765\n",
            "resetting env. episode 935.000000, reward total was -17.000000. running mean: -20.188557\n",
            "resetting env. episode 936.000000, reward total was -18.000000. running mean: -20.166672\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.165005\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.163355\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.171722\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.180004\n",
            "resetting env. episode 941.000000, reward total was -19.000000. running mean: -20.168204\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.176522\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -20.174757\n",
            "resetting env. episode 944.000000, reward total was -19.000000. running mean: -20.163009\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -20.161379\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.159766\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -20.148168\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.146686\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.145219\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.153767\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.162229\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.170607\n",
            "resetting env. episode 953.000000, reward total was -19.000000. running mean: -20.158901\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.167312\n",
            "resetting env. episode 955.000000, reward total was -19.000000. running mean: -20.155639\n",
            "resetting env. episode 956.000000, reward total was -20.000000. running mean: -20.154083\n",
            "resetting env. episode 957.000000, reward total was -18.000000. running mean: -20.132542\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -20.131216\n",
            "resetting env. episode 959.000000, reward total was -18.000000. running mean: -20.109904\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.108805\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.107717\n",
            "resetting env. episode 962.000000, reward total was -19.000000. running mean: -20.096640\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.105674\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -20.094617\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.083671\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.092834\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.101906\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.100887\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.109878\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.108779\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.117691\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.126514\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.135249\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.133897\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.142558\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.141132\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.139721\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.148323\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.156840\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.155272\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.163719\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.172082\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.180361\n",
            "resetting env. episode 984.000000, reward total was -19.000000. running mean: -20.168558\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.176872\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.185103\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.193252\n",
            "resetting env. episode 988.000000, reward total was -19.000000. running mean: -20.181320\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.189506\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.197611\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.205635\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.213579\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.221443\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.229229\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.236936\n",
            "resetting env. episode 996.000000, reward total was -18.000000. running mean: -20.214567\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.222421\n",
            "resetting env. episode 998.000000, reward total was -19.000000. running mean: -20.210197\n",
            "resetting env. episode 999.000000, reward total was -19.000000. running mean: -20.198095\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.206114\n",
            "resetting env. episode 1001.000000, reward total was -19.000000. running mean: -20.194053\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.192113\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.200191\n",
            "resetting env. episode 1004.000000, reward total was -20.000000. running mean: -20.198190\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.196208\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.194246\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -20.192303\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.200380\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.208376\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.206293\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.214230\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.222087\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.229866\n",
            "resetting env. episode 1014.000000, reward total was -18.000000. running mean: -20.207568\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.215492\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.213337\n",
            "resetting env. episode 1017.000000, reward total was -19.000000. running mean: -20.201204\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.209192\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.217100\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -20.214929\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.212780\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -20.210652\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.218545\n",
            "resetting env. episode 1024.000000, reward total was -19.000000. running mean: -20.206360\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.214296\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.222153\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.229932\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.237632\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.245256\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.242803\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.250375\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.247872\n",
            "resetting env. episode 1033.000000, reward total was -19.000000. running mean: -20.235393\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.243039\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.250609\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.258103\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.265522\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.272866\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.280138\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.287336\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -20.284463\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.291618\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -20.288702\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.295815\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.292857\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.299928\n",
            "resetting env. episode 1047.000000, reward total was -18.000000. running mean: -20.276929\n",
            "resetting env. episode 1048.000000, reward total was -19.000000. running mean: -20.264160\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.271518\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -20.268803\n",
            "resetting env. episode 1051.000000, reward total was -19.000000. running mean: -20.256115\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.263554\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.270918\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -20.268209\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.275527\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.272772\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.280044\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.287244\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.294371\n",
            "resetting env. episode 1060.000000, reward total was -19.000000. running mean: -20.281427\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.288613\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.285727\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.292870\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.299941\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.306942\n",
            "resetting env. episode 1066.000000, reward total was -19.000000. running mean: -20.293872\n",
            "resetting env. episode 1067.000000, reward total was -19.000000. running mean: -20.280934\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.278124\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.275343\n",
            "resetting env. episode 1070.000000, reward total was -19.000000. running mean: -20.262590\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -20.259964\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.257364\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.264790\n",
            "resetting env. episode 1074.000000, reward total was -18.000000. running mean: -20.242142\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.249721\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.257224\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.264652\n",
            "resetting env. episode 1078.000000, reward total was -19.000000. running mean: -20.252005\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.259485\n",
            "resetting env. episode 1080.000000, reward total was -19.000000. running mean: -20.246890\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.254421\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.261877\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -20.259258\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.266666\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -20.263999\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.271359\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.278645\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -20.275859\n",
            "resetting env. episode 1089.000000, reward total was -21.000000. running mean: -20.283100\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.290269\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.297367\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -20.294393\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.301449\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.308435\n",
            "resetting env. episode 1095.000000, reward total was -19.000000. running mean: -20.295350\n",
            "resetting env. episode 1096.000000, reward total was -18.000000. running mean: -20.272397\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.279673\n",
            "resetting env. episode 1098.000000, reward total was -19.000000. running mean: -20.266876\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -20.264207\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.261565\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.258950\n",
            "resetting env. episode 1102.000000, reward total was -19.000000. running mean: -20.246360\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -20.243896\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.251458\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.258943\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.256354\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.253790\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.261252\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -20.258640\n",
            "resetting env. episode 1110.000000, reward total was -19.000000. running mean: -20.246053\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.243593\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.241157\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.248745\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.256258\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.263695\n",
            "resetting env. episode 1116.000000, reward total was -19.000000. running mean: -20.251058\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.258548\n",
            "resetting env. episode 1118.000000, reward total was -18.000000. running mean: -20.235962\n",
            "resetting env. episode 1119.000000, reward total was -19.000000. running mean: -20.223602\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.231366\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -20.229053\n",
            "resetting env. episode 1122.000000, reward total was -18.000000. running mean: -20.206762\n",
            "resetting env. episode 1123.000000, reward total was -20.000000. running mean: -20.204695\n",
            "resetting env. episode 1124.000000, reward total was -16.000000. running mean: -20.162648\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.161021\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.169411\n",
            "resetting env. episode 1127.000000, reward total was -19.000000. running mean: -20.157717\n",
            "resetting env. episode 1128.000000, reward total was -18.000000. running mean: -20.136140\n",
            "resetting env. episode 1129.000000, reward total was -16.000000. running mean: -20.094778\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -20.083831\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -20.072992\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.082262\n",
            "resetting env. episode 1133.000000, reward total was -19.000000. running mean: -20.071440\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.070725\n",
            "resetting env. episode 1135.000000, reward total was -19.000000. running mean: -20.060018\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -20.069418\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.078724\n",
            "resetting env. episode 1138.000000, reward total was -19.000000. running mean: -20.067936\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.067257\n",
            "resetting env. episode 1140.000000, reward total was -19.000000. running mean: -20.056585\n",
            "resetting env. episode 1141.000000, reward total was -19.000000. running mean: -20.046019\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.055558\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -20.055003\n",
            "resetting env. episode 1144.000000, reward total was -19.000000. running mean: -20.044453\n",
            "resetting env. episode 1145.000000, reward total was -17.000000. running mean: -20.014008\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.013868\n",
            "resetting env. episode 1147.000000, reward total was -19.000000. running mean: -20.003730\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.003692\n",
            "resetting env. episode 1149.000000, reward total was -17.000000. running mean: -19.973655\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -19.983919\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -19.994080\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.004139\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.014097\n",
            "resetting env. episode 1154.000000, reward total was -17.000000. running mean: -19.983956\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -19.994117\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.004176\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -20.004134\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.014093\n",
            "resetting env. episode 1159.000000, reward total was -19.000000. running mean: -20.003952\n",
            "resetting env. episode 1160.000000, reward total was -19.000000. running mean: -19.993912\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -19.993973\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -19.994033\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.004093\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.004052\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.014012\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.013871\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.023733\n",
            "resetting env. episode 1168.000000, reward total was -19.000000. running mean: -20.013495\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.023360\n",
            "resetting env. episode 1170.000000, reward total was -20.000000. running mean: -20.023127\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.032896\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.042567\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.042141\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.051720\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.061202\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.060590\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.069984\n",
            "resetting env. episode 1178.000000, reward total was -18.000000. running mean: -20.049285\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.058792\n",
            "resetting env. episode 1180.000000, reward total was -20.000000. running mean: -20.058204\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.067622\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -20.066946\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.076276\n",
            "resetting env. episode 1184.000000, reward total was -20.000000. running mean: -20.075513\n",
            "resetting env. episode 1185.000000, reward total was -18.000000. running mean: -20.054758\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.064211\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.073569\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -20.072833\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -20.072104\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -20.061383\n",
            "resetting env. episode 1191.000000, reward total was -19.000000. running mean: -20.050770\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.060262\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.069659\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.078963\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.088173\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.087291\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -20.086418\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.095554\n",
            "resetting env. episode 1199.000000, reward total was -17.000000. running mean: -20.064599\n",
            "resetting env. episode 1200.000000, reward total was -18.000000. running mean: -20.043953\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -20.043513\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.053078\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -20.042547\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -20.042122\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.051701\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.051184\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.060672\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.070065\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -20.069364\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -20.068671\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.077984\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -20.077204\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.086432\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.085568\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.094712\n",
            "resetting env. episode 1216.000000, reward total was -18.000000. running mean: -20.073765\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.073027\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.082297\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -20.081474\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.090659\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.099753\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -20.098755\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -20.087768\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -20.086890\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.096021\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.095061\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.104110\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -20.093069\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -20.092139\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.091217\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.100305\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -20.109302\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.108209\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.117127\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.125956\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -20.124696\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.133449\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.142115\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.150693\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.159186\n",
            "resetting env. episode 1241.000000, reward total was -19.000000. running mean: -20.147595\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -20.136119\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.144757\n",
            "resetting env. episode 1244.000000, reward total was -19.000000. running mean: -20.133310\n",
            "resetting env. episode 1245.000000, reward total was -19.000000. running mean: -20.121977\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.130757\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -20.129449\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.128155\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.136873\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.135505\n",
            "resetting env. episode 1251.000000, reward total was -19.000000. running mean: -20.124150\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.132908\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.141579\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.150163\n",
            "resetting env. episode 1255.000000, reward total was -20.000000. running mean: -20.148662\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.157175\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.165603\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -20.153947\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -20.152408\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.150884\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -20.149375\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -20.137881\n",
            "resetting env. episode 1263.000000, reward total was -20.000000. running mean: -20.136502\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.145137\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.153686\n",
            "resetting env. episode 1266.000000, reward total was -19.000000. running mean: -20.142149\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.150728\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.149220\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.157728\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.166151\n",
            "resetting env. episode 1271.000000, reward total was -19.000000. running mean: -20.154489\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.152944\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -20.151415\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -20.149901\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.158402\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.166818\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.175150\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.183398\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.181564\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.189748\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -20.177851\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.186072\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.194212\n",
            "resetting env. episode 1284.000000, reward total was -17.000000. running mean: -20.162270\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.170647\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -20.168940\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.177251\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -20.185479\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.183624\n",
            "resetting env. episode 1290.000000, reward total was -19.000000. running mean: -20.171788\n",
            "resetting env. episode 1291.000000, reward total was -19.000000. running mean: -20.160070\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.168469\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.176784\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.185016\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.193166\n",
            "resetting env. episode 1296.000000, reward total was -20.000000. running mean: -20.191235\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -20.189322\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.197429\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -20.185455\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.193600\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -20.181664\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.179848\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.188049\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.196169\n",
            "resetting env. episode 1305.000000, reward total was -19.000000. running mean: -20.184207\n",
            "resetting env. episode 1306.000000, reward total was -18.000000. running mean: -20.162365\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.170741\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -20.169034\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.167343\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.165670\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -20.164013\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.172373\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -20.170649\n",
            "resetting env. episode 1314.000000, reward total was -18.000000. running mean: -20.148943\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.157454\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -20.155879\n",
            "resetting env. episode 1317.000000, reward total was -18.000000. running mean: -20.134320\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -20.132977\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.141647\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.150231\n",
            "resetting env. episode 1321.000000, reward total was -19.000000. running mean: -20.138728\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.137341\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -20.135968\n",
            "resetting env. episode 1324.000000, reward total was -19.000000. running mean: -20.124608\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.133362\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.142028\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -20.140608\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -20.139202\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -20.137810\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.146432\n",
            "resetting env. episode 1331.000000, reward total was -19.000000. running mean: -20.134968\n",
            "resetting env. episode 1332.000000, reward total was -18.000000. running mean: -20.113618\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.122482\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.131257\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -20.129944\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.128645\n",
            "resetting env. episode 1337.000000, reward total was -18.000000. running mean: -20.107358\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.116285\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.115122\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.123971\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -20.122731\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.121504\n",
            "resetting env. episode 1343.000000, reward total was -20.000000. running mean: -20.120289\n",
            "resetting env. episode 1344.000000, reward total was -19.000000. running mean: -20.109086\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.117995\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.116815\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.125647\n",
            "resetting env. episode 1348.000000, reward total was -17.000000. running mean: -20.094390\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.093446\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.102512\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.101487\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.100472\n",
            "resetting env. episode 1353.000000, reward total was -19.000000. running mean: -20.089467\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.088573\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -20.077687\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -20.076910\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.086141\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.095280\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.104327\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -20.103283\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.112251\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.121128\n",
            "resetting env. episode 1363.000000, reward total was -16.000000. running mean: -20.079917\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -20.069118\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -20.058427\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.057842\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -20.057264\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.066691\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.076024\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.085264\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.094411\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.103467\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.102433\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.111408\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -20.110294\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.119191\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.117999\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.126819\n",
            "resetting env. episode 1379.000000, reward total was -15.000000. running mean: -20.075551\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -20.074796\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.064048\n",
            "resetting env. episode 1382.000000, reward total was -18.000000. running mean: -20.043407\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -20.032973\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.042643\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.052217\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.051695\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.061178\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.070566\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.079860\n",
            "resetting env. episode 1390.000000, reward total was -18.000000. running mean: -20.059062\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.068471\n",
            "resetting env. episode 1392.000000, reward total was -19.000000. running mean: -20.057786\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.067209\n",
            "resetting env. episode 1394.000000, reward total was -19.000000. running mean: -20.056537\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.055971\n",
            "resetting env. episode 1396.000000, reward total was -20.000000. running mean: -20.055411\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -20.054857\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.054309\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -20.043766\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.043328\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.052895\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.062366\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -20.061742\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -20.061125\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.070513\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.079808\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -20.079010\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.088220\n",
            "resetting env. episode 1409.000000, reward total was -19.000000. running mean: -20.077338\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.086565\n",
            "resetting env. episode 1411.000000, reward total was -18.000000. running mean: -20.065699\n",
            "resetting env. episode 1412.000000, reward total was -19.000000. running mean: -20.055042\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.064492\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -20.053847\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.063308\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.072675\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -20.071948\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -20.071229\n",
            "resetting env. episode 1419.000000, reward total was -18.000000. running mean: -20.050517\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -20.060011\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -20.059411\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.068817\n",
            "resetting env. episode 1423.000000, reward total was -19.000000. running mean: -20.058129\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.067548\n",
            "resetting env. episode 1425.000000, reward total was -19.000000. running mean: -20.056872\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.066303\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.065640\n",
            "resetting env. episode 1428.000000, reward total was -19.000000. running mean: -20.054984\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.054434\n",
            "resetting env. episode 1430.000000, reward total was -19.000000. running mean: -20.043890\n",
            "resetting env. episode 1431.000000, reward total was -17.000000. running mean: -20.013451\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -20.003316\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -20.003283\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.013250\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.023118\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.022887\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -20.022658\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.032431\n",
            "resetting env. episode 1439.000000, reward total was -19.000000. running mean: -20.022107\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.031886\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.031567\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.041251\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -20.040839\n",
            "resetting env. episode 1444.000000, reward total was -18.000000. running mean: -20.020431\n",
            "resetting env. episode 1445.000000, reward total was -17.000000. running mean: -19.990226\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.000324\n",
            "resetting env. episode 1447.000000, reward total was -18.000000. running mean: -19.980321\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -19.980517\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -19.990712\n",
            "resetting env. episode 1450.000000, reward total was -19.000000. running mean: -19.980805\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -19.990997\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.001087\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -20.001076\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.011066\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.020955\n",
            "resetting env. episode 1456.000000, reward total was -19.000000. running mean: -20.010745\n",
            "resetting env. episode 1457.000000, reward total was -19.000000. running mean: -20.000638\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.010631\n",
            "resetting env. episode 1459.000000, reward total was -19.000000. running mean: -20.000525\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -20.000520\n",
            "resetting env. episode 1461.000000, reward total was -19.000000. running mean: -19.990515\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -19.990610\n",
            "resetting env. episode 1463.000000, reward total was -19.000000. running mean: -19.980703\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -19.980896\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -19.991087\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.001177\n",
            "resetting env. episode 1467.000000, reward total was -19.000000. running mean: -19.991165\n",
            "resetting env. episode 1468.000000, reward total was -19.000000. running mean: -19.981253\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -19.991441\n",
            "resetting env. episode 1470.000000, reward total was -19.000000. running mean: -19.981526\n",
            "resetting env. episode 1471.000000, reward total was -18.000000. running mean: -19.961711\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -19.972094\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -19.982373\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -19.992549\n",
            "resetting env. episode 1475.000000, reward total was -19.000000. running mean: -19.982624\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -19.982797\n",
            "resetting env. episode 1477.000000, reward total was -18.000000. running mean: -19.962970\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.973340\n",
            "resetting env. episode 1479.000000, reward total was -19.000000. running mean: -19.963606\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -19.973970\n",
            "resetting env. episode 1481.000000, reward total was -15.000000. running mean: -19.924231\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -19.934988\n",
            "resetting env. episode 1483.000000, reward total was -19.000000. running mean: -19.925638\n",
            "resetting env. episode 1484.000000, reward total was -19.000000. running mean: -19.916382\n",
            "resetting env. episode 1485.000000, reward total was -19.000000. running mean: -19.907218\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -19.918146\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -19.928965\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -19.929675\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -19.940378\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -19.940974\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -19.941565\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -19.932149\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -19.932828\n",
            "resetting env. episode 1494.000000, reward total was -17.000000. running mean: -19.903499\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -19.914464\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -19.925320\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -19.936066\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -19.936706\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -19.937339\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -19.937965\n",
            "CPU times: user 5h 19min 32s, sys: 52min 3s, total: 6h 11min 35s\n",
            "Wall time: 3h 16min 54s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "w2NblmwDsL3y",
        "outputId": "740a680d-1524-46b1-c257-97110ff30151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Iteration_8_h=800_1e_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}