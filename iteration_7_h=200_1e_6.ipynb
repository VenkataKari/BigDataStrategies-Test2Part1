{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "142299ce-3ff9-49d0-e4fa-f0b17bba4a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 30.5 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=2dcfdc1e40c806bb8b7712608107e34dff51761ce64c086a330fcf0afd016a39\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "18e21708-34f7-4985-c9f2-6440a20737fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "17bc48e7-1843-4e13-ee99-895ba1d9a95d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "7afc1e01-cfdb-4a0e-99fb-ce0803ecc00d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "15683d18-1f0e-4785-a817-b7a696b68792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-6\n",
        "learning_rate = 1e-6\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "9849cbd8-ec5b-476e-f7cf-313eb39ada30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.990394\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.990490\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.980585\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.970779\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.961072\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.961461\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.941846\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.942428\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.943003\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.933573\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -20.914238\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.915095\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.915944\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.916785\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.907617\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.908541\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.909456\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.910361\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.911257\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.902145\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.893123\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.894192\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.895250\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.896298\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.897335\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.898361\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.889378\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.890484\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.891579\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.892663\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.893737\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.894799\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.895851\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.896893\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.887924\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.879045\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.880254\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.871452\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.872737\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.874010\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.855270\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.836717\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.828350\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.830066\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.831766\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.833448\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.835114\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.836762\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.838395\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -20.820011\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.821811\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.813593\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.815457\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.807302\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.789229\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.791337\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.783423\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.785589\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.787733\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.769856\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.762157\n",
            "resetting env. episode 70.000000, reward total was -18.000000. running mean: -20.734536\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.737190\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.739819\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.722420\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.725196\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.717944\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.720765\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.723557\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.716322\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.719158\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.721967\n",
            "resetting env. episode 81.000000, reward total was -17.000000. running mean: -20.684747\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.677900\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.671121\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.674409\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.677665\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.670889\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.654180\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.647638\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.651162\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.654650\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.658103\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.661522\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.664907\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.658258\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.661676\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.665059\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.648408\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.641924\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.645505\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.639050\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.622659\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.616433\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.620268\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.614066\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.607925\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.611846\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.615727\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.619570\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.623374\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.627141\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.630869\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.634561\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.628215\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.621933\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.615713\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.619556\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.623361\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.617127\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.610956\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.614846\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.618698\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.622511\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.616286\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.600123\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.604122\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.608080\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.612000\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.615880\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.619721\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.623524\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.627288\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.631016\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.624705\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.618458\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.622274\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.626051\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.619791\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.613593\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.607457\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.611382\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.615268\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.619116\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.612924\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.596795\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.600827\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.604819\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.608771\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.602683\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -20.586656\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.580790\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.574982\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.579232\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.573440\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.577705\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.571928\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.576209\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.570447\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.574742\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.578995\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.573205\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.577473\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.581698\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.585881\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.590022\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.594122\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.598181\n",
            "resetting env. episode 167.000000, reward total was -18.000000. running mean: -20.572199\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.576477\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.580712\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.564905\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.569256\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.573564\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.567828\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.572150\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.576428\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.580664\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.564857\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.559209\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.563617\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.567981\n",
            "resetting env. episode 181.000000, reward total was -18.000000. running mean: -20.542301\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.546878\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.551409\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.545895\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.550436\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.554932\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.539382\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.533988\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.528648\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.523362\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.528128\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.522847\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.517619\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.502442\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.507418\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.502344\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.507320\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.502247\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.507225\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.502152\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.507131\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.492060\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.497139\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.502168\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.507146\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.512075\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.516954\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.521784\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.526566\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.521301\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.526088\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.520827\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.515619\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.520462\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.525258\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.520005\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.524805\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.529557\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.524262\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.529019\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.523729\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.508491\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.503407\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.508372\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.503289\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.508256\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.493173\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.498242\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.503259\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.498227\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.503244\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.508212\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.513130\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.517998\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.512818\n",
            "resetting env. episode 236.000000, reward total was -18.000000. running mean: -20.487690\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.482813\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.487985\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -20.473105\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.458374\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.463791\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.459153\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.454561\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.460016\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.455415\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.450861\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.456353\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.461789\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.467171\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.472499\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.477774\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.472997\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.478267\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.473484\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.478749\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.483962\n",
            "resetting env. episode 257.000000, reward total was -18.000000. running mean: -20.459122\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.464531\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.469886\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.455187\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.460635\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.466029\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.451368\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.456855\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.462286\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.457663\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.463087\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.458456\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.463871\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.459232\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.454640\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.460094\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.465493\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.470838\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.466129\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.471468\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.466753\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.462086\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.457465\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.462890\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.468262\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.473579\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.468843\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.474155\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.469413\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.454719\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.460172\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.465570\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.470914\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.476205\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.481443\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.486629\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.491762\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.496845\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.491876\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.486958\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.492088\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.487167\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.492296\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.497373\n",
            "resetting env. episode 301.000000, reward total was -18.000000. running mean: -20.472399\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.467675\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.472998\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.478268\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.483485\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.488651\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.493764\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.498826\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.503838\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.498800\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.493812\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.498874\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.503885\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.498846\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.483858\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.479019\n",
            "resetting env. episode 317.000000, reward total was -18.000000. running mean: -20.454229\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.459687\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.455090\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.450539\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.446033\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.451573\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.447057\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.442587\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.448161\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.453679\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.459143\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.454551\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.460006\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.455406\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.460851\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.466243\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.451581\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.457065\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.452494\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.457969\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.453389\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.448856\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.454367\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.459823\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.465225\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.460573\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.465967\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.461307\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.466694\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.472027\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.467307\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.472634\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.477908\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.483129\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.488297\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.493414\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.488480\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.473595\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.468859\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.474171\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.479429\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.484635\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.489789\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.494891\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.499942\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.504942\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.499893\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.494894\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.479945\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.485146\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -20.470294\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.465591\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.470935\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.476226\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.461464\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.466849\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.472181\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.477459\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.462684\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.468057\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.463377\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.468743\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.474056\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.479315\n",
            "resetting env. episode 381.000000, reward total was -18.000000. running mean: -20.454522\n",
            "resetting env. episode 382.000000, reward total was -17.000000. running mean: -20.419977\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.405777\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.401719\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.407702\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.413625\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -20.389489\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.395594\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.401638\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.397621\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.393645\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.399709\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.395712\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.391755\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.397837\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.383859\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.390020\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.386120\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.392259\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.398336\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.404353\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.410309\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.406206\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.412144\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.418023\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.413842\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.409704\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.405607\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.411551\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.407435\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.403361\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.399327\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.405334\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.401281\n",
            "resetting env. episode 415.000000, reward total was -17.000000. running mean: -20.367268\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.363595\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.369959\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.376260\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.382497\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.378672\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.384885\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.381037\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.387226\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.383354\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.389520\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.395625\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.401669\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.407652\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.413576\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.419440\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.425246\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.420993\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.426783\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.422515\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.418290\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.424107\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.429866\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.435568\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.441212\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.436800\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.442432\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.448007\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.453527\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.458992\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.454402\n",
            "resetting env. episode 446.000000, reward total was -18.000000. running mean: -20.429858\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.415560\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.421404\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.407190\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.413118\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.418987\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.424797\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.430549\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.436244\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.431881\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.437562\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.443187\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.438755\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.444367\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.439924\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.445524\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.451069\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.456558\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.461993\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.467373\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.472699\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.477972\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.483192\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.488361\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.493477\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.488542\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.473657\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.468920\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.464231\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.469589\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.474893\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.470144\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.475442\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.480688\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.485881\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.491022\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.496112\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.501151\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.506139\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.511078\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.505967\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.510908\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.505799\n",
            "resetting env. episode 489.000000, reward total was -17.000000. running mean: -20.470741\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.456033\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.461473\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.466858\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.472189\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.477468\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.482693\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.467866\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.473187\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.478455\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.483671\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.488834\n",
            "CPU times: user 23min 7s, sys: 10min 43s, total: 33min 50s\n",
            "Wall time: 17min 32s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "a97251f8-2042-4b34-a0e3-f1bce1b1745c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.039800\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -19.049402\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -19.048908\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.058419\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.077835\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -19.077056\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.096286\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -19.105323\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.124270\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.143027\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.161597\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.179981\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -19.188181\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -19.186299\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.194436\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -19.202492\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -19.200467\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.218462\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.226278\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.244015\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -19.241575\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -19.239159\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -19.236767\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.254400\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.271856\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.289137\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.306246\n",
            "resetting env. episode 30.000000, reward total was -18.000000. running mean: -19.293183\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.300251\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -19.307249\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.324176\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.330935\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.337625\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.354249\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -19.350707\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.367200\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.383528\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.399692\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.405695\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.421638\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.437422\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -19.443048\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.458617\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.474031\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.489291\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.504398\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -19.509354\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -19.504260\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.519218\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.534026\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -19.538685\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -19.533298\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.547966\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.562486\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -19.556861\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.571292\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.585579\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.599724\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -19.593726\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.607789\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.621711\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.635494\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.639139\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.652748\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.656220\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.669658\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.682962\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.696132\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.709171\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.712079\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.724958\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -19.717709\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.730531\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.743226\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.755794\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.768236\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.770554\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.782848\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.785020\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.797169\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.809198\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.821106\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.832895\n",
            "resetting env. episode 86.000000, reward total was -18.000000. running mean: -19.814566\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.826420\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -19.828156\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -19.829874\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.831576\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.843260\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.844827\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.856379\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.867815\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.879137\n",
            "resetting env. episode 96.000000, reward total was -18.000000. running mean: -19.860346\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.871742\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.883025\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.894194\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.905253\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -19.906200\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.907138\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.918067\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.928886\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.929597\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.940301\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.950898\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.961389\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -19.961775\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.972158\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.982436\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.992612\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.002685\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -19.992659\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.002732\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.012705\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.022578\n",
            "resetting env. episode 118.000000, reward total was -18.000000. running mean: -20.002352\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.002328\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.002305\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.012282\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.022159\n",
            "resetting env. episode 123.000000, reward total was -18.000000. running mean: -20.001938\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.001918\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.011899\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.021780\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.031562\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.041247\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.040834\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.050426\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.059922\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.069322\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.078629\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.087843\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.096964\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.105995\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.114935\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.113785\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.122648\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.121421\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.130207\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.118905\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.117716\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.126539\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.135273\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.123921\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.132681\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.131355\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.130041\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.118741\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.127553\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.136278\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.134915\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.133566\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.142230\n",
            "resetting env. episode 156.000000, reward total was -18.000000. running mean: -20.120808\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.119600\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.128404\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.137120\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.145748\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.154291\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.162748\n",
            "resetting env. episode 163.000000, reward total was -18.000000. running mean: -20.141121\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.149709\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.158212\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.166630\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.174964\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.183214\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.191382\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.199468\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.187474\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.195599\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.203643\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.211606\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.219490\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.227295\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.215022\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.212872\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.220744\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.228536\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.236251\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.243888\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.241449\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.239035\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.236645\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.234278\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.241935\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.249516\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.257021\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.264451\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.261806\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.269188\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.276496\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -20.253731\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.251194\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.248682\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.256195\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.263633\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.260997\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.268387\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.255703\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.263146\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.270514\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.267809\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.255131\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.252580\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.260054\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.267454\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.274779\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.282031\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.269211\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.276519\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.283754\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.270916\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.278207\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.285425\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.292571\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.279645\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.276848\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.284080\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.291239\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.288327\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.295444\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.302489\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.309464\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.306370\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.313306\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.320173\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.316971\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.323801\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.330563\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.317258\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -20.304085\n",
            "resetting env. episode 234.000000, reward total was -18.000000. running mean: -20.281044\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.278234\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.285452\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.282597\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.289771\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.296873\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.303905\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.310866\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.297757\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.304779\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.301732\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.308714\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.315627\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.312471\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.309346\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.316253\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.313090\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.319959\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.326760\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.333492\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.340157\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.346756\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.353288\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.359755\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.366158\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.372496\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.378771\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.374983\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.371233\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.377521\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.383746\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.389908\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.396009\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.402049\n",
            "resetting env. episode 268.000000, reward total was -18.000000. running mean: -20.378029\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.384248\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.390406\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.386502\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.392637\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.388711\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.394823\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.390875\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.396966\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.392997\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.399067\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.405076\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.411025\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.416915\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.422746\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.428519\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.434233\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.439891\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.435492\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.431137\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.436826\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.442458\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.438033\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.443653\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.449216\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.454724\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.450177\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.445675\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.441218\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.446806\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.442338\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.447915\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.453435\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.458901\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.444312\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.449869\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.445370\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.430917\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.436607\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.442241\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.437819\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.443441\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.449006\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.454516\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.459971\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.455371\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.460818\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.456209\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.461647\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.447031\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.452561\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -20.438035\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.443655\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.439218\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.444826\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.430378\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.426074\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.431813\n",
            "resetting env. episode 326.000000, reward total was -18.000000. running mean: -20.407495\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.413420\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.409286\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.415193\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.411041\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.406931\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.392861\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.378933\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.385143\n",
            "resetting env. episode 335.000000, reward total was -18.000000. running mean: -20.361292\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.367679\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.374002\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.370262\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.376560\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.362794\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -20.339166\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.345774\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.352317\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.348794\n",
            "resetting env. episode 345.000000, reward total was -18.000000. running mean: -20.325306\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.332053\n",
            "resetting env. episode 347.000000, reward total was -17.000000. running mean: -20.298732\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.305745\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.312687\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.319560\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.326365\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.333101\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.339770\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.346372\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.342909\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.329480\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.336185\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.342823\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.349395\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.345901\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.352442\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.358917\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.365328\n",
            "resetting env. episode 364.000000, reward total was -18.000000. running mean: -20.341675\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.348258\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.344776\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.341328\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.337915\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.344535\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.351090\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.347579\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.354103\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.360562\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.366957\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.373287\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.379554\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.375759\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.382001\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.388181\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.384299\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.390456\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.396552\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.402586\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.398560\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.404575\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.410529\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.416424\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.412259\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.418137\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.423956\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.429716\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.435419\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.431065\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.436754\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.442386\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.437963\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.433583\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.429247\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.424955\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.430705\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.436398\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.442034\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.447614\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.443138\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.448706\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.444219\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.449777\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.455279\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.440726\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.446319\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.451856\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.457337\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.462764\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.468136\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.473455\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.478720\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.473933\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.469194\n",
            "resetting env. episode 419.000000, reward total was -17.000000. running mean: -20.434502\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.440157\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.445755\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.451298\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.436785\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.432417\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.438093\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.443712\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.449275\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.454782\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.460234\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.465632\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.470976\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.456266\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.461703\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.467086\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.472415\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.477691\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.482914\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.488085\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.483204\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.478372\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.483588\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.488753\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.493865\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.498926\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.503937\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.498898\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.503909\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.488870\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.493981\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.499041\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.484051\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.479210\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.464418\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.469774\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -20.445076\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.440625\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.446219\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.451757\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.447239\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.442767\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.448339\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.433856\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.439517\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.445122\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.450671\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.446164\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.451703\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.447186\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.452714\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.458187\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.463605\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.458969\n",
            "resetting env. episode 473.000000, reward total was -18.000000. running mean: -20.434379\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.430035\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.425735\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.411478\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.397363\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.403389\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.409355\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.415262\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.421109\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.426898\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.422629\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.428403\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.434119\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.439778\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.445380\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.450926\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.446417\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.451953\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.457433\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.462859\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.468230\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.463548\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.468912\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.464223\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.459581\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.464985\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.470335\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.475632\n",
            "CPU times: user 23min 17s, sys: 10min 44s, total: 34min 1s\n",
            "Wall time: 17min 39s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "e805b6db-e72c-43f4-b685-e8e9687b11fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "411c1d70-010b-44a9-c1c1-4fe928eb4011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.039404\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.039010\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.038620\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.048234\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.057751\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.047174\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.056702\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.066135\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.075474\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.084719\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.093872\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.102933\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.101904\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.110885\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.119776\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.128578\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.137292\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.145919\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.154460\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.152916\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.151386\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.159873\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.168274\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.176591\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.164825\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.163177\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.161545\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.169930\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.168230\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.176548\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.164783\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.173135\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.181403\n",
            "resetting env. episode 38.000000, reward total was -18.000000. running mean: -20.159589\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.167994\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.176314\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.184550\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.192705\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.190778\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.198870\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.206881\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.204813\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.202764\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.210737\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.218629\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.226443\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.234179\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -20.221837\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.229619\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.237322\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.244949\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.252500\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.249975\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -20.237475\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.245100\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.252649\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.260123\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.257521\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.264946\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.272297\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.269574\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.266878\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.274209\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.281467\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.288653\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.295766\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.292808\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.289880\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.296981\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.304012\n",
            "resetting env. episode 75.000000, reward total was -18.000000. running mean: -20.280972\n",
            "resetting env. episode 76.000000, reward total was -18.000000. running mean: -20.258162\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.255580\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.263024\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.250394\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.247890\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.245411\n",
            "resetting env. episode 82.000000, reward total was -19.000000. running mean: -20.232957\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.220628\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.228421\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.226137\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.233876\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.231537\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.239222\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.236829\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.244461\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.252017\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.249496\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.257001\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.264431\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.271787\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -20.259069\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.266478\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.273814\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.281076\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.278265\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.275482\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.282727\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.289900\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.297001\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.304031\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.310991\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.297881\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.304902\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.311853\n",
            "resetting env. episode 110.000000, reward total was -18.000000. running mean: -20.288734\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.295847\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.282889\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.290060\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.297159\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.304188\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.301146\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.308134\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.315053\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.321902\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.318683\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.325497\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.332242\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.328919\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.315630\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.322474\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.329249\n",
            "resetting env. episode 127.000000, reward total was -18.000000. running mean: -20.305956\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.292897\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.299968\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.296968\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.303999\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.300959\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.307949\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.314869\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.321721\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.318504\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.325319\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.332065\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.338745\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.335357\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.342004\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.348584\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.335098\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.331747\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.328429\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.335145\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.341794\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.348376\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.344892\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.351443\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.357929\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.364349\n",
            "resetting env. episode 153.000000, reward total was -18.000000. running mean: -20.340706\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.347299\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.353826\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.360287\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.366685\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.373018\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -20.359288\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.355695\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.362138\n",
            "resetting env. episode 162.000000, reward total was -18.000000. running mean: -20.338516\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.325131\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.321880\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.328661\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.335375\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.332021\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.338701\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.335314\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.321960\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.328741\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.315453\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.322299\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.329076\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.335785\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.342427\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.339003\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.345613\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.352157\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.358635\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.355049\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.361498\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.347883\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.354405\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.350861\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.347352\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.333878\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.340540\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.347134\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.353663\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.360126\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.356525\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.342960\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -20.319530\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.326335\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.333072\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.339741\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.326343\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.333080\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.329749\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.336452\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.333087\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.339756\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.346359\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.342895\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.329466\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.326172\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.332910\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.329581\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.336285\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.342922\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.339493\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.326098\n",
            "resetting env. episode 214.000000, reward total was -18.000000. running mean: -20.302837\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.309809\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.316710\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.323543\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.330308\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.337005\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.333635\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.340298\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.346895\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.353427\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.359892\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.356293\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.362730\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.359103\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.365512\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.351857\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.348338\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.354855\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.361306\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.367693\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.374016\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.380276\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.386474\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.382609\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.378783\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.384995\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.371145\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.367433\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.373759\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.380022\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.386221\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.392359\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.398436\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.394451\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.390507\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.396602\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.392636\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.398709\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.404722\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.400675\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.406668\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.412601\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.408475\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.414391\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.420247\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.426044\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.421784\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.417566\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.423390\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.429156\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.424865\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.430616\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.436310\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.441947\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.447528\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.453052\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.458522\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.463937\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.459297\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.464704\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.470057\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.465357\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.470703\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.475996\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.481236\n",
            "resetting env. episode 279.000000, reward total was -18.000000. running mean: -20.456424\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.461859\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.457241\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.452668\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.458142\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.463560\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.468925\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.464235\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.469593\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.474897\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.480148\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.485347\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.490493\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -20.465588\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.470932\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.476223\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.481461\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.486646\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.491780\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.496862\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.501893\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.506874\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.501806\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.506788\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.511720\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.516603\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.501437\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.506422\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.511358\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.496244\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.481282\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.486469\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.491604\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.476688\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.481922\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.487102\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.482231\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.477409\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.462635\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.448009\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.443528\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.439093\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.444702\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.440255\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.425853\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.431594\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.437278\n",
            "resetting env. episode 326.000000, reward total was -18.000000. running mean: -20.412905\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.418776\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.414589\n",
            "resetting env. episode 329.000000, reward total was -16.000000. running mean: -20.370443\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.376738\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.382971\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.369141\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.365450\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.371795\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.358077\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.354497\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.360952\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.357342\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.363769\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.360131\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.366530\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.372864\n",
            "resetting env. episode 343.000000, reward total was -18.000000. running mean: -20.349136\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.355644\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.362088\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.358467\n",
            "resetting env. episode 347.000000, reward total was -18.000000. running mean: -20.334882\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.341534\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.328118\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.334837\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.341489\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.348074\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.344593\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.351147\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.357636\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.364059\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.370419\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.366714\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.373047\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.379317\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.375524\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.371768\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.358051\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.344470\n",
            "resetting env. episode 365.000000, reward total was -18.000000. running mean: -20.321026\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.317815\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.324637\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.331391\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.338077\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.344696\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.351249\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.347737\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.334259\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.340917\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.347508\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.354032\n",
            "resetting env. episode 377.000000, reward total was -17.000000. running mean: -20.320492\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.327287\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.324014\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.330774\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.317466\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.324292\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.321049\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.327838\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.334560\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.331214\n",
            "resetting env. episode 387.000000, reward total was -17.000000. running mean: -20.297902\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.304923\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.291874\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.298955\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.305966\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.302906\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.299877\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.306878\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.313809\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.310671\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.317565\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.324389\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.311145\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.318034\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.324853\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.331605\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.338289\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.334906\n",
            "resetting env. episode 405.000000, reward total was -18.000000. running mean: -20.311557\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.318441\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.325257\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.332004\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.338684\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.345297\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.351844\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.358326\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.364743\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.371095\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.367384\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.373710\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.379973\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.386174\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.392312\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.398389\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.404405\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.400361\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.396357\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.402394\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.408370\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.414286\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.410143\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.406042\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.411981\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.407861\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.413783\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.419645\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.425449\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.421194\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.416982\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.422812\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.408584\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.414498\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.420353\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.426150\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.431888\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.427569\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.433294\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.438961\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.444571\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.450126\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.455624\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.461068\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.466457\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.451793\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.447275\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.452802\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.458274\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.463691\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.459054\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.454464\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.459919\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.455320\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.460767\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.456159\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.451598\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.447082\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.442611\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.438185\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.443803\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.449365\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -20.424871\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.430622\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.426316\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.422053\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.417833\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.413654\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.419518\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.425322\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.411069\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.416959\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.422789\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.428561\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.434275\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.429933\n",
            "resetting env. episode 481.000000, reward total was -18.000000. running mean: -20.405633\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.411577\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.397461\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.393487\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.399552\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.385556\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.391701\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.397784\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.383806\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.369968\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.356268\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.362705\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.369078\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.375388\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.381634\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.377817\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.384039\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.380199\n",
            "resetting env. episode 499.000000, reward total was -18.000000. running mean: -20.356397\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.362833\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.369205\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.375513\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.381757\n",
            "resetting env. episode 504.000000, reward total was -19.000000. running mean: -20.367940\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.374260\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.380518\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.386713\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.382846\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.389017\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.385127\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.391276\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.397363\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.403389\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.409355\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.415262\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.411109\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.416998\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.422828\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.428600\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.434314\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.429971\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -20.425671\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.431414\n",
            "resetting env. episode 524.000000, reward total was -19.000000. running mean: -20.417100\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.422929\n",
            "resetting env. episode 526.000000, reward total was -18.000000. running mean: -20.398700\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.404713\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.410666\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.416559\n",
            "resetting env. episode 530.000000, reward total was -19.000000. running mean: -20.402393\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.408370\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.404286\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.410243\n",
            "resetting env. episode 534.000000, reward total was -19.000000. running mean: -20.396141\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.402179\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.408157\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.414076\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.419935\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.415736\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.421578\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.427363\n",
            "resetting env. episode 542.000000, reward total was -18.000000. running mean: -20.403089\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.409058\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.414967\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.410818\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.416710\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.422542\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.418317\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.424134\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.429893\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.435594\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.431238\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.436925\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.442556\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.448130\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -20.453649\n",
            "resetting env. episode 557.000000, reward total was -19.000000. running mean: -20.439113\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.444722\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.450274\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.455772\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.461214\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.466602\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.471936\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.467216\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.472544\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.477819\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.483041\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.488210\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.493328\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.498395\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.503411\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.508377\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.503293\n",
            "resetting env. episode 574.000000, reward total was -18.000000. running mean: -20.478260\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.483477\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.488643\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.493756\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.498819\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.503830\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.508792\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -20.503704\n",
            "resetting env. episode 582.000000, reward total was -19.000000. running mean: -20.488667\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.493781\n",
            "resetting env. episode 584.000000, reward total was -19.000000. running mean: -20.478843\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.484054\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.479214\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.484422\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.469577\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.464882\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.470233\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.475531\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.480775\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.475967\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.471208\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.476496\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.471731\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.477013\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.482243\n",
            "resetting env. episode 599.000000, reward total was -18.000000. running mean: -20.457421\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.462847\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.468218\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.473536\n",
            "resetting env. episode 603.000000, reward total was -19.000000. running mean: -20.458801\n",
            "resetting env. episode 604.000000, reward total was -19.000000. running mean: -20.444213\n",
            "resetting env. episode 605.000000, reward total was -19.000000. running mean: -20.429771\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.435473\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.441118\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.446707\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.442240\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.447817\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.443339\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.448906\n",
            "resetting env. episode 613.000000, reward total was -19.000000. running mean: -20.434417\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.430073\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.425772\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.421514\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.427299\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.433026\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.428696\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.434409\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.430065\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.425764\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.431506\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -20.417191\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.423019\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.428789\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.434501\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.430156\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.435855\n",
            "resetting env. episode 630.000000, reward total was -19.000000. running mean: -20.421496\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.427281\n",
            "resetting env. episode 632.000000, reward total was -19.000000. running mean: -20.413008\n",
            "resetting env. episode 633.000000, reward total was -18.000000. running mean: -20.388878\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.384990\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.391140\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.397228\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.393256\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.399323\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.405330\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.411277\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.417164\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.422993\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.418763\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.424575\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.430329\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.436026\n",
            "resetting env. episode 647.000000, reward total was -20.000000. running mean: -20.431666\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.437349\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.442976\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.448546\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.454060\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.459520\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.464925\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.460275\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.465673\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.461016\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.466406\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -20.461742\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.457124\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.462553\n",
            "resetting env. episode 661.000000, reward total was -18.000000. running mean: -20.437927\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.433548\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.439213\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.444821\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.440372\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.445969\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.451509\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.456994\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.452424\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.457900\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -20.443321\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.438887\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.434499\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.440154\n",
            "resetting env. episode 675.000000, reward total was -19.000000. running mean: -20.425752\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.421495\n",
            "resetting env. episode 677.000000, reward total was -19.000000. running mean: -20.407280\n",
            "resetting env. episode 678.000000, reward total was -20.000000. running mean: -20.403207\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.409175\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.415083\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -20.410932\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.416823\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.422655\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.428428\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.434144\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.439802\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.435404\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.441050\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.446640\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.442173\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -20.427752\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.433474\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.439139\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -20.434748\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.440400\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.445996\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.451537\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.457021\n",
            "resetting env. episode 699.000000, reward total was -20.000000. running mean: -20.452451\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.447926\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -20.443447\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.439013\n",
            "resetting env. episode 703.000000, reward total was -19.000000. running mean: -20.424623\n",
            "resetting env. episode 704.000000, reward total was -19.000000. running mean: -20.410376\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.406273\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -20.402210\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.398188\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.404206\n",
            "resetting env. episode 709.000000, reward total was -20.000000. running mean: -20.400164\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.406162\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.402101\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.408080\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.413999\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.419859\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.425660\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.431404\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.437090\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.432719\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.438391\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.444008\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.449567\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.445072\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.450621\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.456115\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.461554\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.456938\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.462369\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.467745\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.473068\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.478337\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.483554\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -20.478718\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.483931\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.489092\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.484201\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.479359\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.484565\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -20.479719\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.484922\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.490073\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.495172\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.490221\n",
            "resetting env. episode 743.000000, reward total was -19.000000. running mean: -20.475318\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.470565\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.475860\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.481101\n",
            "resetting env. episode 747.000000, reward total was -20.000000. running mean: -20.476290\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.481527\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -20.476712\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.481945\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.487125\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.492254\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.497331\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.492358\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -20.497434\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.492460\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.497536\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -20.492560\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.487635\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.492758\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.497831\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.502852\n",
            "resetting env. episode 763.000000, reward total was -18.000000. running mean: -20.477824\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.483046\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.488215\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.493333\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.498400\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -20.493416\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.498482\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.493497\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.488562\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.483676\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.478839\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.484051\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.489210\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.494318\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.499375\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.504381\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.509338\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.514244\n",
            "resetting env. episode 781.000000, reward total was -19.000000. running mean: -20.499102\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.494111\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.489170\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.494278\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -20.479335\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.474542\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.479796\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.474998\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -20.460248\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.455646\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.461090\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.456479\n",
            "resetting env. episode 793.000000, reward total was -19.000000. running mean: -20.441914\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -20.437495\n",
            "resetting env. episode 795.000000, reward total was -20.000000. running mean: -20.433120\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.438789\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.434401\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.430057\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.435756\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.441399\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.436985\n",
            "resetting env. episode 802.000000, reward total was -19.000000. running mean: -20.422615\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.428389\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.424105\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.429864\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.435565\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.441209\n",
            "resetting env. episode 808.000000, reward total was -20.000000. running mean: -20.436797\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.442429\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.448005\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.443525\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.449090\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.454599\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.460053\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.455452\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.460898\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.466289\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.471626\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.466910\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.462241\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -20.457618\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.463042\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.468412\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.463727\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.459090\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.464499\n",
            "resetting env. episode 827.000000, reward total was -19.000000. running mean: -20.449854\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.455356\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.460802\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.466194\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.471532\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.476817\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.482049\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.477228\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.482456\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.487631\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.482755\n",
            "resetting env. episode 838.000000, reward total was -20.000000. running mean: -20.477927\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.483148\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.478317\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.483534\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.488698\n",
            "resetting env. episode 843.000000, reward total was -19.000000. running mean: -20.473811\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -20.469073\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.474382\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.479639\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.484842\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.489994\n",
            "resetting env. episode 849.000000, reward total was -19.000000. running mean: -20.475094\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.480343\n",
            "resetting env. episode 851.000000, reward total was -19.000000. running mean: -20.465539\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.460884\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -20.456275\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.451712\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.457195\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.462623\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.467997\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.463317\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.468684\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.463997\n",
            "resetting env. episode 861.000000, reward total was -19.000000. running mean: -20.449357\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.444864\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.450415\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.455911\n",
            "resetting env. episode 865.000000, reward total was -19.000000. running mean: -20.441352\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.446938\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.452469\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.457944\n",
            "resetting env. episode 869.000000, reward total was -20.000000. running mean: -20.453365\n",
            "resetting env. episode 870.000000, reward total was -17.000000. running mean: -20.418831\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.414643\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.410496\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.416391\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.422227\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.428005\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.433725\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.439388\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.444994\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.450544\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.456039\n",
            "resetting env. episode 881.000000, reward total was -18.000000. running mean: -20.431478\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.437163\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.432792\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.428464\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -20.424179\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.429937\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.425638\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.431382\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -20.427068\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.432797\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.438469\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -20.434085\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.429744\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.435446\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.441092\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.446681\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.452214\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.457692\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.463115\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -20.458484\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.463899\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.459260\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.464667\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.460021\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.465421\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.470766\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.466059\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.471398\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.476684\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.471917\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.467198\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.472526\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.477801\n",
            "resetting env. episode 914.000000, reward total was -19.000000. running mean: -20.463023\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.468393\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.473709\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -20.468972\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.474282\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -20.469539\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.474844\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.480095\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.485294\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.490441\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.495537\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.500582\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -20.495576\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -20.490620\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.495714\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.490757\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.495849\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.500891\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.505882\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.510823\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.515715\n",
            "resetting env. episode 935.000000, reward total was -19.000000. running mean: -20.500558\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.505552\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.510496\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.505391\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -20.490338\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -20.475434\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.480680\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.485873\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -20.481014\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.486204\n",
            "resetting env. episode 945.000000, reward total was -18.000000. running mean: -20.461342\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.466729\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.472061\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.477341\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.482567\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -20.477742\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.482964\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.488135\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.493253\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.498321\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.503338\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.508304\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.513221\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -20.508089\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -20.503008\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.507978\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.512898\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -20.507769\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.502691\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.507665\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.492588\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.487662\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.492785\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.497858\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.502879\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.507850\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.512772\n",
            "resetting env. episode 972.000000, reward total was -18.000000. running mean: -20.487644\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.492768\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.487840\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.492961\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.498032\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.493052\n",
            "resetting env. episode 978.000000, reward total was -19.000000. running mean: -20.478121\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.483340\n",
            "resetting env. episode 980.000000, reward total was -19.000000. running mean: -20.468506\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -20.463821\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.469183\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.474491\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.479746\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.484949\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -20.470099\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.475398\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.470644\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.475938\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.481179\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.476367\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.481603\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.486787\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.491919\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -20.487000\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.492130\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.497209\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.492237\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -20.487314\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.492441\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.487517\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.492642\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.497715\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.502738\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.497711\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.502734\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.507706\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.512629\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.507503\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.512428\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.517304\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.512131\n",
            "resetting env. episode 1013.000000, reward total was -20.000000. running mean: -20.507009\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.511939\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.516820\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.521652\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.526435\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.531171\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.535859\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.540500\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.535095\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.539744\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.544347\n",
            "resetting env. episode 1024.000000, reward total was -20.000000. running mean: -20.538904\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -20.533514\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.538179\n",
            "resetting env. episode 1027.000000, reward total was -18.000000. running mean: -20.512798\n",
            "resetting env. episode 1028.000000, reward total was -20.000000. running mean: -20.507670\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.512593\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.517467\n",
            "resetting env. episode 1031.000000, reward total was -19.000000. running mean: -20.502292\n",
            "resetting env. episode 1032.000000, reward total was -18.000000. running mean: -20.477269\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.472497\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.467772\n",
            "resetting env. episode 1035.000000, reward total was -18.000000. running mean: -20.443094\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.448663\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.454176\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.459635\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.465038\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.470388\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.475684\n",
            "resetting env. episode 1042.000000, reward total was -19.000000. running mean: -20.460927\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.466318\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.471655\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.476938\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -20.462169\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.467547\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -20.462872\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.468243\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -20.463560\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.458925\n",
            "resetting env. episode 1052.000000, reward total was -19.000000. running mean: -20.444336\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -20.439892\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -20.435493\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.441138\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.446727\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -20.432260\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -20.427937\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -20.423658\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.429421\n",
            "resetting env. episode 1061.000000, reward total was -19.000000. running mean: -20.415127\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.420976\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.426766\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.412498\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.418373\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.424190\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.429948\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.435648\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.431292\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.436979\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.442609\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.448183\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.443701\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.449264\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.444771\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.450324\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.455821\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.461262\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.466650\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.471983\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -20.457263\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.462691\n",
            "resetting env. episode 1083.000000, reward total was -19.000000. running mean: -20.448064\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.453583\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.459047\n",
            "resetting env. episode 1086.000000, reward total was -20.000000. running mean: -20.454457\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -20.449912\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.455413\n",
            "resetting env. episode 1089.000000, reward total was -21.000000. running mean: -20.460859\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.466250\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.471588\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.456872\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.462303\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.467680\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.463004\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.468373\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -20.463690\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.469053\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -20.464362\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.469719\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.475022\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.480271\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.485469\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.490614\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.495708\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.500751\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.495743\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.500786\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.505778\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.510720\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.515613\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.520457\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.525252\n",
            "resetting env. episode 1114.000000, reward total was -19.000000. running mean: -20.510000\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.514900\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.519751\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.524553\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.529308\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.534015\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.528674\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -20.523388\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.528154\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.532872\n",
            "resetting env. episode 1124.000000, reward total was -18.000000. running mean: -20.507544\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.502468\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -20.497443\n",
            "resetting env. episode 1127.000000, reward total was -20.000000. running mean: -20.492469\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.497544\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.502569\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -20.497543\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.502568\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.507542\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.512467\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.517342\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.512169\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.507047\n",
            "resetting env. episode 1137.000000, reward total was -20.000000. running mean: -20.501976\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.506957\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.501887\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.506868\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.501800\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -20.496782\n",
            "resetting env. episode 1143.000000, reward total was -19.000000. running mean: -20.481814\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -20.486996\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.492126\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.497204\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -20.492232\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.487310\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.492437\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.487513\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -20.482637\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.487811\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.492933\n",
            "resetting env. episode 1154.000000, reward total was -20.000000. running mean: -20.488004\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -20.483124\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.488292\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.493409\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.498475\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -20.503491\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.508456\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.513371\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.518237\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.523055\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.517824\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.522646\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.527420\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.532146\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -20.526824\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.531556\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.536240\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -20.530878\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.525569\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.530313\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.525010\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.529760\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.534463\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -20.529118\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -20.523827\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.528589\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.533303\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.537970\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -20.522590\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.527364\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.532090\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.536769\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.541402\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.545988\n",
            "resetting env. episode 1188.000000, reward total was -18.000000. running mean: -20.520528\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -20.515323\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -20.510169\n",
            "resetting env. episode 1191.000000, reward total was -19.000000. running mean: -20.495068\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.500117\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.505116\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.510065\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.514964\n",
            "resetting env. episode 1196.000000, reward total was -19.000000. running mean: -20.499814\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.504816\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.509768\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -20.504670\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.509624\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.514527\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.519382\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.524188\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -20.518946\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.523757\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.528519\n",
            "resetting env. episode 1207.000000, reward total was -20.000000. running mean: -20.523234\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.528002\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.532722\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -20.527395\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.532121\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -20.526800\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -20.521532\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.526316\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -20.521053\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.515843\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.520684\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.525477\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.530222\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.534920\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.539571\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.544175\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -20.538734\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.543346\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.547913\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.552434\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.556909\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -20.551340\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.555827\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.560269\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.564666\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -20.569019\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.563329\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.567696\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.572019\n",
            "resetting env. episode 1236.000000, reward total was -19.000000. running mean: -20.556299\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.560736\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.565128\n",
            "resetting env. episode 1239.000000, reward total was -19.000000. running mean: -20.549477\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.553982\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -20.548442\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.552958\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.557428\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.551854\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.556336\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -20.550772\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.555264\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.559712\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.564115\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.568474\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.562789\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.567161\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.571489\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.575774\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.580017\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.584217\n",
            "resetting env. episode 1257.000000, reward total was -19.000000. running mean: -20.568374\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.572691\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.576964\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.571194\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.575482\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.579727\n",
            "resetting env. episode 1263.000000, reward total was -20.000000. running mean: -20.573930\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -20.558191\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.552609\n",
            "resetting env. episode 1266.000000, reward total was -20.000000. running mean: -20.547083\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.551612\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.546096\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -20.540635\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.545228\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.549776\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.554278\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.558736\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -20.553148\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.557617\n",
            "resetting env. episode 1276.000000, reward total was -19.000000. running mean: -20.542041\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.546620\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.551154\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.545642\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.540186\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.544784\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.549336\n",
            "resetting env. episode 1283.000000, reward total was -20.000000. running mean: -20.543843\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.548405\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.542921\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.547491\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -20.542016\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.536596\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.541230\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.545818\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -20.540360\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.544956\n",
            "resetting env. episode 1293.000000, reward total was -19.000000. running mean: -20.529507\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.534212\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.538869\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.543481\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.548046\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -20.532565\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.537240\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.541867\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -20.536449\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.531084\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.535773\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.540416\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.545012\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.549561\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -20.544066\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.548625\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.553139\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.557608\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.562031\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.566411\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.570747\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.575040\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.569289\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.573596\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.577860\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.582082\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.586261\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.590398\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.594494\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.598549\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -20.592564\n",
            "resetting env. episode 1324.000000, reward total was -19.000000. running mean: -20.576638\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.570872\n",
            "resetting env. episode 1326.000000, reward total was -19.000000. running mean: -20.555163\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.559611\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.564015\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.568375\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -20.562691\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.567065\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.571394\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.575680\n",
            "resetting env. episode 1334.000000, reward total was -20.000000. running mean: -20.569923\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.574224\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.578482\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.582697\n",
            "resetting env. episode 1338.000000, reward total was -19.000000. running mean: -20.566870\n",
            "resetting env. episode 1339.000000, reward total was -19.000000. running mean: -20.551201\n",
            "resetting env. episode 1340.000000, reward total was -19.000000. running mean: -20.535689\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.540332\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.544929\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.549480\n",
            "resetting env. episode 1344.000000, reward total was -19.000000. running mean: -20.533985\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.538645\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.533259\n",
            "resetting env. episode 1347.000000, reward total was -18.000000. running mean: -20.507926\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -20.492847\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.497918\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.502939\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.507910\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.512831\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -20.507702\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.512625\n",
            "resetting env. episode 1355.000000, reward total was -17.000000. running mean: -20.477499\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.482724\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.477897\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.483118\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.488287\n",
            "resetting env. episode 1360.000000, reward total was -19.000000. running mean: -20.473404\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.468670\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.473983\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.479243\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.484451\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.489606\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.494710\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.499763\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.504765\n",
            "resetting env. episode 1369.000000, reward total was -19.000000. running mean: -20.489718\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.494821\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.499872\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.504874\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.499825\n",
            "resetting env. episode 1374.000000, reward total was -19.000000. running mean: -20.484827\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.489978\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.485079\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.490228\n",
            "resetting env. episode 1378.000000, reward total was -19.000000. running mean: -20.475326\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -20.470572\n",
            "resetting env. episode 1380.000000, reward total was -17.000000. running mean: -20.435867\n",
            "resetting env. episode 1381.000000, reward total was -21.000000. running mean: -20.441508\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.447093\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -20.442622\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.438196\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.443814\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.449376\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.454882\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.460333\n",
            "resetting env. episode 1389.000000, reward total was -18.000000. running mean: -20.435730\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.441372\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.446959\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.452489\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.457964\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.453385\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.448851\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.454362\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.459819\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.465220\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -20.460568\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.465963\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.461303\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.466690\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.472023\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.477303\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.482530\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.487704\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -20.482827\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.487999\n",
            "resetting env. episode 1409.000000, reward total was -18.000000. running mean: -20.463119\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.468488\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -20.463803\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.469165\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.474473\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.469729\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.475031\n",
            "resetting env. episode 1416.000000, reward total was -17.000000. running mean: -20.440281\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -20.435878\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.441519\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.447104\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.442633\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -20.448207\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.453725\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.459188\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.464596\n",
            "resetting env. episode 1425.000000, reward total was -19.000000. running mean: -20.449950\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.455450\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.460896\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.466287\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.461624\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -20.457008\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.462438\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.467813\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.473135\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -20.468404\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -20.463720\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.469082\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.474392\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.479648\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -20.474851\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.480103\n",
            "resetting env. episode 1441.000000, reward total was -19.000000. running mean: -20.465302\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.470649\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.475942\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -20.481183\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.486371\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.491507\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.496592\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -20.491626\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.496710\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.491743\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.496825\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.501857\n",
            "resetting env. episode 1453.000000, reward total was -19.000000. running mean: -20.486839\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -20.481970\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.487151\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.492279\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.497356\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -20.492383\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.497459\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.502484\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -20.497459\n",
            "resetting env. episode 1462.000000, reward total was -19.000000. running mean: -20.482485\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -20.477660\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.482883\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -20.478055\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.483274\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.488441\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -20.483557\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -20.478721\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -20.473934\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.479195\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.484403\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -20.489559\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.494663\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -20.489717\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -20.484819\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.489971\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -20.495071\n",
            "resetting env. episode 1479.000000, reward total was -20.000000. running mean: -20.490121\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -20.485220\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.490367\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -20.485464\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.490609\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.495703\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -20.490746\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.495838\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.500880\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.505871\n",
            "resetting env. episode 1489.000000, reward total was -19.000000. running mean: -20.490813\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -20.485904\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -20.481045\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -20.476235\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.481473\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.486658\n",
            "resetting env. episode 1495.000000, reward total was -18.000000. running mean: -20.461791\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -20.457173\n",
            "resetting env. episode 1497.000000, reward total was -18.000000. running mean: -20.432602\n",
            "resetting env. episode 1498.000000, reward total was -19.000000. running mean: -20.418276\n",
            "resetting env. episode 1499.000000, reward total was -19.000000. running mean: -20.404093\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -20.400052\n",
            "CPU times: user 1h 8min 41s, sys: 31min 32s, total: 1h 40min 13s\n",
            "Wall time: 51min 41s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "w2NblmwDsL3y",
        "outputId": "14438e7f-99ac-4229-b66b-f3ccf14a9ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "iteration_7_h=200_1e_6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}