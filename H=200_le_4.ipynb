{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VenkataKari/BigDataStrategies-Test2Part1/blob/main/H%3D200_le_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "7abb3f37-5960-4345-c109-343e731de945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=170c71276c5fb27f8521c77089f16b75d81af624f296fc6e90800db1409cb869\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "6db1ce32-4314-4792-e1d5-b7f627c25998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "d0912a1d-0660-4ba6-f02e-2e617f28c69e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "8c90bfbd-a357-4ecc-d6a9-a3fe1555dff5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "b2ea208b-ae22-4779-ab11-107ddd8f2ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "09d9e759-9e02-48fc-9ed5-3509f5c4293a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.970299\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.970596\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.970890\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.971181\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.971469\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.961755\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.952137\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.952616\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.953090\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.953559\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.944023\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.944583\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.945137\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.945686\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.946229\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.936767\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.937399\n",
            "resetting env. episode 23.000000, reward total was -18.000000. running mean: -20.908025\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.908945\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.909855\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.900757\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.901749\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.882732\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.883904\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.885065\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.886215\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -20.857352\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.838779\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.840391\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.841987\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.833567\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -20.815232\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.817079\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.808909\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.810819\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.812711\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -20.794584\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.796638\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.798672\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.800685\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.782678\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.764852\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.747203\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.749731\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.752234\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.754711\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.747164\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.749693\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.742196\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.734774\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.737426\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.740052\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.742651\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.745225\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.747772\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.740295\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.742892\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.745463\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.748008\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.740528\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.743123\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.745692\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.738235\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.740852\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.733444\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.736109\n",
            "resetting env. episode 72.000000, reward total was -18.000000. running mean: -20.708748\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.711661\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.714544\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.707399\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.690325\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.693422\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.696487\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.689522\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.692627\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.695701\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.698744\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.681757\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.684939\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.688090\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.691209\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.694297\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.697354\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -20.680380\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.673576\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.676841\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.680072\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.673271\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.676539\n",
            "resetting env. episode 95.000000, reward total was -18.000000. running mean: -20.649773\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.653276\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.656743\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.660175\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.663574\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.666938\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.660268\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.653666\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.637129\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.640758\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.644350\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.647907\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.641428\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.645013\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.648563\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.652078\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.655557\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.639001\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.642611\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.646185\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.649723\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.653226\n",
            "resetting env. episode 117.000000, reward total was -18.000000. running mean: -20.626694\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.610427\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.614323\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.618179\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.621998\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.625778\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.619520\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.603325\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.597291\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.601319\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.595305\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.599352\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.603359\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.607325\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.611252\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.605139\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.589088\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.593197\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.597265\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.601292\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.605280\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.599227\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.603235\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.607202\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.611130\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.605019\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.598969\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.602979\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.606949\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.600880\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.604871\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.608822\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.612734\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.616607\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.610441\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.614336\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.608193\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.612111\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.615990\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.619830\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.623632\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.627395\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.631121\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.634810\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.628462\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.632177\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.635856\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.639497\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.643102\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.646671\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.640204\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.643802\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.647364\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.650891\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.654382\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.637838\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.621460\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.625245\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.618992\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.622803\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.606575\n",
            "resetting env. episode 178.000000, reward total was -16.000000. running mean: -20.560509\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.544904\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.549455\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.553960\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.558420\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.562836\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.557208\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.551636\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.536119\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.540758\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.545351\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.549897\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.554398\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.548854\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.553366\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.537832\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.532454\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.527129\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.531858\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.536539\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.521174\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.525962\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.520703\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.515496\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.510341\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.495237\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.500285\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.485282\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.470429\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.475725\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.480968\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.486158\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.491296\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.496383\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.501420\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.506405\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.501341\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.496328\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.501365\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.506351\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.511287\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.516175\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.511013\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.515903\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.520744\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.525536\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.520281\n",
            "resetting env. episode 225.000000, reward total was -16.000000. running mean: -20.475078\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.480327\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.475524\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.480769\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.475961\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.481201\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.466389\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.461726\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.467108\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.472437\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.467713\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.463036\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.448405\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.453921\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -20.439382\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.434988\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.430638\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.426332\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.432069\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.437748\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.433371\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.439037\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.444646\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.440200\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.445798\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.451340\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.456827\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.462258\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.467636\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.472959\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.468230\n",
            "resetting env. episode 256.000000, reward total was -17.000000. running mean: -20.433548\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.429212\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.424920\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.420671\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.426464\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.432199\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.437877\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.443499\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.449064\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -20.434573\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.420227\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.426025\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.421765\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.427547\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.433272\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.438939\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.444550\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.450104\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.445603\n",
            "resetting env. episode 275.000000, reward total was -18.000000. running mean: -20.421147\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.426935\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.432666\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.428339\n",
            "resetting env. episode 279.000000, reward total was -18.000000. running mean: -20.404056\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.410015\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.415915\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.411756\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.417639\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.413462\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.419328\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.425134\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.420883\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.426674\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.432407\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.418083\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.423903\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.429663\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.415367\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.411213\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.417101\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.412930\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.418801\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.404613\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.410567\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.416461\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.422296\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.428073\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.433793\n",
            "resetting env. episode 304.000000, reward total was -18.000000. running mean: -20.409455\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.415360\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.411207\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.407095\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.413024\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.418893\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.414704\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.410557\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.416452\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.412287\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.418164\n",
            "resetting env. episode 315.000000, reward total was -17.000000. running mean: -20.383983\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.390143\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.396241\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.382279\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.388456\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.374572\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.380826\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.387018\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.373148\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.379416\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.385622\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.391766\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.397848\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.403870\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.399831\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.405833\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.401774\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.407756\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.403679\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.399642\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.395646\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.401689\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.397672\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.403696\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.409659\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.415562\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.421406\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.427192\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.422920\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.428691\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.424404\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.420160\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.405959\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.411899\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.417780\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.423602\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.429366\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.425073\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.430822\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.436514\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.432149\n",
            "resetting env. episode 356.000000, reward total was -18.000000. running mean: -20.407827\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.403749\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.399711\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.395714\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.401757\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.407740\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.413662\n",
            "resetting env. episode 363.000000, reward total was -16.000000. running mean: -20.369525\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.365830\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.362172\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.348550\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.345065\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.341614\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.328198\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.334916\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.331567\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.328251\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.324969\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.331719\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.328402\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.325118\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.331867\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.338548\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.335162\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.341811\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.348393\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.354909\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.361360\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.367746\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.374069\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.360328\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.356725\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.363157\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.369526\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.375831\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.382072\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.378252\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.374469\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.380724\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.386917\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.393048\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.389117\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.395226\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.401274\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.407261\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.413189\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.419057\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.414866\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.420718\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.426510\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.422245\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.418023\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.413843\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.409704\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.415607\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.421451\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.417237\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.403064\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.409034\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.414943\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.400794\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.396786\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.382818\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.378990\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.385200\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.391348\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.397434\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.403460\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.399425\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.385431\n",
            "resetting env. episode 426.000000, reward total was -18.000000. running mean: -20.361577\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.367961\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.374281\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.380539\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.376733\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.382966\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.379136\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.375345\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.381591\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.387776\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.393898\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.389959\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.396059\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.402099\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.398078\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.394097\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.390156\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.396254\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.392292\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.398369\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.404385\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.390341\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.396438\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.402474\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.398449\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.394464\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.390520\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.396615\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.402648\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.398622\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.404636\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.410589\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.416483\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.422319\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.408095\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.414014\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.399874\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.395876\n",
            "resetting env. episode 464.000000, reward total was -18.000000. running mean: -20.371917\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.378198\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.384416\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.380571\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.366766\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.373098\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.379367\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.385573\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.391718\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.397801\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.403823\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.399784\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.405786\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.411729\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -20.387611\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.393735\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.389798\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.395900\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.401941\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.397921\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.403942\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.409903\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.415804\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.401646\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.407629\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.403553\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.409517\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.405422\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.401368\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.407354\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.413281\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.409148\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.415057\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.410906\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.416797\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.422629\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.428403\n",
            "CPU times: user 23min 11s, sys: 10min 42s, total: 33min 53s\n",
            "Wall time: 17min 41s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "d3f00a1a-5a8d-4402-af39-35a572e4f456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.029900\n",
            "resetting env. episode 4.000000, reward total was -19.000000. running mean: -19.029601\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.049305\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.068812\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -19.078124\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.087343\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.106469\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.125404\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.144150\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.162709\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.181082\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.199271\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.217278\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -19.215106\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -19.212954\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.230825\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.248517\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.266032\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.283371\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -19.290537\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.307632\n",
            "resetting env. episode 24.000000, reward total was -18.000000. running mean: -19.294556\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -19.291610\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.308694\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -19.315607\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -19.312451\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.329327\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.346033\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.362573\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.378947\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.395158\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.411206\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.427094\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.442823\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -19.448395\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.463911\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.479272\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.494479\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -19.489534\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -19.484639\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -19.479793\n",
            "resetting env. episode 44.000000, reward total was -18.000000. running mean: -19.464995\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -19.470345\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -19.475641\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.490885\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.505976\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.520916\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.535707\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.540350\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.554947\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.569397\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.583703\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.597866\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -19.591887\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.595969\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.610009\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.623909\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.637670\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.651293\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.664780\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -19.668132\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -19.671451\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.684736\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.697889\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.700910\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.713901\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.716762\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.729594\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.732299\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.734976\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.747626\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.750150\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.762648\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.765022\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -19.767371\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -19.759698\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -19.752101\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.754580\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.767034\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.779364\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.791570\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.803654\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.805618\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.817561\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.819386\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -19.821192\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.832980\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.844650\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.846204\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.847742\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.849264\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.860772\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.872164\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.883442\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.894608\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.905662\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.916605\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.927439\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -19.918165\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.928983\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.929693\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.940396\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.950992\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.961482\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.971868\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.972149\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -19.962427\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.972803\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.983075\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.993244\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.993312\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -19.983379\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.993545\n",
            "resetting env. episode 116.000000, reward total was -18.000000. running mean: -19.973610\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.983873\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.994035\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.994094\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.004153\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.014112\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.023971\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.033731\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.033394\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.043060\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.032629\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.032303\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.041980\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.041560\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.031145\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.030833\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.030525\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.020220\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.020017\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.009817\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.009719\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.019622\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.019426\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.029231\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.018939\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.028750\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.028462\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.038177\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.037796\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.047418\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.056944\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.056374\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.045810\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.055352\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.064799\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.064151\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.063509\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.072874\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.082145\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.091324\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.100411\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.109407\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.118313\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.127129\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.125858\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.134600\n",
            "resetting env. episode 162.000000, reward total was -18.000000. running mean: -20.113254\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.112121\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.121000\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.129790\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.128492\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.127207\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.125935\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.124676\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.133429\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.132095\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.130774\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.139466\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.138071\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.146691\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.135224\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.143871\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.142433\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.151008\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.159498\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.167903\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.156224\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.164662\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.173015\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.181285\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.179472\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.187678\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.185801\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.183943\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.192103\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.190182\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.198281\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.196298\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.204335\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.202291\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.210269\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.198166\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.196184\n",
            "resetting env. episode 199.000000, reward total was -18.000000. running mean: -20.174222\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.182480\n",
            "resetting env. episode 201.000000, reward total was -18.000000. running mean: -20.160655\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.159049\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.167458\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.175784\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.184026\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.192186\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.200264\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.208261\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.216178\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.214017\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.221877\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.209658\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.217561\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.205386\n",
            "resetting env. episode 215.000000, reward total was -19.000000. running mean: -20.193332\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.201398\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.209384\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.207291\n",
            "resetting env. episode 219.000000, reward total was -18.000000. running mean: -20.185218\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.193365\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.191432\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.189518\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.197622\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.205646\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.213590\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.221454\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.219239\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.217047\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.224876\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.222628\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.230401\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.228097\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.235816\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.243458\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.241024\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.238613\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.236227\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.243865\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.251426\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.258912\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.266323\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.253660\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.251123\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.248612\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.256126\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.263564\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.250929\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.238420\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.226035\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.223775\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.221537\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.229322\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.237029\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.234658\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.222312\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.230089\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.227788\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.225510\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.223255\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.231022\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.238712\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.246325\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.233862\n",
            "resetting env. episode 264.000000, reward total was -18.000000. running mean: -20.211523\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.209408\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.207314\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.215241\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.213088\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.210957\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.218848\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.226659\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.234393\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.232049\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.219728\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.227531\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.235256\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.232903\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.230574\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.218268\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.226086\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.233825\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.231487\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.239172\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.236780\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.234412\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.232068\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.239747\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.247350\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.254876\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.262328\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.269704\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.277007\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.284237\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.281395\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.288581\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.275695\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.282938\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.270109\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.277408\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.284634\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.291787\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.298869\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.305881\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.302822\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.289794\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.286896\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.284027\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.281187\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.268375\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.275691\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.282934\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.290105\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.297204\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.294232\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.291289\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.298376\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.305393\n",
            "resetting env. episode 318.000000, reward total was -18.000000. running mean: -20.282339\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.289515\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.296620\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.303654\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.300617\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.297611\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.294635\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.301689\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.308672\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.305585\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.312529\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.319404\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.316210\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.323048\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.329817\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.336519\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.333154\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.329822\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.316524\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.323359\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.320125\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.316924\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.313755\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.310617\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.307511\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.314436\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.321292\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.328079\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.334798\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.341450\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.338036\n",
            "resetting env. episode 349.000000, reward total was -18.000000. running mean: -20.314655\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.321509\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.318294\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.325111\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.331860\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.328541\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.315256\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.322103\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.328882\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.325593\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.332337\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.339014\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.345624\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.342167\n",
            "resetting env. episode 363.000000, reward total was -18.000000. running mean: -20.318746\n",
            "resetting env. episode 364.000000, reward total was -18.000000. running mean: -20.295558\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.282603\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.289777\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.296879\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.303910\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.310871\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.297762\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.294785\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.301837\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.308818\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.305730\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.312673\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.309546\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.296451\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.293486\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.300551\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.297546\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.304570\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.311525\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.308410\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.315325\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.322172\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.328950\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.315661\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.302504\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.299479\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.306484\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.313420\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.300285\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.307283\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.314210\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.301068\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.298057\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.305076\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.302026\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.309005\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.315915\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.312756\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.309629\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.316532\n",
            "resetting env. episode 404.000000, reward total was -19.000000. running mean: -20.303367\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.310333\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.317230\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.324058\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.330817\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.337509\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.344134\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.340693\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.347286\n",
            "resetting env. episode 413.000000, reward total was -18.000000. running mean: -20.323813\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.330575\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.337269\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.333896\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.340557\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.337152\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.343780\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.350342\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.356839\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.363271\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.359638\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.366041\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.372381\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.378657\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.384871\n",
            "resetting env. episode 428.000000, reward total was -17.000000. running mean: -20.351022\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.347512\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.354037\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.360496\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.366891\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.353222\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.359690\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.356093\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.362532\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.358907\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.355318\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.361765\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.368147\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.354466\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.360921\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.367312\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.373639\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.359902\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.356303\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.362740\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.359113\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.365522\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.361866\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.348248\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.344765\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.341318\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.327904\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.334625\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.341279\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.337866\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.344488\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.341043\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.347632\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.354156\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.360615\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.367008\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.373338\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.379605\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.385809\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.381951\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.388131\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.384250\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.380407\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.376603\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.382837\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.389009\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.375119\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.381368\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.387554\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.383678\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.379842\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.376043\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.372283\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.368560\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.364874\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.371226\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.367513\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.373838\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.370100\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.366399\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.372735\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.379008\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.385217\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.391365\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.397452\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.383477\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.379642\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.375846\n",
            "resetting env. episode 496.000000, reward total was -18.000000. running mean: -20.352087\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.358567\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.344981\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.351531\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.358016\n",
            "CPU times: user 23min 23s, sys: 10min 41s, total: 34min 5s\n",
            "Wall time: 17min 35s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "8c767bd8-eb74-4dd2-8c06-976ba35b3721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "c56f4dda-5663-4ceb-a98d-bb0e248f7678"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.039404\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.039010\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.038620\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.048234\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.057751\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.057174\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.056602\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.066036\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.055376\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.054822\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.064274\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.063631\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.072995\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.082265\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.081442\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.080628\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.079821\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.079023\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.078233\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.077451\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.066676\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.076009\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.085249\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.094397\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.103453\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.112418\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.121294\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.130081\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.118780\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.117593\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.126417\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.135152\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -20.123801\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.122563\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -20.101337\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.110324\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.119221\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.128028\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.136748\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.135381\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.144027\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.152587\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.161061\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.169450\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.177756\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.175978\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.184218\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.192376\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.200452\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.208448\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.216363\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.214200\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.212058\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.209937\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.207838\n",
            "resetting env. episode 60.000000, reward total was -18.000000. running mean: -20.185759\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.193902\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.191963\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.200043\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.198043\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.196062\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.204102\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.202061\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.210040\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.217940\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.225760\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.233503\n",
            "resetting env. episode 72.000000, reward total was -18.000000. running mean: -20.211168\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.199056\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.197065\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.205095\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.213044\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.220913\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.218704\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.226517\n",
            "resetting env. episode 80.000000, reward total was -18.000000. running mean: -20.204252\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.212209\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.220087\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.227887\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.235608\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.243252\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.250819\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.258311\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.255728\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.263170\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.260539\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.247933\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.245454\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.253000\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.260470\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.257865\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.265286\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.252633\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.240107\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.247706\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.245229\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.252777\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.260249\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.257646\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.255070\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.262519\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.269894\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.257195\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.264623\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.261977\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.249357\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.256863\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.254295\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.251752\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.259234\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.266642\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.263976\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.271336\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.268623\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.255936\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.263377\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.260743\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.258136\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.265554\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.252899\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.260370\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.257766\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.265188\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.262537\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.269911\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.277212\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.274440\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.281696\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.288879\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.295990\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.303030\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.310000\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.316900\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.313731\n",
            "resetting env. episode 139.000000, reward total was -18.000000. running mean: -20.290593\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.297687\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.304711\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.311663\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.318547\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.325361\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.332108\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.318787\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.325599\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.322343\n",
            "resetting env. episode 149.000000, reward total was -18.000000. running mean: -20.299119\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.306128\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.313067\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.309936\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.296837\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.303868\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.310830\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.317722\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.314544\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.321399\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.328185\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.334903\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -20.311554\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.318438\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.305254\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.312202\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.299079\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.306089\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.293028\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.300098\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.307097\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.294026\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.291085\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.298174\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.295193\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.302241\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.309218\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.296126\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.293165\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.290233\n",
            "resetting env. episode 179.000000, reward total was -18.000000. running mean: -20.267331\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.274658\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.271911\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.259192\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.266600\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.263934\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.271295\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.268582\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.275896\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.273137\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.270406\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.267702\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.265025\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.262374\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.269751\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.267053\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.274383\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.271639\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.268922\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.276233\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.283471\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.280636\n",
            "resetting env. episode 201.000000, reward total was -18.000000. running mean: -20.257830\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.265251\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.272599\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.279873\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.287074\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.274203\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.281461\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.288647\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.295760\n",
            "resetting env. episode 210.000000, reward total was -17.000000. running mean: -20.262803\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.270175\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.267473\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.274798\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.262050\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.269430\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.266735\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -20.244068\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.251627\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.259111\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.266520\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.273855\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.281116\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.288305\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.275422\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.272668\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.279941\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.287142\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.274270\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.261528\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.268912\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.256223\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.243661\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.251224\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.258712\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.256125\n",
            "resetting env. episode 236.000000, reward total was -18.000000. running mean: -20.233564\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.241228\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.248816\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -20.236328\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.243964\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.231525\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.229210\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.226917\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.234648\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.232302\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.239979\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.247579\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.255103\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.262552\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.249927\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.257427\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.264853\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.272205\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.269482\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.266788\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.264120\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.271479\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.268764\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.276076\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.283315\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.280482\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.287677\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.284801\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.291953\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.299033\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.286043\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.293182\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.300251\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.307248\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.304176\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.311134\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.298022\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.305042\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.311992\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.318872\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.315683\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.322526\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.329301\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.336008\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.342648\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.329222\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.325929\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.332670\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.339343\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.345950\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.352490\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.358965\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.345376\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.351922\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.358403\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.344819\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.341371\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.347957\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.334477\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.321133\n",
            "resetting env. episode 296.000000, reward total was -16.000000. running mean: -20.277921\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.285142\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.282291\n",
            "resetting env. episode 299.000000, reward total was -17.000000. running mean: -20.249468\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.256973\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.264403\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.271759\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.279042\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.266251\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.253589\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.261053\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.268442\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.265758\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.253100\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.260569\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.267964\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.275284\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.282531\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.289706\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.296809\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.303841\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.310802\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.297694\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.304717\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.311670\n",
            "resetting env. episode 321.000000, reward total was -17.000000. running mean: -20.278553\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.285768\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.282910\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.280081\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.287280\n",
            "resetting env. episode 326.000000, reward total was -18.000000. running mean: -20.264408\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.251763\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.259246\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.266653\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.263987\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.261347\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.258733\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.266146\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.253485\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.260950\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.258340\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.265757\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.263099\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.250468\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.257964\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.265384\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.272730\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.280003\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.277203\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.284431\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -20.261587\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.248971\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.256481\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.263916\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.261277\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.268664\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.275978\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.263218\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.270586\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.257880\n",
            "resetting env. episode 356.000000, reward total was -18.000000. running mean: -20.235301\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.232948\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.230618\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.238312\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.235929\n",
            "resetting env. episode 361.000000, reward total was -17.000000. running mean: -20.203570\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.191534\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.179619\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.177823\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.186044\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.194184\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.202242\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.210220\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.208118\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.206036\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.203976\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.201936\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.209917\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.217818\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.215640\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.203483\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.211448\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.219334\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.217140\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.224969\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.232719\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.240392\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.237988\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.245608\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.253152\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.250621\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.258115\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.265533\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.272878\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.280149\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.287348\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.294474\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.301530\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.308514\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.315429\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.302275\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.299252\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.306260\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.303197\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.290165\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.297263\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.294291\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.301348\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.308334\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.295251\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.292299\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.289376\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.286482\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.293617\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.290681\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.297774\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.304796\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.291748\n",
            "resetting env. episode 414.000000, reward total was -18.000000. running mean: -20.268831\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.266142\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.273481\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.280746\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.287939\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.285059\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.292209\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.299287\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.286294\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.283431\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.280597\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.277791\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.265013\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.272363\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.279639\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.286843\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.293974\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.301034\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.308024\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.304944\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.291894\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.288975\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.296086\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.293125\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.290194\n",
            "resetting env. episode 439.000000, reward total was -18.000000. running mean: -20.267292\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.274619\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.281873\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.289054\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.296163\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.303202\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.310170\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.297068\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.304097\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.311056\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.317946\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.324766\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.331519\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.338203\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.344821\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.331373\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.338059\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.334679\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.341332\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.347919\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.344440\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.350995\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.337485\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.344110\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.350669\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.357163\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.363591\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.359955\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.356355\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.362792\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.359164\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.365572\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.351917\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.358397\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.354813\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.351265\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.347753\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.344275\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.350832\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.357324\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.363751\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.360113\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.346512\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.343047\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.339617\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.346220\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.352758\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.349231\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.345738\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.332281\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.328958\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.335669\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.332312\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.318989\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.325799\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.332541\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.329215\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.315923\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.322764\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.329536\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.326241\n",
            "resetting env. episode 500.000000, reward total was -18.000000. running mean: -20.302979\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.309949\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.316849\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.313681\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.310544\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.317439\n",
            "resetting env. episode 506.000000, reward total was -16.000000. running mean: -20.274264\n",
            "resetting env. episode 507.000000, reward total was -19.000000. running mean: -20.261522\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.258906\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.266317\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.263654\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.271018\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.268307\n",
            "resetting env. episode 513.000000, reward total was -19.000000. running mean: -20.255624\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.263068\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -20.260437\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.257833\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.255255\n",
            "resetting env. episode 518.000000, reward total was -19.000000. running mean: -20.242702\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.250275\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.257772\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.265195\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.272543\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.279817\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.277019\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.284249\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.291406\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.288492\n",
            "resetting env. episode 528.000000, reward total was -18.000000. running mean: -20.265607\n",
            "resetting env. episode 529.000000, reward total was -19.000000. running mean: -20.252951\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.260422\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.267818\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.265140\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.262488\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.269863\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.267165\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.274493\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.281748\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.288931\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.296041\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.293081\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.290150\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.297249\n",
            "resetting env. episode 543.000000, reward total was -20.000000. running mean: -20.294276\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.301333\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.298320\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.305337\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.312283\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.309161\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -20.306069\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.313008\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.319878\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.316679\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.323513\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.330277\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.336975\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.333605\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.340269\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.346866\n",
            "resetting env. episode 559.000000, reward total was -19.000000. running mean: -20.333398\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.340064\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.336663\n",
            "resetting env. episode 562.000000, reward total was -19.000000. running mean: -20.323296\n",
            "resetting env. episode 563.000000, reward total was -20.000000. running mean: -20.320063\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.316863\n",
            "resetting env. episode 565.000000, reward total was -18.000000. running mean: -20.293694\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.290757\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.297850\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.304871\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.311822\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.318704\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -20.305517\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.302462\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.309437\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.316343\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -20.313180\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.310048\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.316947\n",
            "resetting env. episode 578.000000, reward total was -20.000000. running mean: -20.313778\n",
            "resetting env. episode 579.000000, reward total was -18.000000. running mean: -20.290640\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.287734\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.294856\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.301908\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.308889\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.305800\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.312742\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.319614\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.316418\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.323254\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.330021\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.336721\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -20.333354\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.330020\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.326720\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.323453\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.330219\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.336916\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.343547\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.350112\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.356611\n",
            "resetting env. episode 600.000000, reward total was -20.000000. running mean: -20.353044\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.349514\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.346019\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.342559\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.349133\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.355642\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.352085\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.358565\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.364979\n",
            "resetting env. episode 609.000000, reward total was -17.000000. running mean: -20.331329\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.338016\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.344636\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.351189\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.357677\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.364101\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.370460\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -20.376755\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.382987\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.389158\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.385266\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.391413\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.387499\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.393624\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.399688\n",
            "resetting env. episode 624.000000, reward total was -20.000000. running mean: -20.395691\n",
            "resetting env. episode 625.000000, reward total was -19.000000. running mean: -20.381734\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.387917\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.384038\n",
            "resetting env. episode 628.000000, reward total was -19.000000. running mean: -20.370197\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.376495\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.382730\n",
            "resetting env. episode 631.000000, reward total was -19.000000. running mean: -20.368903\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.375214\n",
            "resetting env. episode 633.000000, reward total was -18.000000. running mean: -20.351462\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.357947\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.354368\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.360824\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.367216\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.363544\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.369908\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.376209\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.372447\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.378723\n",
            "resetting env. episode 643.000000, reward total was -19.000000. running mean: -20.364935\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.371286\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -20.357573\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.353997\n",
            "resetting env. episode 647.000000, reward total was -20.000000. running mean: -20.350458\n",
            "resetting env. episode 648.000000, reward total was -19.000000. running mean: -20.336953\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.343583\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.350148\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.356646\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.363080\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.369449\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.375754\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -20.361997\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.358377\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.354793\n",
            "resetting env. episode 658.000000, reward total was -19.000000. running mean: -20.341245\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.337833\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.334454\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.341110\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -20.327699\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -20.314422\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -20.311278\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.308165\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.305083\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.312032\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.318912\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.325723\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.332466\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.329141\n",
            "resetting env. episode 672.000000, reward total was -19.000000. running mean: -20.315850\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.312691\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.319564\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.326368\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -20.313105\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.309974\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.316874\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.323705\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.320468\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.327264\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.323991\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.330751\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.337443\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.344069\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.350628\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.347122\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.353651\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.360114\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.356513\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -20.362948\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.359319\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -20.355725\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.362168\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.368546\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.374861\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.381112\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.377301\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.383528\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.379693\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.385896\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.392037\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.388117\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.384236\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.380393\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.366589\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.362923\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.359294\n",
            "resetting env. episode 709.000000, reward total was -17.000000. running mean: -20.325701\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.332444\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.339120\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.335729\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.342371\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.348948\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.355458\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.361903\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.358284\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.364702\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.371055\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.377344\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.383571\n",
            "resetting env. episode 722.000000, reward total was -18.000000. running mean: -20.359735\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.366138\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.372476\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -20.368751\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.375064\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.381313\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.387500\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.393625\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.399689\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.395692\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.401735\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.407718\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.403641\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.409604\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.415508\n",
            "resetting env. episode 737.000000, reward total was -19.000000. running mean: -20.401353\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.407339\n",
            "resetting env. episode 739.000000, reward total was -18.000000. running mean: -20.383266\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.379433\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.385639\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.381783\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.387965\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.384085\n",
            "resetting env. episode 745.000000, reward total was -19.000000. running mean: -20.370244\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.366542\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.372877\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.379148\n",
            "resetting env. episode 749.000000, reward total was -19.000000. running mean: -20.365356\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.361703\n",
            "resetting env. episode 751.000000, reward total was -19.000000. running mean: -20.348086\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.354605\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.351059\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.357548\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.353973\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.360433\n",
            "resetting env. episode 757.000000, reward total was -19.000000. running mean: -20.346829\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -20.343360\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.339927\n",
            "resetting env. episode 760.000000, reward total was -19.000000. running mean: -20.326527\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -20.323262\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.320030\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -20.306829\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.313761\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.310623\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.317517\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.324342\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -20.321099\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.327888\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.324609\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.321363\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.318149\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.314967\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.321818\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.328600\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.325314\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.322061\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.318840\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.325652\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.332395\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.339071\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.345680\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.342224\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.348801\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.355313\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.361760\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.368143\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.374461\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -20.360717\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.357109\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.363538\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.369903\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.376204\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.382442\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.388617\n",
            "resetting env. episode 796.000000, reward total was -18.000000. running mean: -20.364731\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.371084\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -20.357373\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.363799\n",
            "resetting env. episode 800.000000, reward total was -19.000000. running mean: -20.350161\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.346660\n",
            "resetting env. episode 802.000000, reward total was -20.000000. running mean: -20.343193\n",
            "resetting env. episode 803.000000, reward total was -18.000000. running mean: -20.319761\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.316564\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.323398\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -20.320164\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.326962\n",
            "resetting env. episode 808.000000, reward total was -19.000000. running mean: -20.313693\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.320556\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.317350\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -20.324177\n",
            "resetting env. episode 812.000000, reward total was -18.000000. running mean: -20.300935\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.297926\n",
            "resetting env. episode 814.000000, reward total was -20.000000. running mean: -20.294946\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.301997\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -20.298977\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.305987\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.302927\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.309898\n",
            "resetting env. episode 820.000000, reward total was -19.000000. running mean: -20.296799\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -20.283831\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.290993\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.298083\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.295102\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.302151\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.299129\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.306138\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.313077\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -20.309946\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.306847\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.313778\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.320640\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -20.307434\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.314360\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.321216\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.318004\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.324824\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.331576\n",
            "resetting env. episode 839.000000, reward total was -19.000000. running mean: -20.318260\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.325077\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.321826\n",
            "resetting env. episode 842.000000, reward total was -19.000000. running mean: -20.308608\n",
            "resetting env. episode 843.000000, reward total was -19.000000. running mean: -20.295522\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -20.292567\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.299641\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -20.296645\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.303678\n",
            "resetting env. episode 848.000000, reward total was -19.000000. running mean: -20.290642\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -20.287735\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.284858\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.282009\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.279189\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.286397\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.293533\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.300598\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.297592\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.304616\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.301570\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.308554\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.315469\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.322314\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.329091\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.335800\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -20.322442\n",
            "resetting env. episode 865.000000, reward total was -19.000000. running mean: -20.309217\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -20.306125\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.313064\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.319933\n",
            "resetting env. episode 869.000000, reward total was -19.000000. running mean: -20.306734\n",
            "resetting env. episode 870.000000, reward total was -19.000000. running mean: -20.293667\n",
            "resetting env. episode 871.000000, reward total was -17.000000. running mean: -20.260730\n",
            "resetting env. episode 872.000000, reward total was -19.000000. running mean: -20.248123\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.255642\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.263085\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.260454\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.267850\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.275171\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.282419\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.279595\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.276799\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.284031\n",
            "resetting env. episode 882.000000, reward total was -19.000000. running mean: -20.271191\n",
            "resetting env. episode 883.000000, reward total was -17.000000. running mean: -20.238479\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.236094\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -20.233733\n",
            "resetting env. episode 886.000000, reward total was -19.000000. running mean: -20.221396\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.229182\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -20.226890\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.234621\n",
            "resetting env. episode 890.000000, reward total was -19.000000. running mean: -20.222275\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.230052\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -20.227752\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.235474\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.243120\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.250688\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.258182\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.265600\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.272944\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.280214\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.287412\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.284538\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -20.271693\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.278976\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.286186\n",
            "resetting env. episode 905.000000, reward total was -20.000000. running mean: -20.283324\n",
            "resetting env. episode 906.000000, reward total was -19.000000. running mean: -20.270491\n",
            "resetting env. episode 907.000000, reward total was -19.000000. running mean: -20.257786\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.265208\n",
            "resetting env. episode 909.000000, reward total was -19.000000. running mean: -20.252556\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.250030\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.257530\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.264955\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.272305\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.279582\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.276786\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.274019\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -20.271278\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.278566\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.285780\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -20.282922\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.290093\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.297192\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.294220\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -20.291278\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.298365\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.305381\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.312328\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -20.299204\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.296212\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.303250\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.310218\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.317115\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -20.303944\n",
            "resetting env. episode 934.000000, reward total was -20.000000. running mean: -20.300905\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.307896\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.314817\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.311669\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.308552\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.315467\n",
            "resetting env. episode 940.000000, reward total was -18.000000. running mean: -20.292312\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.289389\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.296495\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.303530\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.300495\n",
            "resetting env. episode 945.000000, reward total was -19.000000. running mean: -20.287490\n",
            "resetting env. episode 946.000000, reward total was -17.000000. running mean: -20.254615\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -20.242069\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.249648\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.247151\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.254680\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -20.252133\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.259612\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -20.257016\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.264446\n",
            "resetting env. episode 955.000000, reward total was -19.000000. running mean: -20.251801\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.259283\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.266690\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.274023\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.281283\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.278470\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.285686\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -20.282829\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.290000\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.287100\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.294229\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.291287\n",
            "resetting env. episode 967.000000, reward total was -18.000000. running mean: -20.268374\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.265690\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.273034\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.280303\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -20.277500\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.284725\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.291878\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.288959\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.296070\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.293109\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.290178\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.297276\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.304303\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.301260\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -20.298248\n",
            "resetting env. episode 982.000000, reward total was -19.000000. running mean: -20.285265\n",
            "resetting env. episode 983.000000, reward total was -19.000000. running mean: -20.272413\n",
            "resetting env. episode 984.000000, reward total was -18.000000. running mean: -20.249688\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.257192\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -20.244620\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.252173\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.249652\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.247155\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -20.244684\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.252237\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.249714\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.257217\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -20.254645\n",
            "resetting env. episode 995.000000, reward total was -19.000000. running mean: -20.242099\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.249678\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.257181\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.254609\n",
            "resetting env. episode 999.000000, reward total was -19.000000. running mean: -20.242063\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.239642\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.247246\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.254773\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.262226\n",
            "resetting env. episode 1004.000000, reward total was -20.000000. running mean: -20.259603\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.267007\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.274337\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -20.271594\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.268878\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.266189\n",
            "resetting env. episode 1010.000000, reward total was -19.000000. running mean: -20.253527\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.260992\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.258382\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.265798\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.273140\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.280409\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.287605\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.294729\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -20.281782\n",
            "resetting env. episode 1019.000000, reward total was -19.000000. running mean: -20.268964\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -20.266274\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.273611\n",
            "resetting env. episode 1022.000000, reward total was -19.000000. running mean: -20.260875\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.268266\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.275584\n",
            "resetting env. episode 1025.000000, reward total was -19.000000. running mean: -20.262828\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.260200\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -20.257598\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.265022\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -20.262371\n",
            "resetting env. episode 1030.000000, reward total was -16.000000. running mean: -20.219748\n",
            "resetting env. episode 1031.000000, reward total was -20.000000. running mean: -20.217550\n",
            "resetting env. episode 1032.000000, reward total was -19.000000. running mean: -20.205375\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.203321\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.211288\n",
            "resetting env. episode 1035.000000, reward total was -19.000000. running mean: -20.199175\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.207183\n",
            "resetting env. episode 1037.000000, reward total was -20.000000. running mean: -20.205111\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -20.193060\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.201130\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.209118\n",
            "resetting env. episode 1041.000000, reward total was -18.000000. running mean: -20.187027\n",
            "resetting env. episode 1042.000000, reward total was -16.000000. running mean: -20.145157\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -20.143705\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.142268\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.150846\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -20.149337\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.147844\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -20.146365\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -20.144902\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.153453\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.161918\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.170299\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.178596\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -20.176810\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -20.175042\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.183291\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.191459\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -20.189544\n",
            "resetting env. episode 1059.000000, reward total was -19.000000. running mean: -20.177649\n",
            "resetting env. episode 1060.000000, reward total was -19.000000. running mean: -20.165872\n",
            "resetting env. episode 1061.000000, reward total was -19.000000. running mean: -20.154213\n",
            "resetting env. episode 1062.000000, reward total was -19.000000. running mean: -20.142671\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.141245\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.149832\n",
            "resetting env. episode 1065.000000, reward total was -19.000000. running mean: -20.138334\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.136950\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.135581\n",
            "resetting env. episode 1068.000000, reward total was -18.000000. running mean: -20.114225\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.123083\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.131852\n",
            "resetting env. episode 1071.000000, reward total was -19.000000. running mean: -20.120533\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.119328\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.118135\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -20.116954\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.115784\n",
            "resetting env. episode 1076.000000, reward total was -19.000000. running mean: -20.104626\n",
            "resetting env. episode 1077.000000, reward total was -19.000000. running mean: -20.093580\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.092644\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.101718\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.110700\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -20.099593\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.108598\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.117512\n",
            "resetting env. episode 1084.000000, reward total was -20.000000. running mean: -20.116336\n",
            "resetting env. episode 1085.000000, reward total was -19.000000. running mean: -20.105173\n",
            "resetting env. episode 1086.000000, reward total was -20.000000. running mean: -20.104121\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -20.103080\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -20.102049\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.101029\n",
            "resetting env. episode 1090.000000, reward total was -19.000000. running mean: -20.090019\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -20.089118\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -20.088227\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.097345\n",
            "resetting env. episode 1094.000000, reward total was -18.000000. running mean: -20.076371\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.085608\n",
            "resetting env. episode 1096.000000, reward total was -19.000000. running mean: -20.074752\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -20.074004\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.083264\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.092431\n",
            "resetting env. episode 1100.000000, reward total was -19.000000. running mean: -20.081507\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.080692\n",
            "resetting env. episode 1102.000000, reward total was -19.000000. running mean: -20.069885\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -20.069186\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.068494\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.077810\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.077031\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.076261\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.085498\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.094643\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.093697\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.102760\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.101732\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.110715\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.109608\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -20.108512\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -20.107427\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -20.106353\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.105289\n",
            "resetting env. episode 1119.000000, reward total was -19.000000. running mean: -20.094236\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.093294\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.102361\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.111337\n",
            "resetting env. episode 1123.000000, reward total was -19.000000. running mean: -20.100224\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.109222\n",
            "resetting env. episode 1125.000000, reward total was -18.000000. running mean: -20.088129\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.097248\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.106276\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.115213\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -20.114061\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -20.112920\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -20.111791\n",
            "resetting env. episode 1132.000000, reward total was -19.000000. running mean: -20.100673\n",
            "resetting env. episode 1133.000000, reward total was -18.000000. running mean: -20.079666\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.078870\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.078081\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.077300\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.086527\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.085662\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.094805\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.103857\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.102819\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -20.091790\n",
            "resetting env. episode 1143.000000, reward total was -19.000000. running mean: -20.080873\n",
            "resetting env. episode 1144.000000, reward total was -18.000000. running mean: -20.060064\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -20.049463\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.058969\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.068379\n",
            "resetting env. episode 1148.000000, reward total was -19.000000. running mean: -20.057695\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -20.057118\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.056547\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.065981\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.065322\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.064668\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.074022\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.083282\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -20.082449\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -20.081624\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.090808\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.089900\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -20.089001\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -20.078111\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.087330\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.096456\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -20.085492\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.094637\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.103691\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -20.092654\n",
            "resetting env. episode 1168.000000, reward total was -18.000000. running mean: -20.071727\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.081010\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.090200\n",
            "resetting env. episode 1171.000000, reward total was -19.000000. running mean: -20.079298\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.078505\n",
            "resetting env. episode 1173.000000, reward total was -18.000000. running mean: -20.057720\n",
            "resetting env. episode 1174.000000, reward total was -18.000000. running mean: -20.037143\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.046771\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.046303\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.055840\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -20.055282\n",
            "resetting env. episode 1179.000000, reward total was -19.000000. running mean: -20.044729\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.054282\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.063739\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -20.053102\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.062571\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.071945\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.071226\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.080513\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -20.079708\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -20.078911\n",
            "resetting env. episode 1189.000000, reward total was -18.000000. running mean: -20.058122\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -20.047541\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.047065\n",
            "resetting env. episode 1192.000000, reward total was -18.000000. running mean: -20.026595\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -20.026329\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.036065\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.045705\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.055248\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.064695\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.074048\n",
            "resetting env. episode 1199.000000, reward total was -18.000000. running mean: -20.053308\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.062775\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -20.062147\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -20.061526\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.070910\n",
            "resetting env. episode 1204.000000, reward total was -19.000000. running mean: -20.060201\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -20.059599\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.059003\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.048413\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.057929\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -20.057350\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.066776\n",
            "resetting env. episode 1211.000000, reward total was -18.000000. running mean: -20.046108\n",
            "resetting env. episode 1212.000000, reward total was -18.000000. running mean: -20.025647\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.035391\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.045037\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.054587\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -20.044041\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.053600\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.063064\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -20.062434\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.071809\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -20.071091\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -20.070380\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -20.059677\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.069080\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.078389\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.077605\n",
            "resetting env. episode 1227.000000, reward total was -18.000000. running mean: -20.056829\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -20.056261\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.065698\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.075041\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.084291\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.083448\n",
            "resetting env. episode 1233.000000, reward total was -19.000000. running mean: -20.072613\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.081887\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -20.071068\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.080358\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.089554\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -20.088659\n",
            "resetting env. episode 1239.000000, reward total was -19.000000. running mean: -20.077772\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.086994\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.096124\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.105163\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.114111\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.122970\n",
            "resetting env. episode 1245.000000, reward total was -20.000000. running mean: -20.121741\n",
            "resetting env. episode 1246.000000, reward total was -18.000000. running mean: -20.100523\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.109518\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.108423\n",
            "resetting env. episode 1249.000000, reward total was -19.000000. running mean: -20.097339\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -20.086365\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.085502\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.084647\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.093800\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.102862\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.111833\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.120715\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.129508\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -20.118213\n",
            "resetting env. episode 1259.000000, reward total was -18.000000. running mean: -20.097031\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.106060\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -20.095000\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.104050\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.113009\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.121879\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.130660\n",
            "resetting env. episode 1266.000000, reward total was -20.000000. running mean: -20.129354\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -20.128060\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.136780\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.145412\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.143958\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.152518\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.160993\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.169383\n",
            "resetting env. episode 1274.000000, reward total was -18.000000. running mean: -20.147689\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.156212\n",
            "resetting env. episode 1276.000000, reward total was -19.000000. running mean: -20.144650\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.153204\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -20.151672\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.160155\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.168553\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.176868\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.185099\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.193248\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -20.191316\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.189403\n",
            "resetting env. episode 1286.000000, reward total was -18.000000. running mean: -20.167509\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -20.165833\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.164175\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.162533\n",
            "resetting env. episode 1290.000000, reward total was -18.000000. running mean: -20.140908\n",
            "resetting env. episode 1291.000000, reward total was -18.000000. running mean: -20.119499\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -20.118304\n",
            "resetting env. episode 1293.000000, reward total was -19.000000. running mean: -20.107121\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -20.106050\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -20.104989\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.113939\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -20.112800\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.121672\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.130455\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.139151\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.147759\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.156282\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.164719\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.173072\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.181341\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.189527\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.197632\n",
            "resetting env. episode 1308.000000, reward total was -19.000000. running mean: -20.185656\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.183799\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.181961\n",
            "resetting env. episode 1311.000000, reward total was -19.000000. running mean: -20.170142\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.178440\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.186656\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.194789\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.202841\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.210813\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.208705\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.216618\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.224452\n",
            "resetting env. episode 1320.000000, reward total was -20.000000. running mean: -20.222207\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -20.219985\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.227785\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.235507\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -20.233152\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.240821\n",
            "resetting env. episode 1326.000000, reward total was -19.000000. running mean: -20.228413\n",
            "resetting env. episode 1327.000000, reward total was -19.000000. running mean: -20.216128\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.223967\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.231727\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.239410\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.247016\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.254546\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -20.252000\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.259480\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -20.256886\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.254317\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -20.251774\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -20.249256\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.256763\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.264196\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -20.251554\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.259038\n",
            "resetting env. episode 1343.000000, reward total was -19.000000. running mean: -20.246448\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -20.243983\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.251544\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.259028\n",
            "resetting env. episode 1347.000000, reward total was -18.000000. running mean: -20.236438\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -20.224073\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.221833\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.229614\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.227318\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.225045\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -20.222795\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.230567\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.238261\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.245878\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.253420\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.260885\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -20.258277\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -20.255694\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.263137\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.260505\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.257900\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -20.255321\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.262768\n",
            "resetting env. episode 1366.000000, reward total was -19.000000. running mean: -20.250141\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.257639\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -20.255063\n",
            "resetting env. episode 1369.000000, reward total was -18.000000. running mean: -20.232512\n",
            "resetting env. episode 1370.000000, reward total was -19.000000. running mean: -20.220187\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.227985\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.235705\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.233348\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -20.231015\n",
            "resetting env. episode 1375.000000, reward total was -19.000000. running mean: -20.218705\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.216518\n",
            "resetting env. episode 1377.000000, reward total was -18.000000. running mean: -20.194352\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.202409\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -20.200385\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.208381\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.196297\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -20.194334\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.202391\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.200367\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.198363\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.196380\n",
            "resetting env. episode 1387.000000, reward total was -19.000000. running mean: -20.184416\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.192572\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.200646\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -20.198639\n",
            "resetting env. episode 1391.000000, reward total was -19.000000. running mean: -20.186653\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -20.184787\n",
            "resetting env. episode 1393.000000, reward total was -19.000000. running mean: -20.172939\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.181209\n",
            "resetting env. episode 1395.000000, reward total was -19.000000. running mean: -20.169397\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.177703\n",
            "resetting env. episode 1397.000000, reward total was -19.000000. running mean: -20.165926\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.174267\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.182524\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.180699\n",
            "resetting env. episode 1401.000000, reward total was -19.000000. running mean: -20.168892\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.177203\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -20.175431\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -20.173677\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.181940\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.190121\n",
            "resetting env. episode 1407.000000, reward total was -18.000000. running mean: -20.168219\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.166537\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.174872\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.183123\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.191292\n",
            "resetting env. episode 1412.000000, reward total was -19.000000. running mean: -20.179379\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -20.177585\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.185809\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.193951\n",
            "resetting env. episode 1416.000000, reward total was -19.000000. running mean: -20.182012\n",
            "resetting env. episode 1417.000000, reward total was -18.000000. running mean: -20.160192\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.168590\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.176904\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.175135\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -20.183383\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -20.181550\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -20.179734\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.187937\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.196057\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.204097\n",
            "resetting env. episode 1427.000000, reward total was -18.000000. running mean: -20.182056\n",
            "resetting env. episode 1428.000000, reward total was -20.000000. running mean: -20.180235\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.178433\n",
            "resetting env. episode 1430.000000, reward total was -19.000000. running mean: -20.166649\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.174982\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -20.163232\n",
            "resetting env. episode 1433.000000, reward total was -19.000000. running mean: -20.151600\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.160084\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.168483\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.166798\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.175130\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.183379\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.191545\n",
            "resetting env. episode 1440.000000, reward total was -18.000000. running mean: -20.169630\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.177933\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.186154\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.194293\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -20.202350\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.210326\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.218223\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.226041\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -20.213780\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.211642\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.219526\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.227331\n",
            "resetting env. episode 1452.000000, reward total was -15.000000. running mean: -20.175057\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.183307\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.191474\n",
            "resetting env. episode 1455.000000, reward total was -19.000000. running mean: -20.179559\n",
            "resetting env. episode 1456.000000, reward total was -19.000000. running mean: -20.167763\n",
            "resetting env. episode 1457.000000, reward total was -19.000000. running mean: -20.156086\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -20.154525\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.162980\n",
            "resetting env. episode 1460.000000, reward total was -18.000000. running mean: -20.141350\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.149936\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -20.148437\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -20.146953\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.155483\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -20.143928\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -20.142489\n",
            "resetting env. episode 1467.000000, reward total was -18.000000. running mean: -20.121064\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.129854\n",
            "resetting env. episode 1469.000000, reward total was -18.000000. running mean: -20.108555\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.117469\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -20.116295\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.115132\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -20.113980\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.122841\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -20.121612\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.130396\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.139092\n",
            "resetting env. episode 1478.000000, reward total was -19.000000. running mean: -20.127701\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.136424\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.145060\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.153609\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.162073\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.170453\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.178748\n",
            "resetting env. episode 1485.000000, reward total was -19.000000. running mean: -20.166961\n",
            "resetting env. episode 1486.000000, reward total was -19.000000. running mean: -20.155291\n",
            "resetting env. episode 1487.000000, reward total was -20.000000. running mean: -20.153738\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -20.152201\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -20.150679\n",
            "resetting env. episode 1490.000000, reward total was -18.000000. running mean: -20.129172\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.137880\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -20.136501\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.145136\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -20.143685\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -20.142248\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.150826\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -20.149317\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -20.147824\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.156346\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -20.154783\n",
            "CPU times: user 1h 12min 36s, sys: 33min 18s, total: 1h 45min 55s\n",
            "Wall time: 54min 53s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "w2NblmwDsL3y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "4c4b7066-2758-46c9-84a9-b6ffaaf9be4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "H=200_le_4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}