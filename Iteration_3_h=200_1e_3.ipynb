{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "b5bba1cc-be2e-4c36-8e2c-d4879b3e29fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=42c23a992fbbe8ad6ee29c0e06eb86100c58770b8fd7381093622afd02f45a04\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "1d1fe125-9989-4230-ccd9-ba735425e2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "8968d954-5ba1-420e-9261-db85520c8034"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "405dfa00-79cd-4693-db91-3716ef46ce7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "f9534d47-b1ee-4c2e-be88-f8bc7020d203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-3\n",
        "learning_rate = 1e-3\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "72e51e16-6861-43bb-f838-a9695b5b3f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.980394\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.970590\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.950884\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.951375\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.951862\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.952343\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.952820\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.953291\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.953758\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.954221\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.954679\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.955132\n",
            "resetting env. episode 19.000000, reward total was -18.000000. running mean: -20.925581\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.926325\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.907061\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.897991\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.889011\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.890121\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.891220\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.882307\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.883484\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.884650\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.885803\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.876945\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.878176\n",
            "resetting env. episode 32.000000, reward total was -16.000000. running mean: -20.829394\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.831100\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.832789\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.834461\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.836116\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.837755\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.829378\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.831084\n",
            "resetting env. episode 40.000000, reward total was -18.000000. running mean: -20.802773\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.794745\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.796798\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.778830\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.781042\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.783231\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.775399\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.767645\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.769968\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.772269\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.774546\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.766801\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.769133\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.761441\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.753827\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.746289\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.748826\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.741337\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.733924\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.716585\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.709419\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.702325\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.695302\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.688349\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.681465\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.674650\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.677904\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.681125\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.684314\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.677470\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.680696\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.683889\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.687050\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.680179\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.673378\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -20.656644\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.650077\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.653577\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.647041\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.630570\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.634265\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.637922\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.641543\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.645127\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.638676\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.622289\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.626067\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.609806\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.603708\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.607671\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.601594\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.585578\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.589722\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.583825\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.577987\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.582207\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.586385\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.570521\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.564816\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.569168\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.553476\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.547941\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.542462\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.547037\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -20.531567\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.526251\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.520989\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.525779\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.530521\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.515216\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.520064\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.504863\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.509814\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.504716\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.499669\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.504672\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.509626\n",
            "resetting env. episode 117.000000, reward total was -19.000000. running mean: -20.494529\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.499584\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.504588\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.489542\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.474647\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.469900\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.475201\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.470449\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.475745\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.460987\n",
            "resetting env. episode 127.000000, reward total was -18.000000. running mean: -20.436378\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.442014\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.437594\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.433218\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.438886\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.444497\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.440052\n",
            "resetting env. episode 134.000000, reward total was -18.000000. running mean: -20.415651\n",
            "resetting env. episode 135.000000, reward total was -18.000000. running mean: -20.391495\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.387580\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.393704\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.399767\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.405769\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.411712\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.417594\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.423419\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.429184\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.424892\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.420644\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.426437\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.412173\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.418051\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.413871\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.399732\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.405734\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.401677\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.397660\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.393684\n",
            "resetting env. episode 155.000000, reward total was -18.000000. running mean: -20.369747\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.376049\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.382289\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.388466\n",
            "resetting env. episode 159.000000, reward total was -18.000000. running mean: -20.364581\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.370936\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.377226\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.383454\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.389619\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.395723\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.391766\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.387848\n",
            "resetting env. episode 167.000000, reward total was -18.000000. running mean: -20.363970\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.360330\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -20.336727\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.333360\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.330026\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.336726\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.323358\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.330125\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.336824\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.343455\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.340021\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.336621\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.333254\n",
            "resetting env. episode 180.000000, reward total was -16.000000. running mean: -20.289922\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -20.277023\n",
            "resetting env. episode 182.000000, reward total was -18.000000. running mean: -20.254252\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.251710\n",
            "resetting env. episode 184.000000, reward total was -17.000000. running mean: -20.219193\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.217001\n",
            "resetting env. episode 186.000000, reward total was -18.000000. running mean: -20.194831\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.182883\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.181054\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.169243\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.157551\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.155975\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.144416\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.152971\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.141442\n",
            "resetting env. episode 195.000000, reward total was -18.000000. running mean: -20.120027\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.128827\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.117539\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.126363\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -20.115100\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.113949\n",
            "resetting env. episode 201.000000, reward total was -18.000000. running mean: -20.092809\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.101881\n",
            "resetting env. episode 203.000000, reward total was -18.000000. running mean: -20.080862\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.090054\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.099153\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.098162\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.087180\n",
            "resetting env. episode 208.000000, reward total was -18.000000. running mean: -20.066308\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.075645\n",
            "resetting env. episode 210.000000, reward total was -18.000000. running mean: -20.054889\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.064340\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.073696\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.072959\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.072230\n",
            "resetting env. episode 215.000000, reward total was -18.000000. running mean: -20.051508\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.060992\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.050383\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.059879\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.069280\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.068587\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.077901\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.087122\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.096251\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.105288\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.104236\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.103193\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.112161\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.121040\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.119829\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.128631\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.127345\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.116071\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.114911\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.123761\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.122524\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.131299\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.129986\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.138686\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.137299\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.125926\n",
            "resetting env. episode 241.000000, reward total was -16.000000. running mean: -20.084667\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.083820\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.092982\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.082052\n",
            "resetting env. episode 245.000000, reward total was -17.000000. running mean: -20.051231\n",
            "resetting env. episode 246.000000, reward total was -18.000000. running mean: -20.030719\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.030412\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.030108\n",
            "resetting env. episode 249.000000, reward total was -17.000000. running mean: -19.999807\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.009809\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.009711\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.009613\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.009517\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.009422\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.019328\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.019135\n",
            "resetting env. episode 257.000000, reward total was -18.000000. running mean: -19.998943\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -19.998954\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -19.998964\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.008975\n",
            "resetting env. episode 261.000000, reward total was -18.000000. running mean: -19.988885\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -19.998996\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -19.999006\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -19.999016\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -19.999026\n",
            "resetting env. episode 266.000000, reward total was -18.000000. running mean: -19.979036\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -19.979245\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -19.979453\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -19.979658\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -19.979862\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -19.970063\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -19.960362\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -19.970759\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -19.981051\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -19.991241\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -19.991328\n",
            "resetting env. episode 277.000000, reward total was -17.000000. running mean: -19.961415\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -19.961801\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -19.962183\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -19.962561\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -19.952935\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -19.963406\n",
            "resetting env. episode 283.000000, reward total was -17.000000. running mean: -19.933772\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -19.934434\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -19.935090\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -19.925739\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -19.936482\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -19.947117\n",
            "resetting env. episode 289.000000, reward total was -18.000000. running mean: -19.927646\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -19.938369\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -19.948986\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -19.949496\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -19.960001\n",
            "resetting env. episode 294.000000, reward total was -18.000000. running mean: -19.940401\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -19.940997\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -19.951587\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -19.942071\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -19.952650\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -19.953124\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -19.943592\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -19.934157\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -19.924815\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -19.925567\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -19.916311\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -19.917148\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -19.927977\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -19.938697\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -19.949310\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -19.949817\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -19.950319\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -19.960815\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -19.971207\n",
            "resetting env. episode 313.000000, reward total was -17.000000. running mean: -19.941495\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -19.932080\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -19.942759\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -19.933332\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -19.943998\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -19.944558\n",
            "resetting env. episode 319.000000, reward total was -18.000000. running mean: -19.925113\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -19.925862\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -19.926603\n",
            "resetting env. episode 322.000000, reward total was -18.000000. running mean: -19.907337\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -19.918264\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -19.909081\n",
            "resetting env. episode 325.000000, reward total was -18.000000. running mean: -19.889990\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -19.881090\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -19.892280\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -19.893357\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -19.904423\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -19.915379\n",
            "resetting env. episode 331.000000, reward total was -17.000000. running mean: -19.886225\n",
            "resetting env. episode 332.000000, reward total was -17.000000. running mean: -19.857363\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -19.848789\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -19.840301\n",
            "resetting env. episode 335.000000, reward total was -18.000000. running mean: -19.821898\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -19.833679\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -19.835343\n",
            "resetting env. episode 338.000000, reward total was -18.000000. running mean: -19.816989\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -19.808819\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -19.810731\n",
            "resetting env. episode 341.000000, reward total was -17.000000. running mean: -19.782624\n",
            "resetting env. episode 342.000000, reward total was -18.000000. running mean: -19.764798\n",
            "resetting env. episode 343.000000, reward total was -17.000000. running mean: -19.737150\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -19.739778\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -19.732380\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -19.725056\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -19.727806\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -19.730528\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -19.743223\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -19.745790\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -19.748332\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -19.760849\n",
            "resetting env. episode 353.000000, reward total was -18.000000. running mean: -19.743241\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -19.735808\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -19.738450\n",
            "resetting env. episode 356.000000, reward total was -18.000000. running mean: -19.721066\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -19.733855\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -19.736516\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -19.729151\n",
            "resetting env. episode 360.000000, reward total was -18.000000. running mean: -19.711860\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -19.724741\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -19.727494\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -19.730219\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -19.722917\n",
            "resetting env. episode 365.000000, reward total was -17.000000. running mean: -19.695687\n",
            "resetting env. episode 366.000000, reward total was -18.000000. running mean: -19.678731\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -19.671943\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -19.675224\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -19.678472\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -19.681687\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -19.684870\n",
            "resetting env. episode 372.000000, reward total was -18.000000. running mean: -19.668021\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -19.671341\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -19.684628\n",
            "resetting env. episode 375.000000, reward total was -17.000000. running mean: -19.657781\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -19.661204\n",
            "resetting env. episode 377.000000, reward total was -18.000000. running mean: -19.644592\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -19.658146\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -19.651564\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -19.655049\n",
            "resetting env. episode 381.000000, reward total was -18.000000. running mean: -19.638498\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -19.652113\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -19.655592\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -19.659036\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -19.662446\n",
            "resetting env. episode 386.000000, reward total was -17.000000. running mean: -19.635821\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -19.639463\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -19.643068\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -19.656638\n",
            "resetting env. episode 390.000000, reward total was -18.000000. running mean: -19.640071\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -19.643671\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -19.647234\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -19.660762\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -19.664154\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -19.657512\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -19.660937\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -19.664328\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -19.677685\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -19.670908\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -19.684199\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -19.687357\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -19.680483\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -19.683678\n",
            "resetting env. episode 404.000000, reward total was -15.000000. running mean: -19.636842\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -19.650473\n",
            "resetting env. episode 406.000000, reward total was -17.000000. running mean: -19.623968\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -19.637729\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -19.641351\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -19.654938\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -19.648389\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -19.641905\n",
            "resetting env. episode 412.000000, reward total was -16.000000. running mean: -19.605486\n",
            "resetting env. episode 413.000000, reward total was -18.000000. running mean: -19.589431\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -19.593536\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -19.597601\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -19.591625\n",
            "resetting env. episode 417.000000, reward total was -17.000000. running mean: -19.565709\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -19.570052\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -19.574351\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -19.578608\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -19.572822\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -19.587093\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -19.581222\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -19.585410\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -19.599556\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -19.593561\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -19.597625\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -19.601649\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -19.615632\n",
            "resetting env. episode 430.000000, reward total was -17.000000. running mean: -19.589476\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -19.593581\n",
            "resetting env. episode 432.000000, reward total was -16.000000. running mean: -19.557645\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -19.572069\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -19.586348\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -19.600485\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -19.604480\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -19.608435\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -19.612351\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -19.626227\n",
            "resetting env. episode 440.000000, reward total was -17.000000. running mean: -19.599965\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -19.593965\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -19.588026\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -19.602145\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -19.596124\n",
            "resetting env. episode 445.000000, reward total was -18.000000. running mean: -19.580163\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -19.594361\n",
            "resetting env. episode 447.000000, reward total was -18.000000. running mean: -19.578417\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -19.572633\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -19.566907\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -19.571238\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -19.585525\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -19.589670\n",
            "resetting env. episode 453.000000, reward total was -18.000000. running mean: -19.573774\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -19.578036\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -19.582255\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -19.586433\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -19.600569\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -19.604563\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -19.598517\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -19.592532\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -19.586607\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -19.590741\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -19.594833\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -19.588885\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -19.602996\n",
            "resetting env. episode 466.000000, reward total was -18.000000. running mean: -19.586966\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -19.571096\n",
            "resetting env. episode 468.000000, reward total was -17.000000. running mean: -19.545385\n",
            "resetting env. episode 469.000000, reward total was -18.000000. running mean: -19.529932\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -19.524632\n",
            "resetting env. episode 471.000000, reward total was -16.000000. running mean: -19.489386\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -19.504492\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -19.509447\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -19.514353\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -19.519209\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -19.514017\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -19.508877\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -19.493788\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -19.498850\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -19.503862\n",
            "resetting env. episode 481.000000, reward total was -18.000000. running mean: -19.488823\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -19.493935\n",
            "resetting env. episode 483.000000, reward total was -18.000000. running mean: -19.478996\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -19.474206\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -19.489464\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -19.494569\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -19.489623\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -19.484727\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -19.489880\n",
            "resetting env. episode 490.000000, reward total was -18.000000. running mean: -19.474981\n",
            "resetting env. episode 491.000000, reward total was -15.000000. running mean: -19.430231\n",
            "resetting env. episode 492.000000, reward total was -18.000000. running mean: -19.415929\n",
            "resetting env. episode 493.000000, reward total was -16.000000. running mean: -19.381770\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -19.377952\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -19.374172\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -19.380431\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -19.386626\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -19.392760\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -19.388832\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -19.384944\n",
            "CPU times: user 27min 24s, sys: 12min 27s, total: 39min 51s\n",
            "Wall time: 20min 39s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "577baceb-ac28-44f8-d9f5-64f63e556889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -19.990000\n",
            "resetting env. episode 4.000000, reward total was -18.000000. running mean: -19.970100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.980399\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.990595\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.000689\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.000682\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.000675\n",
            "resetting env. episode 10.000000, reward total was -18.000000. running mean: -19.980669\n",
            "resetting env. episode 11.000000, reward total was -16.000000. running mean: -19.940862\n",
            "resetting env. episode 12.000000, reward total was -17.000000. running mean: -19.911453\n",
            "resetting env. episode 13.000000, reward total was -18.000000. running mean: -19.892339\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -19.883415\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.894581\n",
            "resetting env. episode 16.000000, reward total was -18.000000. running mean: -19.875635\n",
            "resetting env. episode 17.000000, reward total was -17.000000. running mean: -19.846879\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.858410\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.869826\n",
            "resetting env. episode 20.000000, reward total was -17.000000. running mean: -19.841128\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -19.832717\n",
            "resetting env. episode 22.000000, reward total was -16.000000. running mean: -19.794389\n",
            "resetting env. episode 23.000000, reward total was -17.000000. running mean: -19.766446\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.768781\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -19.771093\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.783382\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -19.785549\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.787693\n",
            "resetting env. episode 29.000000, reward total was -18.000000. running mean: -19.769816\n",
            "resetting env. episode 30.000000, reward total was -18.000000. running mean: -19.752118\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.754597\n",
            "resetting env. episode 32.000000, reward total was -17.000000. running mean: -19.727051\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.739780\n",
            "resetting env. episode 34.000000, reward total was -18.000000. running mean: -19.722382\n",
            "resetting env. episode 35.000000, reward total was -18.000000. running mean: -19.705159\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -19.698107\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -19.691126\n",
            "resetting env. episode 38.000000, reward total was -18.000000. running mean: -19.674215\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -19.657473\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.660898\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.674289\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.687546\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -19.680671\n",
            "resetting env. episode 44.000000, reward total was -14.000000. running mean: -19.623864\n",
            "resetting env. episode 45.000000, reward total was -18.000000. running mean: -19.607625\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -19.611549\n",
            "resetting env. episode 47.000000, reward total was -18.000000. running mean: -19.595433\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -19.589479\n",
            "resetting env. episode 49.000000, reward total was -18.000000. running mean: -19.573584\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.587848\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.591970\n",
            "resetting env. episode 52.000000, reward total was -15.000000. running mean: -19.546050\n",
            "resetting env. episode 53.000000, reward total was -16.000000. running mean: -19.510590\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.515484\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -19.500329\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -19.495326\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.500373\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.515369\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.530215\n",
            "resetting env. episode 60.000000, reward total was -14.000000. running mean: -19.474913\n",
            "resetting env. episode 61.000000, reward total was -18.000000. running mean: -19.460164\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -19.455562\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -19.451007\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -19.456497\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.461932\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -19.457312\n",
            "resetting env. episode 67.000000, reward total was -16.000000. running mean: -19.422739\n",
            "resetting env. episode 68.000000, reward total was -18.000000. running mean: -19.408512\n",
            "resetting env. episode 69.000000, reward total was -18.000000. running mean: -19.394427\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.410482\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -19.406378\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.412314\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -19.408191\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.414109\n",
            "resetting env. episode 75.000000, reward total was -17.000000. running mean: -19.389968\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.396068\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.412107\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -19.417986\n",
            "resetting env. episode 79.000000, reward total was -17.000000. running mean: -19.393806\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.399868\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -19.395870\n",
            "resetting env. episode 82.000000, reward total was -19.000000. running mean: -19.391911\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -19.397992\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.414012\n",
            "resetting env. episode 85.000000, reward total was -17.000000. running mean: -19.389872\n",
            "resetting env. episode 86.000000, reward total was -15.000000. running mean: -19.345973\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.362513\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -19.368888\n",
            "resetting env. episode 89.000000, reward total was -17.000000. running mean: -19.345199\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -19.341747\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.348330\n",
            "resetting env. episode 92.000000, reward total was -18.000000. running mean: -19.334846\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -19.331498\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.348183\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -19.344701\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -19.341254\n",
            "resetting env. episode 97.000000, reward total was -18.000000. running mean: -19.327842\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.324563\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -19.321318\n",
            "resetting env. episode 100.000000, reward total was -18.000000. running mean: -19.308104\n",
            "resetting env. episode 101.000000, reward total was -17.000000. running mean: -19.285023\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.302173\n",
            "resetting env. episode 103.000000, reward total was -17.000000. running mean: -19.279151\n",
            "resetting env. episode 104.000000, reward total was -18.000000. running mean: -19.266360\n",
            "resetting env. episode 105.000000, reward total was -17.000000. running mean: -19.243696\n",
            "resetting env. episode 106.000000, reward total was -18.000000. running mean: -19.231259\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.238947\n",
            "resetting env. episode 108.000000, reward total was -16.000000. running mean: -19.206557\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -19.204492\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.212447\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.230322\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.238019\n",
            "resetting env. episode 113.000000, reward total was -18.000000. running mean: -19.225639\n",
            "resetting env. episode 114.000000, reward total was -18.000000. running mean: -19.213383\n",
            "resetting env. episode 115.000000, reward total was -18.000000. running mean: -19.201249\n",
            "resetting env. episode 116.000000, reward total was -14.000000. running mean: -19.149236\n",
            "resetting env. episode 117.000000, reward total was -18.000000. running mean: -19.137744\n",
            "resetting env. episode 118.000000, reward total was -17.000000. running mean: -19.116366\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -19.115203\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -19.114051\n",
            "resetting env. episode 121.000000, reward total was -16.000000. running mean: -19.082910\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.092081\n",
            "resetting env. episode 123.000000, reward total was -16.000000. running mean: -19.061160\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.070549\n",
            "resetting env. episode 125.000000, reward total was -17.000000. running mean: -19.049843\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -19.059345\n",
            "resetting env. episode 127.000000, reward total was -17.000000. running mean: -19.038751\n",
            "resetting env. episode 128.000000, reward total was -18.000000. running mean: -19.028364\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -19.048080\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -19.047599\n",
            "resetting env. episode 131.000000, reward total was -17.000000. running mean: -19.027123\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -19.036852\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -19.036484\n",
            "resetting env. episode 134.000000, reward total was -15.000000. running mean: -18.996119\n",
            "resetting env. episode 135.000000, reward total was -17.000000. running mean: -18.976158\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -18.986396\n",
            "resetting env. episode 137.000000, reward total was -18.000000. running mean: -18.976532\n",
            "resetting env. episode 138.000000, reward total was -18.000000. running mean: -18.966767\n",
            "resetting env. episode 139.000000, reward total was -18.000000. running mean: -18.957099\n",
            "resetting env. episode 140.000000, reward total was -18.000000. running mean: -18.947528\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -18.968053\n",
            "resetting env. episode 142.000000, reward total was -17.000000. running mean: -18.948372\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -18.958889\n",
            "resetting env. episode 144.000000, reward total was -17.000000. running mean: -18.939300\n",
            "resetting env. episode 145.000000, reward total was -15.000000. running mean: -18.899907\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -18.920908\n",
            "resetting env. episode 147.000000, reward total was -15.000000. running mean: -18.881699\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -18.882882\n",
            "resetting env. episode 149.000000, reward total was -17.000000. running mean: -18.864053\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -18.875412\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -18.886658\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -18.887792\n",
            "resetting env. episode 153.000000, reward total was -18.000000. running mean: -18.878914\n",
            "resetting env. episode 154.000000, reward total was -18.000000. running mean: -18.870124\n",
            "resetting env. episode 155.000000, reward total was -16.000000. running mean: -18.841423\n",
            "resetting env. episode 156.000000, reward total was -15.000000. running mean: -18.803009\n",
            "resetting env. episode 157.000000, reward total was -18.000000. running mean: -18.794979\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -18.807029\n",
            "resetting env. episode 159.000000, reward total was -17.000000. running mean: -18.788959\n",
            "resetting env. episode 160.000000, reward total was -15.000000. running mean: -18.751069\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -18.763559\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -18.765923\n",
            "resetting env. episode 163.000000, reward total was -18.000000. running mean: -18.758264\n",
            "resetting env. episode 164.000000, reward total was -18.000000. running mean: -18.750681\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -18.753174\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -18.755643\n",
            "resetting env. episode 167.000000, reward total was -17.000000. running mean: -18.738086\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -18.750705\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -18.743198\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -18.745766\n",
            "resetting env. episode 171.000000, reward total was -18.000000. running mean: -18.738309\n",
            "resetting env. episode 172.000000, reward total was -18.000000. running mean: -18.730925\n",
            "resetting env. episode 173.000000, reward total was -15.000000. running mean: -18.693616\n",
            "resetting env. episode 174.000000, reward total was -16.000000. running mean: -18.666680\n",
            "resetting env. episode 175.000000, reward total was -17.000000. running mean: -18.650013\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -18.653513\n",
            "resetting env. episode 177.000000, reward total was -16.000000. running mean: -18.626978\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -18.630708\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -18.634401\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -18.638057\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -18.641677\n",
            "resetting env. episode 182.000000, reward total was -16.000000. running mean: -18.615260\n",
            "resetting env. episode 183.000000, reward total was -14.000000. running mean: -18.569107\n",
            "resetting env. episode 184.000000, reward total was -18.000000. running mean: -18.563416\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -18.567782\n",
            "resetting env. episode 186.000000, reward total was -15.000000. running mean: -18.532104\n",
            "resetting env. episode 187.000000, reward total was -16.000000. running mean: -18.506783\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -18.521715\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -18.526498\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -18.531233\n",
            "resetting env. episode 191.000000, reward total was -18.000000. running mean: -18.525921\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -18.550662\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -18.555155\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -18.559603\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -18.574007\n",
            "resetting env. episode 196.000000, reward total was -16.000000. running mean: -18.548267\n",
            "resetting env. episode 197.000000, reward total was -18.000000. running mean: -18.542785\n",
            "resetting env. episode 198.000000, reward total was -18.000000. running mean: -18.537357\n",
            "resetting env. episode 199.000000, reward total was -17.000000. running mean: -18.521983\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -18.536763\n",
            "resetting env. episode 201.000000, reward total was -18.000000. running mean: -18.531396\n",
            "resetting env. episode 202.000000, reward total was -15.000000. running mean: -18.496082\n",
            "resetting env. episode 203.000000, reward total was -17.000000. running mean: -18.481121\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -18.506310\n",
            "resetting env. episode 205.000000, reward total was -17.000000. running mean: -18.491247\n",
            "resetting env. episode 206.000000, reward total was -18.000000. running mean: -18.486334\n",
            "resetting env. episode 207.000000, reward total was -18.000000. running mean: -18.481471\n",
            "resetting env. episode 208.000000, reward total was -14.000000. running mean: -18.436656\n",
            "resetting env. episode 209.000000, reward total was -18.000000. running mean: -18.432290\n",
            "resetting env. episode 210.000000, reward total was -14.000000. running mean: -18.387967\n",
            "resetting env. episode 211.000000, reward total was -13.000000. running mean: -18.334087\n",
            "resetting env. episode 212.000000, reward total was -18.000000. running mean: -18.330746\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -18.357439\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -18.373864\n",
            "resetting env. episode 215.000000, reward total was -17.000000. running mean: -18.360126\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -18.366524\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -18.362859\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -18.379231\n",
            "resetting env. episode 219.000000, reward total was -17.000000. running mean: -18.365438\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -18.371784\n",
            "resetting env. episode 221.000000, reward total was -18.000000. running mean: -18.368066\n",
            "resetting env. episode 222.000000, reward total was -18.000000. running mean: -18.364385\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -18.370742\n",
            "resetting env. episode 224.000000, reward total was -17.000000. running mean: -18.357034\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -18.363464\n",
            "resetting env. episode 226.000000, reward total was -18.000000. running mean: -18.359829\n",
            "resetting env. episode 227.000000, reward total was -14.000000. running mean: -18.316231\n",
            "resetting env. episode 228.000000, reward total was -13.000000. running mean: -18.263069\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -18.280438\n",
            "resetting env. episode 230.000000, reward total was -18.000000. running mean: -18.277633\n",
            "resetting env. episode 231.000000, reward total was -13.000000. running mean: -18.224857\n",
            "resetting env. episode 232.000000, reward total was -18.000000. running mean: -18.222609\n",
            "resetting env. episode 233.000000, reward total was -15.000000. running mean: -18.190382\n",
            "resetting env. episode 234.000000, reward total was -17.000000. running mean: -18.178479\n",
            "resetting env. episode 235.000000, reward total was -18.000000. running mean: -18.176694\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -18.194927\n",
            "resetting env. episode 237.000000, reward total was -17.000000. running mean: -18.182978\n",
            "resetting env. episode 238.000000, reward total was -18.000000. running mean: -18.181148\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -18.199336\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -18.217343\n",
            "resetting env. episode 241.000000, reward total was -18.000000. running mean: -18.215170\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -18.223018\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -18.230788\n",
            "resetting env. episode 244.000000, reward total was -16.000000. running mean: -18.208480\n",
            "resetting env. episode 245.000000, reward total was -16.000000. running mean: -18.186395\n",
            "resetting env. episode 246.000000, reward total was -17.000000. running mean: -18.174531\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -18.172786\n",
            "resetting env. episode 248.000000, reward total was -16.000000. running mean: -18.151058\n",
            "resetting env. episode 249.000000, reward total was -18.000000. running mean: -18.149547\n",
            "resetting env. episode 250.000000, reward total was -12.000000. running mean: -18.088052\n",
            "resetting env. episode 251.000000, reward total was -17.000000. running mean: -18.077171\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -18.086400\n",
            "resetting env. episode 253.000000, reward total was -16.000000. running mean: -18.065536\n",
            "resetting env. episode 254.000000, reward total was -18.000000. running mean: -18.064880\n",
            "resetting env. episode 255.000000, reward total was -17.000000. running mean: -18.054231\n",
            "resetting env. episode 256.000000, reward total was -16.000000. running mean: -18.033689\n",
            "resetting env. episode 257.000000, reward total was -15.000000. running mean: -18.003352\n",
            "resetting env. episode 258.000000, reward total was -16.000000. running mean: -17.983319\n",
            "resetting env. episode 259.000000, reward total was -13.000000. running mean: -17.933486\n",
            "resetting env. episode 260.000000, reward total was -18.000000. running mean: -17.934151\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -17.944809\n",
            "resetting env. episode 262.000000, reward total was -17.000000. running mean: -17.935361\n",
            "resetting env. episode 263.000000, reward total was -17.000000. running mean: -17.926008\n",
            "resetting env. episode 264.000000, reward total was -14.000000. running mean: -17.886747\n",
            "resetting env. episode 265.000000, reward total was -16.000000. running mean: -17.867880\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -17.879201\n",
            "resetting env. episode 267.000000, reward total was -18.000000. running mean: -17.880409\n",
            "resetting env. episode 268.000000, reward total was -18.000000. running mean: -17.881605\n",
            "resetting env. episode 269.000000, reward total was -16.000000. running mean: -17.862789\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -17.894161\n",
            "resetting env. episode 271.000000, reward total was -15.000000. running mean: -17.865220\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -17.876567\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -17.887802\n",
            "resetting env. episode 274.000000, reward total was -13.000000. running mean: -17.838924\n",
            "resetting env. episode 275.000000, reward total was -17.000000. running mean: -17.830534\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -17.842229\n",
            "resetting env. episode 277.000000, reward total was -16.000000. running mean: -17.823807\n",
            "resetting env. episode 278.000000, reward total was -18.000000. running mean: -17.825569\n",
            "resetting env. episode 279.000000, reward total was -12.000000. running mean: -17.767313\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -17.779640\n",
            "resetting env. episode 281.000000, reward total was -18.000000. running mean: -17.781843\n",
            "resetting env. episode 282.000000, reward total was -16.000000. running mean: -17.764025\n",
            "resetting env. episode 283.000000, reward total was -14.000000. running mean: -17.726385\n",
            "resetting env. episode 284.000000, reward total was -14.000000. running mean: -17.689121\n",
            "resetting env. episode 285.000000, reward total was -18.000000. running mean: -17.692230\n",
            "resetting env. episode 286.000000, reward total was -18.000000. running mean: -17.695307\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -17.708354\n",
            "resetting env. episode 288.000000, reward total was -15.000000. running mean: -17.681271\n",
            "resetting env. episode 289.000000, reward total was -16.000000. running mean: -17.664458\n",
            "resetting env. episode 290.000000, reward total was -17.000000. running mean: -17.657814\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -17.671235\n",
            "resetting env. episode 292.000000, reward total was -17.000000. running mean: -17.664523\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -17.677878\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -17.701099\n",
            "resetting env. episode 295.000000, reward total was -17.000000. running mean: -17.694088\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -17.727147\n",
            "resetting env. episode 297.000000, reward total was -17.000000. running mean: -17.719876\n",
            "resetting env. episode 298.000000, reward total was -16.000000. running mean: -17.702677\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -17.725650\n",
            "resetting env. episode 300.000000, reward total was -18.000000. running mean: -17.728394\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -17.741110\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -17.753699\n",
            "resetting env. episode 303.000000, reward total was -18.000000. running mean: -17.756162\n",
            "resetting env. episode 304.000000, reward total was -17.000000. running mean: -17.748600\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -17.771114\n",
            "resetting env. episode 306.000000, reward total was -17.000000. running mean: -17.763403\n",
            "resetting env. episode 307.000000, reward total was -15.000000. running mean: -17.735769\n",
            "resetting env. episode 308.000000, reward total was -15.000000. running mean: -17.708411\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -17.731327\n",
            "resetting env. episode 310.000000, reward total was -16.000000. running mean: -17.714014\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -17.726874\n",
            "resetting env. episode 312.000000, reward total was -18.000000. running mean: -17.729605\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -17.742309\n",
            "resetting env. episode 314.000000, reward total was -17.000000. running mean: -17.734886\n",
            "resetting env. episode 315.000000, reward total was -18.000000. running mean: -17.737537\n",
            "resetting env. episode 316.000000, reward total was -18.000000. running mean: -17.740162\n",
            "resetting env. episode 317.000000, reward total was -16.000000. running mean: -17.722760\n",
            "resetting env. episode 318.000000, reward total was -15.000000. running mean: -17.695532\n",
            "resetting env. episode 319.000000, reward total was -17.000000. running mean: -17.688577\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -17.691691\n",
            "resetting env. episode 321.000000, reward total was -14.000000. running mean: -17.654774\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -17.678227\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -17.711444\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -17.724330\n",
            "resetting env. episode 325.000000, reward total was -13.000000. running mean: -17.677087\n",
            "resetting env. episode 326.000000, reward total was -14.000000. running mean: -17.640316\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -17.653913\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -17.667373\n",
            "resetting env. episode 329.000000, reward total was -11.000000. running mean: -17.600700\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -17.624693\n",
            "resetting env. episode 331.000000, reward total was -16.000000. running mean: -17.608446\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -17.622361\n",
            "resetting env. episode 333.000000, reward total was -18.000000. running mean: -17.626138\n",
            "resetting env. episode 334.000000, reward total was -17.000000. running mean: -17.619876\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -17.643678\n",
            "resetting env. episode 336.000000, reward total was -18.000000. running mean: -17.647241\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -17.660768\n",
            "resetting env. episode 338.000000, reward total was -15.000000. running mean: -17.634161\n",
            "resetting env. episode 339.000000, reward total was -14.000000. running mean: -17.597819\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -17.611841\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -17.615722\n",
            "resetting env. episode 342.000000, reward total was -16.000000. running mean: -17.599565\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -17.613570\n",
            "resetting env. episode 344.000000, reward total was -14.000000. running mean: -17.577434\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -17.591660\n",
            "resetting env. episode 346.000000, reward total was -16.000000. running mean: -17.575743\n",
            "resetting env. episode 347.000000, reward total was -17.000000. running mean: -17.569986\n",
            "resetting env. episode 348.000000, reward total was -18.000000. running mean: -17.574286\n",
            "resetting env. episode 349.000000, reward total was -17.000000. running mean: -17.568543\n",
            "resetting env. episode 350.000000, reward total was -14.000000. running mean: -17.532857\n",
            "resetting env. episode 351.000000, reward total was -14.000000. running mean: -17.497529\n",
            "resetting env. episode 352.000000, reward total was -17.000000. running mean: -17.492554\n",
            "resetting env. episode 353.000000, reward total was -16.000000. running mean: -17.477628\n",
            "resetting env. episode 354.000000, reward total was -18.000000. running mean: -17.482852\n",
            "resetting env. episode 355.000000, reward total was -15.000000. running mean: -17.458023\n",
            "resetting env. episode 356.000000, reward total was -16.000000. running mean: -17.443443\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -17.469009\n",
            "resetting env. episode 358.000000, reward total was -17.000000. running mean: -17.464318\n",
            "resetting env. episode 359.000000, reward total was -17.000000. running mean: -17.459675\n",
            "resetting env. episode 360.000000, reward total was -9.000000. running mean: -17.375079\n",
            "resetting env. episode 361.000000, reward total was -17.000000. running mean: -17.371328\n",
            "resetting env. episode 362.000000, reward total was -16.000000. running mean: -17.357614\n",
            "resetting env. episode 363.000000, reward total was -16.000000. running mean: -17.344038\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -17.360598\n",
            "resetting env. episode 365.000000, reward total was -17.000000. running mean: -17.356992\n",
            "resetting env. episode 366.000000, reward total was -16.000000. running mean: -17.343422\n",
            "resetting env. episode 367.000000, reward total was -12.000000. running mean: -17.289988\n",
            "resetting env. episode 368.000000, reward total was -18.000000. running mean: -17.297088\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -17.314117\n",
            "resetting env. episode 370.000000, reward total was -14.000000. running mean: -17.280976\n",
            "resetting env. episode 371.000000, reward total was -13.000000. running mean: -17.238166\n",
            "resetting env. episode 372.000000, reward total was -15.000000. running mean: -17.215784\n",
            "resetting env. episode 373.000000, reward total was -14.000000. running mean: -17.183627\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -17.211790\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -17.229672\n",
            "resetting env. episode 376.000000, reward total was -16.000000. running mean: -17.217376\n",
            "resetting env. episode 377.000000, reward total was -17.000000. running mean: -17.215202\n",
            "resetting env. episode 378.000000, reward total was -18.000000. running mean: -17.223050\n",
            "resetting env. episode 379.000000, reward total was -15.000000. running mean: -17.200819\n",
            "resetting env. episode 380.000000, reward total was -13.000000. running mean: -17.158811\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -17.197223\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -17.215251\n",
            "resetting env. episode 383.000000, reward total was -17.000000. running mean: -17.213098\n",
            "resetting env. episode 384.000000, reward total was -17.000000. running mean: -17.210967\n",
            "resetting env. episode 385.000000, reward total was -17.000000. running mean: -17.208858\n",
            "resetting env. episode 386.000000, reward total was -18.000000. running mean: -17.216769\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -17.224601\n",
            "resetting env. episode 388.000000, reward total was -15.000000. running mean: -17.202355\n",
            "resetting env. episode 389.000000, reward total was -18.000000. running mean: -17.210332\n",
            "resetting env. episode 390.000000, reward total was -17.000000. running mean: -17.208229\n",
            "resetting env. episode 391.000000, reward total was -12.000000. running mean: -17.156146\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -17.184585\n",
            "resetting env. episode 393.000000, reward total was -14.000000. running mean: -17.152739\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -17.171212\n",
            "resetting env. episode 395.000000, reward total was -18.000000. running mean: -17.179499\n",
            "resetting env. episode 396.000000, reward total was -17.000000. running mean: -17.177704\n",
            "resetting env. episode 397.000000, reward total was -18.000000. running mean: -17.185927\n",
            "resetting env. episode 398.000000, reward total was -17.000000. running mean: -17.184068\n",
            "resetting env. episode 399.000000, reward total was -14.000000. running mean: -17.152227\n",
            "resetting env. episode 400.000000, reward total was -15.000000. running mean: -17.130705\n",
            "resetting env. episode 401.000000, reward total was -13.000000. running mean: -17.089398\n",
            "resetting env. episode 402.000000, reward total was -14.000000. running mean: -17.058504\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -17.077919\n",
            "resetting env. episode 404.000000, reward total was -15.000000. running mean: -17.057140\n",
            "resetting env. episode 405.000000, reward total was -17.000000. running mean: -17.056569\n",
            "resetting env. episode 406.000000, reward total was -12.000000. running mean: -17.006003\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -17.025943\n",
            "resetting env. episode 408.000000, reward total was -13.000000. running mean: -16.985683\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -17.005827\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -17.025768\n",
            "resetting env. episode 411.000000, reward total was -16.000000. running mean: -17.015511\n",
            "resetting env. episode 412.000000, reward total was -14.000000. running mean: -16.985356\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -17.005502\n",
            "resetting env. episode 414.000000, reward total was -14.000000. running mean: -16.975447\n",
            "resetting env. episode 415.000000, reward total was -13.000000. running mean: -16.935692\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -16.956336\n",
            "resetting env. episode 417.000000, reward total was -17.000000. running mean: -16.956772\n",
            "resetting env. episode 418.000000, reward total was -15.000000. running mean: -16.937204\n",
            "resetting env. episode 419.000000, reward total was -17.000000. running mean: -16.937832\n",
            "resetting env. episode 420.000000, reward total was -17.000000. running mean: -16.938454\n",
            "resetting env. episode 421.000000, reward total was -16.000000. running mean: -16.929070\n",
            "resetting env. episode 422.000000, reward total was -12.000000. running mean: -16.879779\n",
            "resetting env. episode 423.000000, reward total was -15.000000. running mean: -16.860981\n",
            "resetting env. episode 424.000000, reward total was -18.000000. running mean: -16.872371\n",
            "resetting env. episode 425.000000, reward total was -17.000000. running mean: -16.873648\n",
            "resetting env. episode 426.000000, reward total was -12.000000. running mean: -16.824911\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -16.846662\n",
            "resetting env. episode 428.000000, reward total was -16.000000. running mean: -16.838195\n",
            "resetting env. episode 429.000000, reward total was -17.000000. running mean: -16.839813\n",
            "resetting env. episode 430.000000, reward total was -18.000000. running mean: -16.851415\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -16.872901\n",
            "resetting env. episode 432.000000, reward total was -15.000000. running mean: -16.854172\n",
            "resetting env. episode 433.000000, reward total was -17.000000. running mean: -16.855630\n",
            "resetting env. episode 434.000000, reward total was -14.000000. running mean: -16.827074\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -16.848803\n",
            "resetting env. episode 436.000000, reward total was -16.000000. running mean: -16.840315\n",
            "resetting env. episode 437.000000, reward total was -15.000000. running mean: -16.821912\n",
            "resetting env. episode 438.000000, reward total was -14.000000. running mean: -16.793693\n",
            "resetting env. episode 439.000000, reward total was -15.000000. running mean: -16.775756\n",
            "resetting env. episode 440.000000, reward total was -17.000000. running mean: -16.777999\n",
            "resetting env. episode 441.000000, reward total was -18.000000. running mean: -16.790219\n",
            "resetting env. episode 442.000000, reward total was -16.000000. running mean: -16.782316\n",
            "resetting env. episode 443.000000, reward total was -17.000000. running mean: -16.784493\n",
            "resetting env. episode 444.000000, reward total was -16.000000. running mean: -16.776648\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -16.808882\n",
            "resetting env. episode 446.000000, reward total was -17.000000. running mean: -16.810793\n",
            "resetting env. episode 447.000000, reward total was -12.000000. running mean: -16.762685\n",
            "resetting env. episode 448.000000, reward total was -14.000000. running mean: -16.735058\n",
            "resetting env. episode 449.000000, reward total was -17.000000. running mean: -16.737708\n",
            "resetting env. episode 450.000000, reward total was -17.000000. running mean: -16.740331\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -16.762927\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -16.795298\n",
            "resetting env. episode 453.000000, reward total was -7.000000. running mean: -16.697345\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -16.720372\n",
            "resetting env. episode 455.000000, reward total was -17.000000. running mean: -16.723168\n",
            "resetting env. episode 456.000000, reward total was -16.000000. running mean: -16.715936\n",
            "resetting env. episode 457.000000, reward total was -17.000000. running mean: -16.718777\n",
            "resetting env. episode 458.000000, reward total was -15.000000. running mean: -16.701589\n",
            "resetting env. episode 459.000000, reward total was -14.000000. running mean: -16.674573\n",
            "resetting env. episode 460.000000, reward total was -15.000000. running mean: -16.657827\n",
            "resetting env. episode 461.000000, reward total was -17.000000. running mean: -16.661249\n",
            "resetting env. episode 462.000000, reward total was -16.000000. running mean: -16.654637\n",
            "resetting env. episode 463.000000, reward total was -13.000000. running mean: -16.618090\n",
            "resetting env. episode 464.000000, reward total was -16.000000. running mean: -16.611909\n",
            "resetting env. episode 465.000000, reward total was -17.000000. running mean: -16.615790\n",
            "resetting env. episode 466.000000, reward total was -17.000000. running mean: -16.619632\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -16.643436\n",
            "resetting env. episode 468.000000, reward total was -16.000000. running mean: -16.637002\n",
            "resetting env. episode 469.000000, reward total was -17.000000. running mean: -16.640632\n",
            "resetting env. episode 470.000000, reward total was -14.000000. running mean: -16.614225\n",
            "resetting env. episode 471.000000, reward total was -14.000000. running mean: -16.588083\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -16.612202\n",
            "resetting env. episode 473.000000, reward total was -16.000000. running mean: -16.606080\n",
            "resetting env. episode 474.000000, reward total was -18.000000. running mean: -16.620019\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -16.653819\n",
            "resetting env. episode 476.000000, reward total was -13.000000. running mean: -16.617281\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -16.641108\n",
            "resetting env. episode 478.000000, reward total was -17.000000. running mean: -16.644697\n",
            "resetting env. episode 479.000000, reward total was -17.000000. running mean: -16.648250\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -16.681768\n",
            "resetting env. episode 481.000000, reward total was -10.000000. running mean: -16.614950\n",
            "resetting env. episode 482.000000, reward total was -13.000000. running mean: -16.578800\n",
            "resetting env. episode 483.000000, reward total was -16.000000. running mean: -16.573012\n",
            "resetting env. episode 484.000000, reward total was -17.000000. running mean: -16.577282\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -16.601510\n",
            "resetting env. episode 486.000000, reward total was -16.000000. running mean: -16.595494\n",
            "resetting env. episode 487.000000, reward total was -17.000000. running mean: -16.599539\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -16.633544\n",
            "resetting env. episode 489.000000, reward total was -14.000000. running mean: -16.607209\n",
            "resetting env. episode 490.000000, reward total was -17.000000. running mean: -16.611137\n",
            "resetting env. episode 491.000000, reward total was -14.000000. running mean: -16.585025\n",
            "resetting env. episode 492.000000, reward total was -14.000000. running mean: -16.559175\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -16.603583\n",
            "resetting env. episode 494.000000, reward total was -15.000000. running mean: -16.587547\n",
            "resetting env. episode 495.000000, reward total was -17.000000. running mean: -16.591672\n",
            "resetting env. episode 496.000000, reward total was -17.000000. running mean: -16.595755\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -16.629798\n",
            "resetting env. episode 498.000000, reward total was -14.000000. running mean: -16.603500\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -16.647465\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -16.670990\n",
            "CPU times: user 43min 36s, sys: 19min 49s, total: 1h 3min 25s\n",
            "Wall time: 32min 44s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "53a25ecf-57ad-46a2-bbc9-b822beb1c67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "925a2b7a-9d01-40c4-e89c-c831321f2fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 4.000000, reward total was -19.000000. running mean: -20.009701\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.019604\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.019408\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.019214\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.029022\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.028732\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.038444\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.048060\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.057579\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.067003\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.066333\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.055670\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -20.045113\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.054662\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.054116\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.063574\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.072939\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.072209\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.071487\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.060772\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.070165\n",
            "resetting env. episode 25.000000, reward total was -18.000000. running mean: -20.049463\n",
            "resetting env. episode 26.000000, reward total was -18.000000. running mean: -20.028968\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.018679\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.018492\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.028307\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.038024\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.047644\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.047167\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.036695\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.046329\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.055865\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.065307\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.074654\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.083907\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -20.063068\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.072437\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.081713\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.090896\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.099987\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.098987\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.107997\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.116917\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.125748\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.134490\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.133146\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.141814\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.140396\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.148992\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.157502\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.165927\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.164268\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.172625\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.170899\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.169190\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.167498\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.175823\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.164065\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.162424\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.170800\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.169092\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.177401\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.185627\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.193771\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.201833\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -20.189815\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.197916\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.205937\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.213878\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.211739\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.219622\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.227426\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.225151\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.232900\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.230571\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.228265\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.235982\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.233623\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.231286\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.238974\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.246584\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.254118\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.261577\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.268961\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.276271\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.283509\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.280674\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.287867\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.294988\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.302038\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.309018\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.305928\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -20.292868\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.299940\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.286940\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.294071\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.301130\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.308119\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.305038\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.311987\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.308868\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -20.295779\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.302821\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.309793\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.306695\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.303628\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.300592\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.297586\n",
            "resetting env. episode 112.000000, reward total was -18.000000. running mean: -20.274610\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.281864\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.279045\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.276255\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -20.263492\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.270857\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.278149\n",
            "resetting env. episode 119.000000, reward total was -18.000000. running mean: -20.255367\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.262814\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.260185\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.267584\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.274908\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.272159\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.279437\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.276643\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.283876\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.271037\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.258327\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.255744\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.253186\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.260655\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.248048\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.255567\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.263012\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -20.250382\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.247878\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.255399\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.262845\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.260217\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.257615\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.265038\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.272388\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.269664\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.276967\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.284198\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.281356\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.268542\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.275857\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.283098\n",
            "resetting env. episode 151.000000, reward total was -18.000000. running mean: -20.260267\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.267665\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.264988\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.272338\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.279615\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.286819\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.283950\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.281111\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.278300\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.275517\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.272762\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.280034\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.277234\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.274461\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.281717\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.288900\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.286011\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.273150\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -20.250419\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.257915\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.255336\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.262782\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.260154\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.247553\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.255077\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.262527\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.259901\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.267302\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.274629\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.271883\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.279164\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.276372\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.273609\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.280873\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.278064\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.285283\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.272430\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.269706\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.257009\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.264439\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.271795\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.269077\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.276386\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.283622\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.290786\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.297878\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.284899\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.292050\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.299130\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.306138\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.313077\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.319946\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.316747\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.303579\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.310544\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.317438\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.324264\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.331021\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.327711\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.314434\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.301289\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.298277\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.305294\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.312241\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.319118\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.325927\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.312668\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.299541\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.296546\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.303580\n",
            "resetting env. episode 221.000000, reward total was -19.000000. running mean: -20.290545\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.287639\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.274763\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.272015\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.269295\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.276602\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.273836\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.271098\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.278387\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.275603\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.262847\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.270218\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.277516\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.274741\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.281994\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.289174\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.286282\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.293419\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.290485\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.297580\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.294604\n",
            "resetting env. episode 242.000000, reward total was -18.000000. running mean: -20.271658\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.278942\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.286152\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.293291\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.280358\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -20.257554\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.254979\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.262429\n",
            "resetting env. episode 250.000000, reward total was -17.000000. running mean: -20.229805\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.227507\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.225231\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.232979\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.230649\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.228343\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.236059\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.233699\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.241362\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.248948\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.256459\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.263894\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.271255\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.258543\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.255957\n",
            "resetting env. episode 265.000000, reward total was -18.000000. running mean: -20.233398\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.241064\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.228653\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.226367\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.234103\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.241762\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.239344\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.236951\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.234581\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.242235\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.249813\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.257315\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.264742\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.252094\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.239573\n",
            "resetting env. episode 280.000000, reward total was -18.000000. running mean: -20.217178\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.215006\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.222856\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.230627\n",
            "resetting env. episode 284.000000, reward total was -18.000000. running mean: -20.208321\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.206238\n",
            "resetting env. episode 286.000000, reward total was -18.000000. running mean: -20.184175\n",
            "resetting env. episode 287.000000, reward total was -17.000000. running mean: -20.152334\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.160810\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.169202\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.157510\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.165935\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -20.144276\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.152833\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.161305\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.169692\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.167995\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.176315\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.164552\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.162906\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.151277\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.149764\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.138267\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.136884\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.145515\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.154060\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.162519\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.160894\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.149285\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.147792\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.136314\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.144951\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.143502\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.142067\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.140646\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.139240\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.147847\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.156369\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.144805\n",
            "resetting env. episode 319.000000, reward total was -17.000000. running mean: -20.113357\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.092223\n",
            "resetting env. episode 321.000000, reward total was -18.000000. running mean: -20.071301\n",
            "resetting env. episode 322.000000, reward total was -18.000000. running mean: -20.050588\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.060082\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.069482\n",
            "resetting env. episode 325.000000, reward total was -18.000000. running mean: -20.048787\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.058299\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.057716\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.057139\n",
            "resetting env. episode 329.000000, reward total was -18.000000. running mean: -20.036567\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.046202\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.055740\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.065182\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.074530\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.073785\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.073047\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.082317\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.091494\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.090579\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.079673\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.088876\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.077987\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.067208\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.056535\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.055970\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.065410\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.054756\n",
            "resetting env. episode 347.000000, reward total was -18.000000. running mean: -20.034209\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.033867\n",
            "resetting env. episode 349.000000, reward total was -18.000000. running mean: -20.013528\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.003393\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.013359\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.003225\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -19.993193\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.003261\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.013228\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.023096\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.022865\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -20.012636\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.022510\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.022285\n",
            "resetting env. episode 361.000000, reward total was -18.000000. running mean: -20.002062\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -19.992042\n",
            "resetting env. episode 363.000000, reward total was -18.000000. running mean: -19.972121\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -19.962400\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -19.952776\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -19.963248\n",
            "resetting env. episode 367.000000, reward total was -18.000000. running mean: -19.943616\n",
            "resetting env. episode 368.000000, reward total was -17.000000. running mean: -19.914180\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -19.905038\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -19.915987\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -19.926827\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -19.937559\n",
            "resetting env. episode 373.000000, reward total was -14.000000. running mean: -19.878184\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -19.889402\n",
            "resetting env. episode 375.000000, reward total was -17.000000. running mean: -19.860508\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -19.871903\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -19.883184\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -19.874352\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -19.885608\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -19.876752\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -19.867985\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -19.879305\n",
            "resetting env. episode 383.000000, reward total was -17.000000. running mean: -19.850512\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -19.842007\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -19.853587\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -19.865051\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -19.876400\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -19.877636\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -19.888860\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -19.889971\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -19.881072\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -19.882261\n",
            "resetting env. episode 393.000000, reward total was -17.000000. running mean: -19.853438\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -19.864904\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -19.866255\n",
            "resetting env. episode 396.000000, reward total was -17.000000. running mean: -19.837592\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -19.829216\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -19.840924\n",
            "resetting env. episode 399.000000, reward total was -18.000000. running mean: -19.822515\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -19.824290\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -19.826047\n",
            "resetting env. episode 402.000000, reward total was -17.000000. running mean: -19.797786\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -19.789809\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -19.801910\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -19.793891\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -19.785952\n",
            "resetting env. episode 407.000000, reward total was -18.000000. running mean: -19.768093\n",
            "resetting env. episode 408.000000, reward total was -17.000000. running mean: -19.740412\n",
            "resetting env. episode 409.000000, reward total was -16.000000. running mean: -19.703008\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -19.715978\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -19.718818\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -19.721630\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -19.724414\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -19.727169\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -19.729898\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -19.742599\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -19.745173\n",
            "resetting env. episode 418.000000, reward total was -17.000000. running mean: -19.717721\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -19.710544\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -19.703438\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -19.706404\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -19.719340\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -19.722147\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -19.734925\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -19.737576\n",
            "resetting env. episode 426.000000, reward total was -17.000000. running mean: -19.710200\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -19.703098\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -19.716067\n",
            "resetting env. episode 429.000000, reward total was -18.000000. running mean: -19.698906\n",
            "resetting env. episode 430.000000, reward total was -17.000000. running mean: -19.671917\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -19.665198\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -19.658546\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -19.651961\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -19.645441\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -19.658987\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -19.672397\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -19.675673\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -19.678916\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -19.692127\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -19.685206\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -19.698354\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -19.711370\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -19.714256\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -19.717114\n",
            "resetting env. episode 445.000000, reward total was -18.000000. running mean: -19.699943\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -19.702943\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -19.695914\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -19.708955\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -19.721865\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -19.724647\n",
            "resetting env. episode 451.000000, reward total was -16.000000. running mean: -19.687400\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -19.680526\n",
            "resetting env. episode 453.000000, reward total was -18.000000. running mean: -19.663721\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -19.657084\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -19.670513\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -19.683808\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -19.676970\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -19.670200\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -19.673498\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -19.686763\n",
            "resetting env. episode 461.000000, reward total was -17.000000. running mean: -19.659895\n",
            "resetting env. episode 462.000000, reward total was -17.000000. running mean: -19.633296\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -19.636963\n",
            "resetting env. episode 464.000000, reward total was -18.000000. running mean: -19.620594\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -19.624388\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -19.628144\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -19.621862\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -19.635644\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -19.649287\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -19.642795\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -19.646367\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -19.649903\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -19.663404\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -19.676770\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -19.690002\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -19.683102\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -19.676271\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -19.659508\n",
            "resetting env. episode 479.000000, reward total was -17.000000. running mean: -19.632913\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -19.626584\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -19.620318\n",
            "resetting env. episode 482.000000, reward total was -17.000000. running mean: -19.594115\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -19.608174\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -19.622092\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -19.605871\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -19.599813\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -19.593814\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -19.587876\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -19.591998\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -19.606078\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -19.600017\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -19.594017\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -19.588076\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -19.602196\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -19.596174\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -19.600212\n",
            "resetting env. episode 497.000000, reward total was -17.000000. running mean: -19.574210\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -19.568468\n",
            "resetting env. episode 499.000000, reward total was -18.000000. running mean: -19.552783\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -19.547255\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -19.551783\n",
            "resetting env. episode 502.000000, reward total was -20.000000. running mean: -19.556265\n",
            "resetting env. episode 503.000000, reward total was -19.000000. running mean: -19.550702\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -19.555195\n",
            "resetting env. episode 505.000000, reward total was -18.000000. running mean: -19.539643\n",
            "resetting env. episode 506.000000, reward total was -19.000000. running mean: -19.534247\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -19.538904\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -19.553515\n",
            "resetting env. episode 509.000000, reward total was -19.000000. running mean: -19.547980\n",
            "resetting env. episode 510.000000, reward total was -18.000000. running mean: -19.532500\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -19.537175\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -19.551804\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -19.566286\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -19.580623\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -19.584817\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -19.588968\n",
            "resetting env. episode 517.000000, reward total was -17.000000. running mean: -19.563079\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -19.577448\n",
            "resetting env. episode 519.000000, reward total was -17.000000. running mean: -19.551673\n",
            "resetting env. episode 520.000000, reward total was -19.000000. running mean: -19.546157\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -19.560695\n",
            "resetting env. episode 522.000000, reward total was -19.000000. running mean: -19.555088\n",
            "resetting env. episode 523.000000, reward total was -18.000000. running mean: -19.539537\n",
            "resetting env. episode 524.000000, reward total was -19.000000. running mean: -19.534142\n",
            "resetting env. episode 525.000000, reward total was -17.000000. running mean: -19.508800\n",
            "resetting env. episode 526.000000, reward total was -19.000000. running mean: -19.503712\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -19.518675\n",
            "resetting env. episode 528.000000, reward total was -19.000000. running mean: -19.513489\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -19.518354\n",
            "resetting env. episode 530.000000, reward total was -19.000000. running mean: -19.513170\n",
            "resetting env. episode 531.000000, reward total was -19.000000. running mean: -19.508038\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -19.522958\n",
            "resetting env. episode 533.000000, reward total was -19.000000. running mean: -19.517729\n",
            "resetting env. episode 534.000000, reward total was -20.000000. running mean: -19.522551\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -19.527326\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -19.542052\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -19.556632\n",
            "resetting env. episode 538.000000, reward total was -19.000000. running mean: -19.551066\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -19.565555\n",
            "resetting env. episode 540.000000, reward total was -19.000000. running mean: -19.559899\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -19.564300\n",
            "resetting env. episode 542.000000, reward total was -18.000000. running mean: -19.548657\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -19.563171\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -19.567539\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -19.581864\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -19.576045\n",
            "resetting env. episode 547.000000, reward total was -19.000000. running mean: -19.570285\n",
            "resetting env. episode 548.000000, reward total was -14.000000. running mean: -19.514582\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -19.519436\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -19.524242\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -19.538999\n",
            "resetting env. episode 552.000000, reward total was -17.000000. running mean: -19.513609\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -19.518473\n",
            "resetting env. episode 554.000000, reward total was -18.000000. running mean: -19.503288\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -19.518256\n",
            "resetting env. episode 556.000000, reward total was -18.000000. running mean: -19.503073\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -19.518042\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -19.522862\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -19.527633\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -19.532357\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -19.547033\n",
            "resetting env. episode 562.000000, reward total was -19.000000. running mean: -19.541563\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -19.556147\n",
            "resetting env. episode 564.000000, reward total was -19.000000. running mean: -19.550586\n",
            "resetting env. episode 565.000000, reward total was -19.000000. running mean: -19.545080\n",
            "resetting env. episode 566.000000, reward total was -19.000000. running mean: -19.539629\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -19.544233\n",
            "resetting env. episode 568.000000, reward total was -18.000000. running mean: -19.528791\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -19.543503\n",
            "resetting env. episode 570.000000, reward total was -18.000000. running mean: -19.528068\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -19.542787\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -19.547359\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -19.551885\n",
            "resetting env. episode 574.000000, reward total was -19.000000. running mean: -19.546367\n",
            "resetting env. episode 575.000000, reward total was -19.000000. running mean: -19.540903\n",
            "resetting env. episode 576.000000, reward total was -18.000000. running mean: -19.525494\n",
            "resetting env. episode 577.000000, reward total was -19.000000. running mean: -19.520239\n",
            "resetting env. episode 578.000000, reward total was -19.000000. running mean: -19.515037\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -19.529886\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -19.534587\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -19.529242\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -19.543949\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -19.548510\n",
            "resetting env. episode 584.000000, reward total was -19.000000. running mean: -19.543025\n",
            "resetting env. episode 585.000000, reward total was -18.000000. running mean: -19.527594\n",
            "resetting env. episode 586.000000, reward total was -19.000000. running mean: -19.522318\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -19.527095\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -19.521824\n",
            "resetting env. episode 589.000000, reward total was -19.000000. running mean: -19.516606\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -19.531440\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -19.546125\n",
            "resetting env. episode 592.000000, reward total was -19.000000. running mean: -19.540664\n",
            "resetting env. episode 593.000000, reward total was -15.000000. running mean: -19.495258\n",
            "resetting env. episode 594.000000, reward total was -18.000000. running mean: -19.480305\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -19.485502\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -19.490647\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -19.485740\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -19.500883\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -19.515874\n",
            "resetting env. episode 600.000000, reward total was -12.000000. running mean: -19.440715\n",
            "resetting env. episode 601.000000, reward total was -19.000000. running mean: -19.436308\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -19.451945\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -19.457426\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -19.472852\n",
            "resetting env. episode 605.000000, reward total was -19.000000. running mean: -19.468123\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -19.483442\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -19.488607\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -19.503721\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -19.498684\n",
            "resetting env. episode 610.000000, reward total was -15.000000. running mean: -19.453697\n",
            "resetting env. episode 611.000000, reward total was -19.000000. running mean: -19.449160\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -19.464669\n",
            "resetting env. episode 613.000000, reward total was -18.000000. running mean: -19.450022\n",
            "resetting env. episode 614.000000, reward total was -18.000000. running mean: -19.435522\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -19.451167\n",
            "resetting env. episode 616.000000, reward total was -17.000000. running mean: -19.426655\n",
            "resetting env. episode 617.000000, reward total was -18.000000. running mean: -19.412388\n",
            "resetting env. episode 618.000000, reward total was -19.000000. running mean: -19.408264\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -19.414182\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -19.430040\n",
            "resetting env. episode 621.000000, reward total was -17.000000. running mean: -19.405740\n",
            "resetting env. episode 622.000000, reward total was -19.000000. running mean: -19.401682\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -19.417665\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -19.433489\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -19.439154\n",
            "resetting env. episode 626.000000, reward total was -16.000000. running mean: -19.404762\n",
            "resetting env. episode 627.000000, reward total was -17.000000. running mean: -19.380715\n",
            "resetting env. episode 628.000000, reward total was -18.000000. running mean: -19.366908\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -19.383238\n",
            "resetting env. episode 630.000000, reward total was -16.000000. running mean: -19.349406\n",
            "resetting env. episode 631.000000, reward total was -18.000000. running mean: -19.335912\n",
            "resetting env. episode 632.000000, reward total was -18.000000. running mean: -19.322553\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -19.329327\n",
            "resetting env. episode 634.000000, reward total was -19.000000. running mean: -19.326034\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -19.342774\n",
            "resetting env. episode 636.000000, reward total was -17.000000. running mean: -19.319346\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -19.336153\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -19.352791\n",
            "resetting env. episode 639.000000, reward total was -19.000000. running mean: -19.349263\n",
            "resetting env. episode 640.000000, reward total was -15.000000. running mean: -19.305770\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -19.322713\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -19.339486\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -19.356091\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -19.362530\n",
            "resetting env. episode 645.000000, reward total was -15.000000. running mean: -19.318905\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -19.325716\n",
            "resetting env. episode 647.000000, reward total was -20.000000. running mean: -19.332458\n",
            "resetting env. episode 648.000000, reward total was -19.000000. running mean: -19.329134\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -19.325842\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -19.342584\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -19.349158\n",
            "resetting env. episode 652.000000, reward total was -19.000000. running mean: -19.345667\n",
            "resetting env. episode 653.000000, reward total was -15.000000. running mean: -19.302210\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -19.319188\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -19.325996\n",
            "resetting env. episode 656.000000, reward total was -17.000000. running mean: -19.302736\n",
            "resetting env. episode 657.000000, reward total was -16.000000. running mean: -19.269709\n",
            "resetting env. episode 658.000000, reward total was -15.000000. running mean: -19.227012\n",
            "resetting env. episode 659.000000, reward total was -16.000000. running mean: -19.194741\n",
            "resetting env. episode 660.000000, reward total was -19.000000. running mean: -19.192794\n",
            "resetting env. episode 661.000000, reward total was -18.000000. running mean: -19.180866\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -19.199057\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -19.197067\n",
            "resetting env. episode 664.000000, reward total was -19.000000. running mean: -19.195096\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -19.203145\n",
            "resetting env. episode 666.000000, reward total was -17.000000. running mean: -19.181114\n",
            "resetting env. episode 667.000000, reward total was -17.000000. running mean: -19.159303\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -19.177710\n",
            "resetting env. episode 669.000000, reward total was -17.000000. running mean: -19.155933\n",
            "resetting env. episode 670.000000, reward total was -18.000000. running mean: -19.144373\n",
            "resetting env. episode 671.000000, reward total was -16.000000. running mean: -19.112929\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -19.131800\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -19.140482\n",
            "resetting env. episode 674.000000, reward total was -17.000000. running mean: -19.119077\n",
            "resetting env. episode 675.000000, reward total was -19.000000. running mean: -19.117887\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -19.116708\n",
            "resetting env. episode 677.000000, reward total was -19.000000. running mean: -19.115541\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -19.114385\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -19.123241\n",
            "resetting env. episode 680.000000, reward total was -19.000000. running mean: -19.122009\n",
            "resetting env. episode 681.000000, reward total was -17.000000. running mean: -19.100789\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -19.119781\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -19.138583\n",
            "resetting env. episode 684.000000, reward total was -18.000000. running mean: -19.127197\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -19.145925\n",
            "resetting env. episode 686.000000, reward total was -19.000000. running mean: -19.144466\n",
            "resetting env. episode 687.000000, reward total was -15.000000. running mean: -19.103021\n",
            "resetting env. episode 688.000000, reward total was -15.000000. running mean: -19.061991\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -19.081371\n",
            "resetting env. episode 690.000000, reward total was -19.000000. running mean: -19.080558\n",
            "resetting env. episode 691.000000, reward total was -17.000000. running mean: -19.059752\n",
            "resetting env. episode 692.000000, reward total was -19.000000. running mean: -19.059155\n",
            "resetting env. episode 693.000000, reward total was -18.000000. running mean: -19.048563\n",
            "resetting env. episode 694.000000, reward total was -19.000000. running mean: -19.048077\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -19.047597\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -19.067121\n",
            "resetting env. episode 697.000000, reward total was -18.000000. running mean: -19.056449\n",
            "resetting env. episode 698.000000, reward total was -17.000000. running mean: -19.035885\n",
            "resetting env. episode 699.000000, reward total was -19.000000. running mean: -19.035526\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -19.055171\n",
            "resetting env. episode 701.000000, reward total was -15.000000. running mean: -19.014619\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -19.024473\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -19.034228\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -19.053886\n",
            "resetting env. episode 705.000000, reward total was -19.000000. running mean: -19.053347\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -19.062814\n",
            "resetting env. episode 707.000000, reward total was -19.000000. running mean: -19.062185\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -19.071564\n",
            "resetting env. episode 709.000000, reward total was -20.000000. running mean: -19.080848\n",
            "resetting env. episode 710.000000, reward total was -19.000000. running mean: -19.080039\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -19.079239\n",
            "resetting env. episode 712.000000, reward total was -17.000000. running mean: -19.058447\n",
            "resetting env. episode 713.000000, reward total was -19.000000. running mean: -19.057862\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -19.077284\n",
            "resetting env. episode 715.000000, reward total was -18.000000. running mean: -19.066511\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -19.075846\n",
            "resetting env. episode 717.000000, reward total was -18.000000. running mean: -19.065087\n",
            "resetting env. episode 718.000000, reward total was -17.000000. running mean: -19.044436\n",
            "resetting env. episode 719.000000, reward total was -17.000000. running mean: -19.023992\n",
            "resetting env. episode 720.000000, reward total was -17.000000. running mean: -19.003752\n",
            "resetting env. episode 721.000000, reward total was -17.000000. running mean: -18.983714\n",
            "resetting env. episode 722.000000, reward total was -19.000000. running mean: -18.983877\n",
            "resetting env. episode 723.000000, reward total was -17.000000. running mean: -18.964039\n",
            "resetting env. episode 724.000000, reward total was -17.000000. running mean: -18.944398\n",
            "resetting env. episode 725.000000, reward total was -16.000000. running mean: -18.914954\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -18.925805\n",
            "resetting env. episode 727.000000, reward total was -15.000000. running mean: -18.886547\n",
            "resetting env. episode 728.000000, reward total was -18.000000. running mean: -18.877681\n",
            "resetting env. episode 729.000000, reward total was -12.000000. running mean: -18.808904\n",
            "resetting env. episode 730.000000, reward total was -15.000000. running mean: -18.770815\n",
            "resetting env. episode 731.000000, reward total was -17.000000. running mean: -18.753107\n",
            "resetting env. episode 732.000000, reward total was -19.000000. running mean: -18.755576\n",
            "resetting env. episode 733.000000, reward total was -20.000000. running mean: -18.768020\n",
            "resetting env. episode 734.000000, reward total was -19.000000. running mean: -18.770340\n",
            "resetting env. episode 735.000000, reward total was -18.000000. running mean: -18.762637\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -18.775010\n",
            "resetting env. episode 737.000000, reward total was -17.000000. running mean: -18.757260\n",
            "resetting env. episode 738.000000, reward total was -17.000000. running mean: -18.739688\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -18.752291\n",
            "resetting env. episode 740.000000, reward total was -18.000000. running mean: -18.744768\n",
            "resetting env. episode 741.000000, reward total was -18.000000. running mean: -18.737320\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -18.749947\n",
            "resetting env. episode 743.000000, reward total was -16.000000. running mean: -18.722448\n",
            "resetting env. episode 744.000000, reward total was -19.000000. running mean: -18.725223\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -18.747971\n",
            "resetting env. episode 746.000000, reward total was -19.000000. running mean: -18.750491\n",
            "resetting env. episode 747.000000, reward total was -18.000000. running mean: -18.742986\n",
            "resetting env. episode 748.000000, reward total was -19.000000. running mean: -18.745556\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -18.758101\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -18.770520\n",
            "resetting env. episode 751.000000, reward total was -17.000000. running mean: -18.752815\n",
            "resetting env. episode 752.000000, reward total was -19.000000. running mean: -18.755286\n",
            "resetting env. episode 753.000000, reward total was -15.000000. running mean: -18.717734\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -18.740556\n",
            "resetting env. episode 755.000000, reward total was -17.000000. running mean: -18.723151\n",
            "resetting env. episode 756.000000, reward total was -18.000000. running mean: -18.715919\n",
            "resetting env. episode 757.000000, reward total was -19.000000. running mean: -18.718760\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -18.741572\n",
            "resetting env. episode 759.000000, reward total was -13.000000. running mean: -18.684157\n",
            "resetting env. episode 760.000000, reward total was -15.000000. running mean: -18.647315\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -18.670842\n",
            "resetting env. episode 762.000000, reward total was -17.000000. running mean: -18.654133\n",
            "resetting env. episode 763.000000, reward total was -20.000000. running mean: -18.667592\n",
            "resetting env. episode 764.000000, reward total was -17.000000. running mean: -18.650916\n",
            "resetting env. episode 765.000000, reward total was -14.000000. running mean: -18.604407\n",
            "resetting env. episode 766.000000, reward total was -16.000000. running mean: -18.578363\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -18.582579\n",
            "resetting env. episode 768.000000, reward total was -19.000000. running mean: -18.586754\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -18.590886\n",
            "resetting env. episode 770.000000, reward total was -17.000000. running mean: -18.574977\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -18.579227\n",
            "resetting env. episode 772.000000, reward total was -17.000000. running mean: -18.563435\n",
            "resetting env. episode 773.000000, reward total was -15.000000. running mean: -18.527801\n",
            "resetting env. episode 774.000000, reward total was -16.000000. running mean: -18.502523\n",
            "resetting env. episode 775.000000, reward total was -19.000000. running mean: -18.507498\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -18.512423\n",
            "resetting env. episode 777.000000, reward total was -19.000000. running mean: -18.517298\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -18.522125\n",
            "resetting env. episode 779.000000, reward total was -17.000000. running mean: -18.506904\n",
            "resetting env. episode 780.000000, reward total was -19.000000. running mean: -18.511835\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -18.536717\n",
            "resetting env. episode 782.000000, reward total was -19.000000. running mean: -18.541350\n",
            "resetting env. episode 783.000000, reward total was -18.000000. running mean: -18.535936\n",
            "resetting env. episode 784.000000, reward total was -19.000000. running mean: -18.540577\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -18.545171\n",
            "resetting env. episode 786.000000, reward total was -17.000000. running mean: -18.529719\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -18.554422\n",
            "resetting env. episode 788.000000, reward total was -14.000000. running mean: -18.508878\n",
            "resetting env. episode 789.000000, reward total was -15.000000. running mean: -18.473789\n",
            "resetting env. episode 790.000000, reward total was -19.000000. running mean: -18.479051\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -18.484261\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -18.499418\n",
            "resetting env. episode 793.000000, reward total was -19.000000. running mean: -18.504424\n",
            "resetting env. episode 794.000000, reward total was -18.000000. running mean: -18.499380\n",
            "resetting env. episode 795.000000, reward total was -19.000000. running mean: -18.504386\n",
            "resetting env. episode 796.000000, reward total was -16.000000. running mean: -18.479342\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -18.504549\n",
            "resetting env. episode 798.000000, reward total was -15.000000. running mean: -18.469503\n",
            "resetting env. episode 799.000000, reward total was -17.000000. running mean: -18.454808\n",
            "resetting env. episode 800.000000, reward total was -19.000000. running mean: -18.460260\n",
            "resetting env. episode 801.000000, reward total was -19.000000. running mean: -18.465657\n",
            "resetting env. episode 802.000000, reward total was -17.000000. running mean: -18.451001\n",
            "resetting env. episode 803.000000, reward total was -16.000000. running mean: -18.426491\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -18.452226\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -18.457704\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -18.473127\n",
            "resetting env. episode 807.000000, reward total was -15.000000. running mean: -18.438395\n",
            "resetting env. episode 808.000000, reward total was -13.000000. running mean: -18.384011\n",
            "resetting env. episode 809.000000, reward total was -19.000000. running mean: -18.390171\n",
            "resetting env. episode 810.000000, reward total was -17.000000. running mean: -18.376270\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -18.382507\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -18.398682\n",
            "resetting env. episode 813.000000, reward total was -19.000000. running mean: -18.404695\n",
            "resetting env. episode 814.000000, reward total was -14.000000. running mean: -18.360648\n",
            "resetting env. episode 815.000000, reward total was -17.000000. running mean: -18.347041\n",
            "resetting env. episode 816.000000, reward total was -19.000000. running mean: -18.353571\n",
            "resetting env. episode 817.000000, reward total was -17.000000. running mean: -18.340035\n",
            "resetting env. episode 818.000000, reward total was -19.000000. running mean: -18.346635\n",
            "resetting env. episode 819.000000, reward total was -17.000000. running mean: -18.333169\n",
            "resetting env. episode 820.000000, reward total was -13.000000. running mean: -18.279837\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -18.287039\n",
            "resetting env. episode 822.000000, reward total was -18.000000. running mean: -18.284168\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -18.311327\n",
            "resetting env. episode 824.000000, reward total was -15.000000. running mean: -18.278213\n",
            "resetting env. episode 825.000000, reward total was -19.000000. running mean: -18.285431\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -18.302577\n",
            "resetting env. episode 827.000000, reward total was -16.000000. running mean: -18.279551\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -18.306756\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -18.323688\n",
            "resetting env. episode 830.000000, reward total was -17.000000. running mean: -18.310451\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -18.317347\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -18.334173\n",
            "resetting env. episode 833.000000, reward total was -18.000000. running mean: -18.330831\n",
            "resetting env. episode 834.000000, reward total was -16.000000. running mean: -18.307523\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -18.334448\n",
            "resetting env. episode 836.000000, reward total was -15.000000. running mean: -18.301103\n",
            "resetting env. episode 837.000000, reward total was -18.000000. running mean: -18.298092\n",
            "resetting env. episode 838.000000, reward total was -17.000000. running mean: -18.285111\n",
            "resetting env. episode 839.000000, reward total was -17.000000. running mean: -18.272260\n",
            "resetting env. episode 840.000000, reward total was -18.000000. running mean: -18.269538\n",
            "resetting env. episode 841.000000, reward total was -15.000000. running mean: -18.236842\n",
            "resetting env. episode 842.000000, reward total was -17.000000. running mean: -18.224474\n",
            "resetting env. episode 843.000000, reward total was -17.000000. running mean: -18.212229\n",
            "resetting env. episode 844.000000, reward total was -19.000000. running mean: -18.220107\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -18.247906\n",
            "resetting env. episode 846.000000, reward total was -17.000000. running mean: -18.235427\n",
            "resetting env. episode 847.000000, reward total was -18.000000. running mean: -18.233072\n",
            "resetting env. episode 848.000000, reward total was -16.000000. running mean: -18.210742\n",
            "resetting env. episode 849.000000, reward total was -16.000000. running mean: -18.188634\n",
            "resetting env. episode 850.000000, reward total was -17.000000. running mean: -18.176748\n",
            "resetting env. episode 851.000000, reward total was -17.000000. running mean: -18.164981\n",
            "resetting env. episode 852.000000, reward total was -13.000000. running mean: -18.113331\n",
            "resetting env. episode 853.000000, reward total was -19.000000. running mean: -18.122197\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -18.140975\n",
            "resetting env. episode 855.000000, reward total was -17.000000. running mean: -18.129566\n",
            "resetting env. episode 856.000000, reward total was -15.000000. running mean: -18.098270\n",
            "resetting env. episode 857.000000, reward total was -18.000000. running mean: -18.097287\n",
            "resetting env. episode 858.000000, reward total was -17.000000. running mean: -18.086314\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -18.105451\n",
            "resetting env. episode 860.000000, reward total was -17.000000. running mean: -18.094397\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -18.113453\n",
            "resetting env. episode 862.000000, reward total was -19.000000. running mean: -18.122318\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -18.141095\n",
            "resetting env. episode 864.000000, reward total was -17.000000. running mean: -18.129684\n",
            "resetting env. episode 865.000000, reward total was -19.000000. running mean: -18.138387\n",
            "resetting env. episode 866.000000, reward total was -19.000000. running mean: -18.147003\n",
            "resetting env. episode 867.000000, reward total was -19.000000. running mean: -18.155533\n",
            "resetting env. episode 868.000000, reward total was -19.000000. running mean: -18.163978\n",
            "resetting env. episode 869.000000, reward total was -17.000000. running mean: -18.152338\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -18.180815\n",
            "resetting env. episode 871.000000, reward total was -18.000000. running mean: -18.179007\n",
            "resetting env. episode 872.000000, reward total was -15.000000. running mean: -18.147217\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -18.175745\n",
            "resetting env. episode 874.000000, reward total was -19.000000. running mean: -18.183987\n",
            "resetting env. episode 875.000000, reward total was -17.000000. running mean: -18.172147\n",
            "resetting env. episode 876.000000, reward total was -17.000000. running mean: -18.160426\n",
            "resetting env. episode 877.000000, reward total was -19.000000. running mean: -18.168821\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -18.177133\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -18.185362\n",
            "resetting env. episode 880.000000, reward total was -18.000000. running mean: -18.183508\n",
            "resetting env. episode 881.000000, reward total was -12.000000. running mean: -18.121673\n",
            "resetting env. episode 882.000000, reward total was -19.000000. running mean: -18.130457\n",
            "resetting env. episode 883.000000, reward total was -18.000000. running mean: -18.129152\n",
            "resetting env. episode 884.000000, reward total was -15.000000. running mean: -18.097860\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -18.126882\n",
            "resetting env. episode 886.000000, reward total was -13.000000. running mean: -18.075613\n",
            "resetting env. episode 887.000000, reward total was -18.000000. running mean: -18.074857\n",
            "resetting env. episode 888.000000, reward total was -19.000000. running mean: -18.084108\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -18.113267\n",
            "resetting env. episode 890.000000, reward total was -19.000000. running mean: -18.122135\n",
            "resetting env. episode 891.000000, reward total was -19.000000. running mean: -18.130913\n",
            "resetting env. episode 892.000000, reward total was -19.000000. running mean: -18.139604\n",
            "resetting env. episode 893.000000, reward total was -17.000000. running mean: -18.128208\n",
            "resetting env. episode 894.000000, reward total was -17.000000. running mean: -18.116926\n",
            "resetting env. episode 895.000000, reward total was -17.000000. running mean: -18.105757\n",
            "resetting env. episode 896.000000, reward total was -19.000000. running mean: -18.114699\n",
            "resetting env. episode 897.000000, reward total was -14.000000. running mean: -18.073552\n",
            "resetting env. episode 898.000000, reward total was -17.000000. running mean: -18.062817\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -18.092188\n",
            "resetting env. episode 900.000000, reward total was -17.000000. running mean: -18.081267\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -18.110454\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -18.119349\n",
            "resetting env. episode 903.000000, reward total was -13.000000. running mean: -18.068156\n",
            "resetting env. episode 904.000000, reward total was -15.000000. running mean: -18.037474\n",
            "resetting env. episode 905.000000, reward total was -15.000000. running mean: -18.007100\n",
            "resetting env. episode 906.000000, reward total was -17.000000. running mean: -17.997029\n",
            "resetting env. episode 907.000000, reward total was -18.000000. running mean: -17.997058\n",
            "resetting env. episode 908.000000, reward total was -18.000000. running mean: -17.997088\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -18.017117\n",
            "resetting env. episode 910.000000, reward total was -18.000000. running mean: -18.016946\n",
            "resetting env. episode 911.000000, reward total was -17.000000. running mean: -18.006776\n",
            "resetting env. episode 912.000000, reward total was -18.000000. running mean: -18.006708\n",
            "resetting env. episode 913.000000, reward total was -13.000000. running mean: -17.956641\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -17.977075\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -18.007304\n",
            "resetting env. episode 916.000000, reward total was -11.000000. running mean: -17.937231\n",
            "resetting env. episode 917.000000, reward total was -13.000000. running mean: -17.887859\n",
            "resetting env. episode 918.000000, reward total was -17.000000. running mean: -17.878980\n",
            "resetting env. episode 919.000000, reward total was -15.000000. running mean: -17.850190\n",
            "resetting env. episode 920.000000, reward total was -19.000000. running mean: -17.861689\n",
            "resetting env. episode 921.000000, reward total was -17.000000. running mean: -17.853072\n",
            "resetting env. episode 922.000000, reward total was -15.000000. running mean: -17.824541\n",
            "resetting env. episode 923.000000, reward total was -13.000000. running mean: -17.776296\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -17.808533\n",
            "resetting env. episode 925.000000, reward total was -17.000000. running mean: -17.800447\n",
            "resetting env. episode 926.000000, reward total was -16.000000. running mean: -17.782443\n",
            "resetting env. episode 927.000000, reward total was -18.000000. running mean: -17.784618\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -17.796772\n",
            "resetting env. episode 929.000000, reward total was -18.000000. running mean: -17.798804\n",
            "resetting env. episode 930.000000, reward total was -17.000000. running mean: -17.790816\n",
            "resetting env. episode 931.000000, reward total was -17.000000. running mean: -17.782908\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -17.795079\n",
            "resetting env. episode 933.000000, reward total was -14.000000. running mean: -17.757128\n",
            "resetting env. episode 934.000000, reward total was -17.000000. running mean: -17.749557\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -17.782061\n",
            "resetting env. episode 936.000000, reward total was -16.000000. running mean: -17.764241\n",
            "resetting env. episode 937.000000, reward total was -17.000000. running mean: -17.756598\n",
            "resetting env. episode 938.000000, reward total was -19.000000. running mean: -17.769032\n",
            "resetting env. episode 939.000000, reward total was -15.000000. running mean: -17.741342\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -17.753929\n",
            "resetting env. episode 941.000000, reward total was -19.000000. running mean: -17.766389\n",
            "resetting env. episode 942.000000, reward total was -18.000000. running mean: -17.768726\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -17.781038\n",
            "resetting env. episode 944.000000, reward total was -14.000000. running mean: -17.743228\n",
            "resetting env. episode 945.000000, reward total was -17.000000. running mean: -17.735796\n",
            "resetting env. episode 946.000000, reward total was -19.000000. running mean: -17.748438\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -17.770953\n",
            "resetting env. episode 948.000000, reward total was -17.000000. running mean: -17.763244\n",
            "resetting env. episode 949.000000, reward total was -18.000000. running mean: -17.765611\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -17.797955\n",
            "resetting env. episode 951.000000, reward total was -17.000000. running mean: -17.789976\n",
            "resetting env. episode 952.000000, reward total was -15.000000. running mean: -17.762076\n",
            "resetting env. episode 953.000000, reward total was -16.000000. running mean: -17.744455\n",
            "resetting env. episode 954.000000, reward total was -17.000000. running mean: -17.737011\n",
            "resetting env. episode 955.000000, reward total was -19.000000. running mean: -17.749640\n",
            "resetting env. episode 956.000000, reward total was -18.000000. running mean: -17.752144\n",
            "resetting env. episode 957.000000, reward total was -16.000000. running mean: -17.734623\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -17.757276\n",
            "resetting env. episode 959.000000, reward total was -17.000000. running mean: -17.749704\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -17.782207\n",
            "resetting env. episode 961.000000, reward total was -19.000000. running mean: -17.794385\n",
            "resetting env. episode 962.000000, reward total was -16.000000. running mean: -17.776441\n",
            "resetting env. episode 963.000000, reward total was -17.000000. running mean: -17.768676\n",
            "resetting env. episode 964.000000, reward total was -17.000000. running mean: -17.760990\n",
            "resetting env. episode 965.000000, reward total was -15.000000. running mean: -17.733380\n",
            "resetting env. episode 966.000000, reward total was -18.000000. running mean: -17.736046\n",
            "resetting env. episode 967.000000, reward total was -17.000000. running mean: -17.728685\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -17.751399\n",
            "resetting env. episode 969.000000, reward total was -15.000000. running mean: -17.723885\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -17.746646\n",
            "resetting env. episode 971.000000, reward total was -19.000000. running mean: -17.759179\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -17.791587\n",
            "resetting env. episode 973.000000, reward total was -18.000000. running mean: -17.793672\n",
            "resetting env. episode 974.000000, reward total was -19.000000. running mean: -17.805735\n",
            "resetting env. episode 975.000000, reward total was -19.000000. running mean: -17.817678\n",
            "resetting env. episode 976.000000, reward total was -15.000000. running mean: -17.789501\n",
            "resetting env. episode 977.000000, reward total was -17.000000. running mean: -17.781606\n",
            "resetting env. episode 978.000000, reward total was -15.000000. running mean: -17.753790\n",
            "resetting env. episode 979.000000, reward total was -17.000000. running mean: -17.746252\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -17.768789\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -17.791101\n",
            "resetting env. episode 982.000000, reward total was -17.000000. running mean: -17.783190\n",
            "resetting env. episode 983.000000, reward total was -14.000000. running mean: -17.745358\n",
            "resetting env. episode 984.000000, reward total was -16.000000. running mean: -17.727905\n",
            "resetting env. episode 985.000000, reward total was -18.000000. running mean: -17.730626\n",
            "resetting env. episode 986.000000, reward total was -16.000000. running mean: -17.713320\n",
            "resetting env. episode 987.000000, reward total was -13.000000. running mean: -17.666186\n",
            "resetting env. episode 988.000000, reward total was -16.000000. running mean: -17.649525\n",
            "resetting env. episode 989.000000, reward total was -19.000000. running mean: -17.663029\n",
            "resetting env. episode 990.000000, reward total was -18.000000. running mean: -17.666399\n",
            "resetting env. episode 991.000000, reward total was -17.000000. running mean: -17.659735\n",
            "resetting env. episode 992.000000, reward total was -16.000000. running mean: -17.643138\n",
            "resetting env. episode 993.000000, reward total was -17.000000. running mean: -17.636706\n",
            "resetting env. episode 994.000000, reward total was -16.000000. running mean: -17.620339\n",
            "resetting env. episode 995.000000, reward total was -15.000000. running mean: -17.594136\n",
            "resetting env. episode 996.000000, reward total was -19.000000. running mean: -17.608194\n",
            "resetting env. episode 997.000000, reward total was -14.000000. running mean: -17.572112\n",
            "resetting env. episode 998.000000, reward total was -14.000000. running mean: -17.536391\n",
            "resetting env. episode 999.000000, reward total was -14.000000. running mean: -17.501027\n",
            "resetting env. episode 1000.000000, reward total was -17.000000. running mean: -17.496017\n",
            "resetting env. episode 1001.000000, reward total was -17.000000. running mean: -17.491057\n",
            "resetting env. episode 1002.000000, reward total was -15.000000. running mean: -17.466146\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -17.491485\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -17.526570\n",
            "resetting env. episode 1005.000000, reward total was -17.000000. running mean: -17.521304\n",
            "resetting env. episode 1006.000000, reward total was -19.000000. running mean: -17.536091\n",
            "resetting env. episode 1007.000000, reward total was -15.000000. running mean: -17.510730\n",
            "resetting env. episode 1008.000000, reward total was -15.000000. running mean: -17.485623\n",
            "resetting env. episode 1009.000000, reward total was -19.000000. running mean: -17.500767\n",
            "resetting env. episode 1010.000000, reward total was -15.000000. running mean: -17.475759\n",
            "resetting env. episode 1011.000000, reward total was -18.000000. running mean: -17.481002\n",
            "resetting env. episode 1012.000000, reward total was -17.000000. running mean: -17.476192\n",
            "resetting env. episode 1013.000000, reward total was -16.000000. running mean: -17.461430\n",
            "resetting env. episode 1014.000000, reward total was -17.000000. running mean: -17.456815\n",
            "resetting env. episode 1015.000000, reward total was -19.000000. running mean: -17.472247\n",
            "resetting env. episode 1016.000000, reward total was -17.000000. running mean: -17.467525\n",
            "resetting env. episode 1017.000000, reward total was -17.000000. running mean: -17.462850\n",
            "resetting env. episode 1018.000000, reward total was -17.000000. running mean: -17.458221\n",
            "resetting env. episode 1019.000000, reward total was -19.000000. running mean: -17.473639\n",
            "resetting env. episode 1020.000000, reward total was -19.000000. running mean: -17.488902\n",
            "resetting env. episode 1021.000000, reward total was -17.000000. running mean: -17.484013\n",
            "resetting env. episode 1022.000000, reward total was -13.000000. running mean: -17.439173\n",
            "resetting env. episode 1023.000000, reward total was -15.000000. running mean: -17.414782\n",
            "resetting env. episode 1024.000000, reward total was -15.000000. running mean: -17.390634\n",
            "resetting env. episode 1025.000000, reward total was -15.000000. running mean: -17.366727\n",
            "resetting env. episode 1026.000000, reward total was -16.000000. running mean: -17.353060\n",
            "resetting env. episode 1027.000000, reward total was -13.000000. running mean: -17.309530\n",
            "resetting env. episode 1028.000000, reward total was -16.000000. running mean: -17.296434\n",
            "resetting env. episode 1029.000000, reward total was -18.000000. running mean: -17.303470\n",
            "resetting env. episode 1030.000000, reward total was -17.000000. running mean: -17.300435\n",
            "resetting env. episode 1031.000000, reward total was -16.000000. running mean: -17.287431\n",
            "resetting env. episode 1032.000000, reward total was -17.000000. running mean: -17.284557\n",
            "resetting env. episode 1033.000000, reward total was -11.000000. running mean: -17.221711\n",
            "resetting env. episode 1034.000000, reward total was -17.000000. running mean: -17.219494\n",
            "resetting env. episode 1035.000000, reward total was -16.000000. running mean: -17.207299\n",
            "resetting env. episode 1036.000000, reward total was -14.000000. running mean: -17.175226\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -17.193474\n",
            "resetting env. episode 1038.000000, reward total was -18.000000. running mean: -17.201539\n",
            "resetting env. episode 1039.000000, reward total was -18.000000. running mean: -17.209524\n",
            "resetting env. episode 1040.000000, reward total was -15.000000. running mean: -17.187428\n",
            "resetting env. episode 1041.000000, reward total was -16.000000. running mean: -17.175554\n",
            "resetting env. episode 1042.000000, reward total was -17.000000. running mean: -17.173798\n",
            "resetting env. episode 1043.000000, reward total was -17.000000. running mean: -17.172061\n",
            "resetting env. episode 1044.000000, reward total was -16.000000. running mean: -17.160340\n",
            "resetting env. episode 1045.000000, reward total was -18.000000. running mean: -17.168737\n",
            "resetting env. episode 1046.000000, reward total was -13.000000. running mean: -17.127049\n",
            "resetting env. episode 1047.000000, reward total was -19.000000. running mean: -17.145779\n",
            "resetting env. episode 1048.000000, reward total was -15.000000. running mean: -17.124321\n",
            "resetting env. episode 1049.000000, reward total was -17.000000. running mean: -17.123078\n",
            "resetting env. episode 1050.000000, reward total was -17.000000. running mean: -17.121847\n",
            "resetting env. episode 1051.000000, reward total was -14.000000. running mean: -17.090628\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -17.119722\n",
            "resetting env. episode 1053.000000, reward total was -17.000000. running mean: -17.118525\n",
            "resetting env. episode 1054.000000, reward total was -13.000000. running mean: -17.077340\n",
            "resetting env. episode 1055.000000, reward total was -16.000000. running mean: -17.066566\n",
            "resetting env. episode 1056.000000, reward total was -15.000000. running mean: -17.045901\n",
            "resetting env. episode 1057.000000, reward total was -18.000000. running mean: -17.055442\n",
            "resetting env. episode 1058.000000, reward total was -15.000000. running mean: -17.034887\n",
            "resetting env. episode 1059.000000, reward total was -15.000000. running mean: -17.014538\n",
            "resetting env. episode 1060.000000, reward total was -16.000000. running mean: -17.004393\n",
            "resetting env. episode 1061.000000, reward total was -16.000000. running mean: -16.994349\n",
            "resetting env. episode 1062.000000, reward total was -18.000000. running mean: -17.004406\n",
            "resetting env. episode 1063.000000, reward total was -19.000000. running mean: -17.024361\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -17.044118\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -17.073677\n",
            "resetting env. episode 1066.000000, reward total was -13.000000. running mean: -17.032940\n",
            "resetting env. episode 1067.000000, reward total was -17.000000. running mean: -17.032610\n",
            "resetting env. episode 1068.000000, reward total was -17.000000. running mean: -17.032284\n",
            "resetting env. episode 1069.000000, reward total was -16.000000. running mean: -17.021962\n",
            "resetting env. episode 1070.000000, reward total was -19.000000. running mean: -17.041742\n",
            "resetting env. episode 1071.000000, reward total was -17.000000. running mean: -17.041325\n",
            "resetting env. episode 1072.000000, reward total was -17.000000. running mean: -17.040911\n",
            "resetting env. episode 1073.000000, reward total was -19.000000. running mean: -17.060502\n",
            "resetting env. episode 1074.000000, reward total was -17.000000. running mean: -17.059897\n",
            "resetting env. episode 1075.000000, reward total was -17.000000. running mean: -17.059298\n",
            "resetting env. episode 1076.000000, reward total was -16.000000. running mean: -17.048705\n",
            "resetting env. episode 1077.000000, reward total was -16.000000. running mean: -17.038218\n",
            "resetting env. episode 1078.000000, reward total was -17.000000. running mean: -17.037836\n",
            "resetting env. episode 1079.000000, reward total was -16.000000. running mean: -17.027458\n",
            "resetting env. episode 1080.000000, reward total was -17.000000. running mean: -17.027183\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -17.046911\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -17.076442\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -17.105678\n",
            "resetting env. episode 1084.000000, reward total was -18.000000. running mean: -17.114621\n",
            "resetting env. episode 1085.000000, reward total was -15.000000. running mean: -17.093475\n",
            "resetting env. episode 1086.000000, reward total was -20.000000. running mean: -17.122540\n",
            "resetting env. episode 1087.000000, reward total was -13.000000. running mean: -17.081315\n",
            "resetting env. episode 1088.000000, reward total was -18.000000. running mean: -17.090501\n",
            "resetting env. episode 1089.000000, reward total was -17.000000. running mean: -17.089596\n",
            "resetting env. episode 1090.000000, reward total was -19.000000. running mean: -17.108700\n",
            "resetting env. episode 1091.000000, reward total was -18.000000. running mean: -17.117613\n",
            "resetting env. episode 1092.000000, reward total was -17.000000. running mean: -17.116437\n",
            "resetting env. episode 1093.000000, reward total was -9.000000. running mean: -17.035273\n",
            "resetting env. episode 1094.000000, reward total was -17.000000. running mean: -17.034920\n",
            "resetting env. episode 1095.000000, reward total was -13.000000. running mean: -16.994571\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -17.034625\n",
            "resetting env. episode 1097.000000, reward total was -18.000000. running mean: -17.044279\n",
            "resetting env. episode 1098.000000, reward total was -18.000000. running mean: -17.053836\n",
            "resetting env. episode 1099.000000, reward total was -14.000000. running mean: -17.023298\n",
            "resetting env. episode 1100.000000, reward total was -18.000000. running mean: -17.033065\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -17.052734\n",
            "resetting env. episode 1102.000000, reward total was -19.000000. running mean: -17.072207\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -17.091485\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -17.120570\n",
            "resetting env. episode 1105.000000, reward total was -19.000000. running mean: -17.139364\n",
            "resetting env. episode 1106.000000, reward total was -19.000000. running mean: -17.157971\n",
            "resetting env. episode 1107.000000, reward total was -19.000000. running mean: -17.176391\n",
            "resetting env. episode 1108.000000, reward total was -18.000000. running mean: -17.184627\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -17.202781\n",
            "resetting env. episode 1110.000000, reward total was -19.000000. running mean: -17.220753\n",
            "resetting env. episode 1111.000000, reward total was -18.000000. running mean: -17.228545\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -17.256260\n",
            "resetting env. episode 1113.000000, reward total was -15.000000. running mean: -17.233697\n",
            "resetting env. episode 1114.000000, reward total was -14.000000. running mean: -17.201360\n",
            "resetting env. episode 1115.000000, reward total was -18.000000. running mean: -17.209347\n",
            "resetting env. episode 1116.000000, reward total was -17.000000. running mean: -17.207253\n",
            "resetting env. episode 1117.000000, reward total was -18.000000. running mean: -17.215181\n",
            "resetting env. episode 1118.000000, reward total was -15.000000. running mean: -17.193029\n",
            "resetting env. episode 1119.000000, reward total was -16.000000. running mean: -17.181099\n",
            "resetting env. episode 1120.000000, reward total was -13.000000. running mean: -17.139288\n",
            "resetting env. episode 1121.000000, reward total was -17.000000. running mean: -17.137895\n",
            "resetting env. episode 1122.000000, reward total was -16.000000. running mean: -17.126516\n",
            "resetting env. episode 1123.000000, reward total was -16.000000. running mean: -17.115251\n",
            "resetting env. episode 1124.000000, reward total was -16.000000. running mean: -17.104098\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -17.143057\n",
            "resetting env. episode 1126.000000, reward total was -13.000000. running mean: -17.101627\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -17.140610\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -17.159204\n",
            "resetting env. episode 1129.000000, reward total was -17.000000. running mean: -17.157612\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -17.186036\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -17.204176\n",
            "resetting env. episode 1132.000000, reward total was -18.000000. running mean: -17.212134\n",
            "resetting env. episode 1133.000000, reward total was -14.000000. running mean: -17.180013\n",
            "resetting env. episode 1134.000000, reward total was -14.000000. running mean: -17.148213\n",
            "resetting env. episode 1135.000000, reward total was -12.000000. running mean: -17.096730\n",
            "resetting env. episode 1136.000000, reward total was -16.000000. running mean: -17.085763\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -17.124905\n",
            "resetting env. episode 1138.000000, reward total was -19.000000. running mean: -17.143656\n",
            "resetting env. episode 1139.000000, reward total was -18.000000. running mean: -17.152220\n",
            "resetting env. episode 1140.000000, reward total was -17.000000. running mean: -17.150698\n",
            "resetting env. episode 1141.000000, reward total was -17.000000. running mean: -17.149191\n",
            "resetting env. episode 1142.000000, reward total was -15.000000. running mean: -17.127699\n",
            "resetting env. episode 1143.000000, reward total was -16.000000. running mean: -17.116422\n",
            "resetting env. episode 1144.000000, reward total was -12.000000. running mean: -17.065258\n",
            "resetting env. episode 1145.000000, reward total was -17.000000. running mean: -17.064605\n",
            "resetting env. episode 1146.000000, reward total was -16.000000. running mean: -17.053959\n",
            "resetting env. episode 1147.000000, reward total was -18.000000. running mean: -17.063419\n",
            "resetting env. episode 1148.000000, reward total was -15.000000. running mean: -17.042785\n",
            "resetting env. episode 1149.000000, reward total was -16.000000. running mean: -17.032357\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -17.062034\n",
            "resetting env. episode 1151.000000, reward total was -13.000000. running mean: -17.021413\n",
            "resetting env. episode 1152.000000, reward total was -17.000000. running mean: -17.021199\n",
            "resetting env. episode 1153.000000, reward total was -15.000000. running mean: -17.000987\n",
            "resetting env. episode 1154.000000, reward total was -18.000000. running mean: -17.010977\n",
            "resetting env. episode 1155.000000, reward total was -16.000000. running mean: -17.000868\n",
            "resetting env. episode 1156.000000, reward total was -17.000000. running mean: -17.000859\n",
            "resetting env. episode 1157.000000, reward total was -12.000000. running mean: -16.950850\n",
            "resetting env. episode 1158.000000, reward total was -17.000000. running mean: -16.951342\n",
            "resetting env. episode 1159.000000, reward total was -12.000000. running mean: -16.901828\n",
            "resetting env. episode 1160.000000, reward total was -8.000000. running mean: -16.812810\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -16.844682\n",
            "resetting env. episode 1162.000000, reward total was -17.000000. running mean: -16.846235\n",
            "resetting env. episode 1163.000000, reward total was -16.000000. running mean: -16.837773\n",
            "resetting env. episode 1164.000000, reward total was -11.000000. running mean: -16.779395\n",
            "resetting env. episode 1165.000000, reward total was -17.000000. running mean: -16.781601\n",
            "resetting env. episode 1166.000000, reward total was -19.000000. running mean: -16.803785\n",
            "resetting env. episode 1167.000000, reward total was -16.000000. running mean: -16.795747\n",
            "resetting env. episode 1168.000000, reward total was -7.000000. running mean: -16.697790\n",
            "resetting env. episode 1169.000000, reward total was -16.000000. running mean: -16.690812\n",
            "resetting env. episode 1170.000000, reward total was -17.000000. running mean: -16.693904\n",
            "resetting env. episode 1171.000000, reward total was -17.000000. running mean: -16.696965\n",
            "resetting env. episode 1172.000000, reward total was -16.000000. running mean: -16.689995\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -16.723095\n",
            "resetting env. episode 1174.000000, reward total was -19.000000. running mean: -16.745864\n",
            "resetting env. episode 1175.000000, reward total was -17.000000. running mean: -16.748406\n",
            "resetting env. episode 1176.000000, reward total was -16.000000. running mean: -16.740922\n",
            "resetting env. episode 1177.000000, reward total was -16.000000. running mean: -16.733512\n",
            "resetting env. episode 1178.000000, reward total was -19.000000. running mean: -16.756177\n",
            "resetting env. episode 1179.000000, reward total was -19.000000. running mean: -16.778615\n",
            "resetting env. episode 1180.000000, reward total was -16.000000. running mean: -16.770829\n",
            "resetting env. episode 1181.000000, reward total was -12.000000. running mean: -16.723121\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -16.755890\n",
            "resetting env. episode 1183.000000, reward total was -14.000000. running mean: -16.728331\n",
            "resetting env. episode 1184.000000, reward total was -16.000000. running mean: -16.721048\n",
            "resetting env. episode 1185.000000, reward total was -14.000000. running mean: -16.693837\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -16.726899\n",
            "resetting env. episode 1187.000000, reward total was -18.000000. running mean: -16.739630\n",
            "resetting env. episode 1188.000000, reward total was -17.000000. running mean: -16.742233\n",
            "resetting env. episode 1189.000000, reward total was -19.000000. running mean: -16.764811\n",
            "resetting env. episode 1190.000000, reward total was -16.000000. running mean: -16.757163\n",
            "resetting env. episode 1191.000000, reward total was -13.000000. running mean: -16.719591\n",
            "resetting env. episode 1192.000000, reward total was -16.000000. running mean: -16.712395\n",
            "resetting env. episode 1193.000000, reward total was -17.000000. running mean: -16.715271\n",
            "resetting env. episode 1194.000000, reward total was -17.000000. running mean: -16.718119\n",
            "resetting env. episode 1195.000000, reward total was -19.000000. running mean: -16.740938\n",
            "resetting env. episode 1196.000000, reward total was -12.000000. running mean: -16.693528\n",
            "resetting env. episode 1197.000000, reward total was -9.000000. running mean: -16.616593\n",
            "resetting env. episode 1198.000000, reward total was -14.000000. running mean: -16.590427\n",
            "resetting env. episode 1199.000000, reward total was -17.000000. running mean: -16.594523\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -16.638578\n",
            "resetting env. episode 1201.000000, reward total was -18.000000. running mean: -16.652192\n",
            "resetting env. episode 1202.000000, reward total was -19.000000. running mean: -16.675670\n",
            "resetting env. episode 1203.000000, reward total was -14.000000. running mean: -16.648913\n",
            "resetting env. episode 1204.000000, reward total was -17.000000. running mean: -16.652424\n",
            "resetting env. episode 1205.000000, reward total was -13.000000. running mean: -16.615900\n",
            "resetting env. episode 1206.000000, reward total was -11.000000. running mean: -16.559741\n",
            "resetting env. episode 1207.000000, reward total was -17.000000. running mean: -16.564143\n",
            "resetting env. episode 1208.000000, reward total was -16.000000. running mean: -16.558502\n",
            "resetting env. episode 1209.000000, reward total was -17.000000. running mean: -16.562917\n",
            "resetting env. episode 1210.000000, reward total was -17.000000. running mean: -16.567288\n",
            "resetting env. episode 1211.000000, reward total was -14.000000. running mean: -16.541615\n",
            "resetting env. episode 1212.000000, reward total was -16.000000. running mean: -16.536199\n",
            "resetting env. episode 1213.000000, reward total was -17.000000. running mean: -16.540837\n",
            "resetting env. episode 1214.000000, reward total was -11.000000. running mean: -16.485428\n",
            "resetting env. episode 1215.000000, reward total was -12.000000. running mean: -16.440574\n",
            "resetting env. episode 1216.000000, reward total was -14.000000. running mean: -16.416168\n",
            "resetting env. episode 1217.000000, reward total was -18.000000. running mean: -16.432007\n",
            "resetting env. episode 1218.000000, reward total was -18.000000. running mean: -16.447687\n",
            "resetting env. episode 1219.000000, reward total was -17.000000. running mean: -16.453210\n",
            "resetting env. episode 1220.000000, reward total was -18.000000. running mean: -16.468678\n",
            "resetting env. episode 1221.000000, reward total was -13.000000. running mean: -16.433991\n",
            "resetting env. episode 1222.000000, reward total was -12.000000. running mean: -16.389651\n",
            "resetting env. episode 1223.000000, reward total was -14.000000. running mean: -16.365754\n",
            "resetting env. episode 1224.000000, reward total was -19.000000. running mean: -16.392097\n",
            "resetting env. episode 1225.000000, reward total was -19.000000. running mean: -16.418176\n",
            "resetting env. episode 1226.000000, reward total was -19.000000. running mean: -16.443994\n",
            "resetting env. episode 1227.000000, reward total was -16.000000. running mean: -16.439554\n",
            "resetting env. episode 1228.000000, reward total was -14.000000. running mean: -16.415159\n",
            "resetting env. episode 1229.000000, reward total was -16.000000. running mean: -16.411007\n",
            "resetting env. episode 1230.000000, reward total was -15.000000. running mean: -16.396897\n",
            "resetting env. episode 1231.000000, reward total was -15.000000. running mean: -16.382928\n",
            "resetting env. episode 1232.000000, reward total was -15.000000. running mean: -16.369099\n",
            "resetting env. episode 1233.000000, reward total was -17.000000. running mean: -16.375408\n",
            "resetting env. episode 1234.000000, reward total was -7.000000. running mean: -16.281654\n",
            "resetting env. episode 1235.000000, reward total was -13.000000. running mean: -16.248837\n",
            "resetting env. episode 1236.000000, reward total was -17.000000. running mean: -16.256349\n",
            "resetting env. episode 1237.000000, reward total was -18.000000. running mean: -16.273785\n",
            "resetting env. episode 1238.000000, reward total was -17.000000. running mean: -16.281047\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -16.318237\n",
            "resetting env. episode 1240.000000, reward total was -12.000000. running mean: -16.275055\n",
            "resetting env. episode 1241.000000, reward total was -19.000000. running mean: -16.302304\n",
            "resetting env. episode 1242.000000, reward total was -18.000000. running mean: -16.319281\n",
            "resetting env. episode 1243.000000, reward total was -17.000000. running mean: -16.326088\n",
            "resetting env. episode 1244.000000, reward total was -13.000000. running mean: -16.292827\n",
            "resetting env. episode 1245.000000, reward total was -13.000000. running mean: -16.259899\n",
            "resetting env. episode 1246.000000, reward total was -11.000000. running mean: -16.207300\n",
            "resetting env. episode 1247.000000, reward total was -19.000000. running mean: -16.235227\n",
            "resetting env. episode 1248.000000, reward total was -17.000000. running mean: -16.242875\n",
            "resetting env. episode 1249.000000, reward total was -11.000000. running mean: -16.190446\n",
            "resetting env. episode 1250.000000, reward total was -16.000000. running mean: -16.188542\n",
            "resetting env. episode 1251.000000, reward total was -15.000000. running mean: -16.176656\n",
            "resetting env. episode 1252.000000, reward total was -15.000000. running mean: -16.164890\n",
            "resetting env. episode 1253.000000, reward total was -16.000000. running mean: -16.163241\n",
            "resetting env. episode 1254.000000, reward total was -19.000000. running mean: -16.191608\n",
            "resetting env. episode 1255.000000, reward total was -17.000000. running mean: -16.199692\n",
            "resetting env. episode 1256.000000, reward total was -11.000000. running mean: -16.147695\n",
            "resetting env. episode 1257.000000, reward total was -15.000000. running mean: -16.136218\n",
            "resetting env. episode 1258.000000, reward total was -13.000000. running mean: -16.104856\n",
            "resetting env. episode 1259.000000, reward total was -15.000000. running mean: -16.093808\n",
            "resetting env. episode 1260.000000, reward total was -17.000000. running mean: -16.102870\n",
            "resetting env. episode 1261.000000, reward total was -15.000000. running mean: -16.091841\n",
            "resetting env. episode 1262.000000, reward total was -7.000000. running mean: -16.000922\n",
            "resetting env. episode 1263.000000, reward total was -11.000000. running mean: -15.950913\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -15.991404\n",
            "resetting env. episode 1265.000000, reward total was -19.000000. running mean: -16.021490\n",
            "resetting env. episode 1266.000000, reward total was -18.000000. running mean: -16.041275\n",
            "resetting env. episode 1267.000000, reward total was -17.000000. running mean: -16.050862\n",
            "resetting env. episode 1268.000000, reward total was -10.000000. running mean: -15.990354\n",
            "resetting env. episode 1269.000000, reward total was -15.000000. running mean: -15.980450\n",
            "resetting env. episode 1270.000000, reward total was -18.000000. running mean: -16.000646\n",
            "resetting env. episode 1271.000000, reward total was -17.000000. running mean: -16.010639\n",
            "resetting env. episode 1272.000000, reward total was -11.000000. running mean: -15.960533\n",
            "resetting env. episode 1273.000000, reward total was -19.000000. running mean: -15.990928\n",
            "resetting env. episode 1274.000000, reward total was -14.000000. running mean: -15.971018\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -16.001308\n",
            "resetting env. episode 1276.000000, reward total was -14.000000. running mean: -15.981295\n",
            "resetting env. episode 1277.000000, reward total was -15.000000. running mean: -15.971482\n",
            "resetting env. episode 1278.000000, reward total was -19.000000. running mean: -16.001767\n",
            "resetting env. episode 1279.000000, reward total was -11.000000. running mean: -15.951750\n",
            "resetting env. episode 1280.000000, reward total was -18.000000. running mean: -15.972232\n",
            "resetting env. episode 1281.000000, reward total was -15.000000. running mean: -15.962510\n",
            "resetting env. episode 1282.000000, reward total was -19.000000. running mean: -15.992885\n",
            "resetting env. episode 1283.000000, reward total was -17.000000. running mean: -16.002956\n",
            "resetting env. episode 1284.000000, reward total was -18.000000. running mean: -16.022926\n",
            "resetting env. episode 1285.000000, reward total was -17.000000. running mean: -16.032697\n",
            "resetting env. episode 1286.000000, reward total was -12.000000. running mean: -15.992370\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -16.022446\n",
            "resetting env. episode 1288.000000, reward total was -15.000000. running mean: -16.012222\n",
            "resetting env. episode 1289.000000, reward total was -14.000000. running mean: -15.992100\n",
            "resetting env. episode 1290.000000, reward total was -16.000000. running mean: -15.992179\n",
            "resetting env. episode 1291.000000, reward total was -15.000000. running mean: -15.982257\n",
            "resetting env. episode 1292.000000, reward total was -17.000000. running mean: -15.992434\n",
            "resetting env. episode 1293.000000, reward total was -15.000000. running mean: -15.982510\n",
            "resetting env. episode 1294.000000, reward total was -15.000000. running mean: -15.972685\n",
            "resetting env. episode 1295.000000, reward total was -16.000000. running mean: -15.972958\n",
            "resetting env. episode 1296.000000, reward total was -14.000000. running mean: -15.953228\n",
            "resetting env. episode 1297.000000, reward total was -11.000000. running mean: -15.903696\n",
            "resetting env. episode 1298.000000, reward total was -7.000000. running mean: -15.814659\n",
            "resetting env. episode 1299.000000, reward total was -16.000000. running mean: -15.816513\n",
            "resetting env. episode 1300.000000, reward total was -13.000000. running mean: -15.788347\n",
            "resetting env. episode 1301.000000, reward total was -15.000000. running mean: -15.780464\n",
            "resetting env. episode 1302.000000, reward total was -18.000000. running mean: -15.802659\n",
            "resetting env. episode 1303.000000, reward total was -14.000000. running mean: -15.784633\n",
            "resetting env. episode 1304.000000, reward total was -17.000000. running mean: -15.796786\n",
            "resetting env. episode 1305.000000, reward total was -19.000000. running mean: -15.828819\n",
            "resetting env. episode 1306.000000, reward total was -15.000000. running mean: -15.820530\n",
            "resetting env. episode 1307.000000, reward total was -14.000000. running mean: -15.802325\n",
            "resetting env. episode 1308.000000, reward total was -19.000000. running mean: -15.834302\n",
            "resetting env. episode 1309.000000, reward total was -18.000000. running mean: -15.855959\n",
            "resetting env. episode 1310.000000, reward total was -13.000000. running mean: -15.827399\n",
            "resetting env. episode 1311.000000, reward total was -15.000000. running mean: -15.819125\n",
            "resetting env. episode 1312.000000, reward total was -15.000000. running mean: -15.810934\n",
            "resetting env. episode 1313.000000, reward total was -14.000000. running mean: -15.792825\n",
            "resetting env. episode 1314.000000, reward total was -16.000000. running mean: -15.794896\n",
            "resetting env. episode 1315.000000, reward total was -19.000000. running mean: -15.826947\n",
            "resetting env. episode 1316.000000, reward total was -13.000000. running mean: -15.798678\n",
            "resetting env. episode 1317.000000, reward total was -12.000000. running mean: -15.760691\n",
            "resetting env. episode 1318.000000, reward total was -19.000000. running mean: -15.793084\n",
            "resetting env. episode 1319.000000, reward total was -19.000000. running mean: -15.825153\n",
            "resetting env. episode 1320.000000, reward total was -19.000000. running mean: -15.856902\n",
            "resetting env. episode 1321.000000, reward total was -15.000000. running mean: -15.848333\n",
            "resetting env. episode 1322.000000, reward total was -15.000000. running mean: -15.839850\n",
            "resetting env. episode 1323.000000, reward total was -8.000000. running mean: -15.761451\n",
            "resetting env. episode 1324.000000, reward total was -10.000000. running mean: -15.703836\n",
            "resetting env. episode 1325.000000, reward total was -18.000000. running mean: -15.726798\n",
            "resetting env. episode 1326.000000, reward total was -11.000000. running mean: -15.679530\n",
            "resetting env. episode 1327.000000, reward total was -11.000000. running mean: -15.632735\n",
            "resetting env. episode 1328.000000, reward total was -15.000000. running mean: -15.626408\n",
            "resetting env. episode 1329.000000, reward total was -16.000000. running mean: -15.630143\n",
            "resetting env. episode 1330.000000, reward total was -17.000000. running mean: -15.643842\n",
            "resetting env. episode 1331.000000, reward total was -14.000000. running mean: -15.627404\n",
            "resetting env. episode 1332.000000, reward total was -13.000000. running mean: -15.601130\n",
            "resetting env. episode 1333.000000, reward total was -17.000000. running mean: -15.615118\n",
            "resetting env. episode 1334.000000, reward total was -15.000000. running mean: -15.608967\n",
            "resetting env. episode 1335.000000, reward total was -14.000000. running mean: -15.592877\n",
            "resetting env. episode 1336.000000, reward total was -14.000000. running mean: -15.576949\n",
            "resetting env. episode 1337.000000, reward total was -18.000000. running mean: -15.601179\n",
            "resetting env. episode 1338.000000, reward total was -14.000000. running mean: -15.585167\n",
            "resetting env. episode 1339.000000, reward total was -15.000000. running mean: -15.579316\n",
            "resetting env. episode 1340.000000, reward total was -14.000000. running mean: -15.563523\n",
            "resetting env. episode 1341.000000, reward total was -11.000000. running mean: -15.517887\n",
            "resetting env. episode 1342.000000, reward total was -11.000000. running mean: -15.472708\n",
            "resetting env. episode 1343.000000, reward total was -13.000000. running mean: -15.447981\n",
            "resetting env. episode 1344.000000, reward total was -11.000000. running mean: -15.403502\n",
            "resetting env. episode 1345.000000, reward total was -17.000000. running mean: -15.419466\n",
            "resetting env. episode 1346.000000, reward total was -14.000000. running mean: -15.405272\n",
            "resetting env. episode 1347.000000, reward total was -15.000000. running mean: -15.401219\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -15.437207\n",
            "resetting env. episode 1349.000000, reward total was -10.000000. running mean: -15.382835\n",
            "resetting env. episode 1350.000000, reward total was -19.000000. running mean: -15.419007\n",
            "resetting env. episode 1351.000000, reward total was -19.000000. running mean: -15.454816\n",
            "resetting env. episode 1352.000000, reward total was -11.000000. running mean: -15.410268\n",
            "resetting env. episode 1353.000000, reward total was -17.000000. running mean: -15.426166\n",
            "resetting env. episode 1354.000000, reward total was -11.000000. running mean: -15.381904\n",
            "resetting env. episode 1355.000000, reward total was -15.000000. running mean: -15.378085\n",
            "resetting env. episode 1356.000000, reward total was -13.000000. running mean: -15.354304\n",
            "resetting env. episode 1357.000000, reward total was -17.000000. running mean: -15.370761\n",
            "resetting env. episode 1358.000000, reward total was -13.000000. running mean: -15.347053\n",
            "resetting env. episode 1359.000000, reward total was -11.000000. running mean: -15.303583\n",
            "resetting env. episode 1360.000000, reward total was -17.000000. running mean: -15.320547\n",
            "resetting env. episode 1361.000000, reward total was -18.000000. running mean: -15.347342\n",
            "resetting env. episode 1362.000000, reward total was -16.000000. running mean: -15.353868\n",
            "resetting env. episode 1363.000000, reward total was -15.000000. running mean: -15.350329\n",
            "resetting env. episode 1364.000000, reward total was -11.000000. running mean: -15.306826\n",
            "resetting env. episode 1365.000000, reward total was -12.000000. running mean: -15.273758\n",
            "resetting env. episode 1366.000000, reward total was -15.000000. running mean: -15.271020\n",
            "resetting env. episode 1367.000000, reward total was -15.000000. running mean: -15.268310\n",
            "resetting env. episode 1368.000000, reward total was -18.000000. running mean: -15.295627\n",
            "resetting env. episode 1369.000000, reward total was -9.000000. running mean: -15.232671\n",
            "resetting env. episode 1370.000000, reward total was -17.000000. running mean: -15.250344\n",
            "resetting env. episode 1371.000000, reward total was -15.000000. running mean: -15.247841\n",
            "resetting env. episode 1372.000000, reward total was -15.000000. running mean: -15.245362\n",
            "resetting env. episode 1373.000000, reward total was -12.000000. running mean: -15.212909\n",
            "resetting env. episode 1374.000000, reward total was -18.000000. running mean: -15.240779\n",
            "resetting env. episode 1375.000000, reward total was -15.000000. running mean: -15.238372\n",
            "resetting env. episode 1376.000000, reward total was -13.000000. running mean: -15.215988\n",
            "resetting env. episode 1377.000000, reward total was -13.000000. running mean: -15.193828\n",
            "resetting env. episode 1378.000000, reward total was -16.000000. running mean: -15.201890\n",
            "resetting env. episode 1379.000000, reward total was -15.000000. running mean: -15.199871\n",
            "resetting env. episode 1380.000000, reward total was -14.000000. running mean: -15.187872\n",
            "resetting env. episode 1381.000000, reward total was -13.000000. running mean: -15.165993\n",
            "resetting env. episode 1382.000000, reward total was -19.000000. running mean: -15.204334\n",
            "resetting env. episode 1383.000000, reward total was -13.000000. running mean: -15.182290\n",
            "resetting env. episode 1384.000000, reward total was -12.000000. running mean: -15.150467\n",
            "resetting env. episode 1385.000000, reward total was -12.000000. running mean: -15.118963\n",
            "resetting env. episode 1386.000000, reward total was -13.000000. running mean: -15.097773\n",
            "resetting env. episode 1387.000000, reward total was -18.000000. running mean: -15.126795\n",
            "resetting env. episode 1388.000000, reward total was -16.000000. running mean: -15.135527\n",
            "resetting env. episode 1389.000000, reward total was -16.000000. running mean: -15.144172\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -15.182730\n",
            "resetting env. episode 1391.000000, reward total was -13.000000. running mean: -15.160903\n",
            "resetting env. episode 1392.000000, reward total was -13.000000. running mean: -15.139294\n",
            "resetting env. episode 1393.000000, reward total was -13.000000. running mean: -15.117901\n",
            "resetting env. episode 1394.000000, reward total was -15.000000. running mean: -15.116722\n",
            "resetting env. episode 1395.000000, reward total was -12.000000. running mean: -15.085555\n",
            "resetting env. episode 1396.000000, reward total was -17.000000. running mean: -15.104699\n",
            "resetting env. episode 1397.000000, reward total was -11.000000. running mean: -15.063652\n",
            "resetting env. episode 1398.000000, reward total was -7.000000. running mean: -14.983016\n",
            "resetting env. episode 1399.000000, reward total was -13.000000. running mean: -14.963186\n",
            "resetting env. episode 1400.000000, reward total was -15.000000. running mean: -14.963554\n",
            "resetting env. episode 1401.000000, reward total was -12.000000. running mean: -14.933918\n",
            "resetting env. episode 1402.000000, reward total was -17.000000. running mean: -14.954579\n",
            "resetting env. episode 1403.000000, reward total was -15.000000. running mean: -14.955033\n",
            "resetting env. episode 1404.000000, reward total was -15.000000. running mean: -14.955483\n",
            "resetting env. episode 1405.000000, reward total was -15.000000. running mean: -14.955928\n",
            "resetting env. episode 1406.000000, reward total was -13.000000. running mean: -14.936369\n",
            "resetting env. episode 1407.000000, reward total was -13.000000. running mean: -14.917005\n",
            "resetting env. episode 1408.000000, reward total was -10.000000. running mean: -14.867835\n",
            "resetting env. episode 1409.000000, reward total was -14.000000. running mean: -14.859157\n",
            "resetting env. episode 1410.000000, reward total was -17.000000. running mean: -14.880565\n",
            "resetting env. episode 1411.000000, reward total was -9.000000. running mean: -14.821759\n",
            "resetting env. episode 1412.000000, reward total was -17.000000. running mean: -14.843542\n",
            "resetting env. episode 1413.000000, reward total was -12.000000. running mean: -14.815106\n",
            "resetting env. episode 1414.000000, reward total was -16.000000. running mean: -14.826955\n",
            "resetting env. episode 1415.000000, reward total was -12.000000. running mean: -14.798686\n",
            "resetting env. episode 1416.000000, reward total was -11.000000. running mean: -14.760699\n",
            "resetting env. episode 1417.000000, reward total was -15.000000. running mean: -14.763092\n",
            "resetting env. episode 1418.000000, reward total was -14.000000. running mean: -14.755461\n",
            "resetting env. episode 1419.000000, reward total was -10.000000. running mean: -14.707906\n",
            "resetting env. episode 1420.000000, reward total was -14.000000. running mean: -14.700827\n",
            "resetting env. episode 1421.000000, reward total was -16.000000. running mean: -14.713819\n",
            "resetting env. episode 1422.000000, reward total was -15.000000. running mean: -14.716681\n",
            "resetting env. episode 1423.000000, reward total was -7.000000. running mean: -14.639514\n",
            "resetting env. episode 1424.000000, reward total was -11.000000. running mean: -14.603119\n",
            "resetting env. episode 1425.000000, reward total was -5.000000. running mean: -14.507088\n",
            "resetting env. episode 1426.000000, reward total was -15.000000. running mean: -14.512017\n",
            "resetting env. episode 1427.000000, reward total was -11.000000. running mean: -14.476897\n",
            "resetting env. episode 1428.000000, reward total was -16.000000. running mean: -14.492128\n",
            "resetting env. episode 1429.000000, reward total was -19.000000. running mean: -14.537207\n",
            "resetting env. episode 1430.000000, reward total was -13.000000. running mean: -14.521834\n",
            "resetting env. episode 1431.000000, reward total was -11.000000. running mean: -14.486616\n",
            "resetting env. episode 1432.000000, reward total was -17.000000. running mean: -14.511750\n",
            "resetting env. episode 1433.000000, reward total was -17.000000. running mean: -14.536632\n",
            "resetting env. episode 1434.000000, reward total was -15.000000. running mean: -14.541266\n",
            "resetting env. episode 1435.000000, reward total was -17.000000. running mean: -14.565853\n",
            "resetting env. episode 1436.000000, reward total was -14.000000. running mean: -14.560195\n",
            "resetting env. episode 1437.000000, reward total was -5.000000. running mean: -14.464593\n",
            "resetting env. episode 1438.000000, reward total was -11.000000. running mean: -14.429947\n",
            "resetting env. episode 1439.000000, reward total was -8.000000. running mean: -14.365648\n",
            "resetting env. episode 1440.000000, reward total was -12.000000. running mean: -14.341991\n",
            "resetting env. episode 1441.000000, reward total was -16.000000. running mean: -14.358571\n",
            "resetting env. episode 1442.000000, reward total was -18.000000. running mean: -14.394985\n",
            "resetting env. episode 1443.000000, reward total was -16.000000. running mean: -14.411036\n",
            "resetting env. episode 1444.000000, reward total was -16.000000. running mean: -14.426925\n",
            "resetting env. episode 1445.000000, reward total was -15.000000. running mean: -14.432656\n",
            "resetting env. episode 1446.000000, reward total was -14.000000. running mean: -14.428329\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -14.484046\n",
            "resetting env. episode 1448.000000, reward total was -13.000000. running mean: -14.469206\n",
            "resetting env. episode 1449.000000, reward total was -16.000000. running mean: -14.484514\n",
            "resetting env. episode 1450.000000, reward total was -15.000000. running mean: -14.489669\n",
            "resetting env. episode 1451.000000, reward total was -14.000000. running mean: -14.484772\n",
            "resetting env. episode 1452.000000, reward total was -17.000000. running mean: -14.509924\n",
            "resetting env. episode 1453.000000, reward total was -9.000000. running mean: -14.454825\n",
            "resetting env. episode 1454.000000, reward total was -14.000000. running mean: -14.450277\n",
            "resetting env. episode 1455.000000, reward total was -11.000000. running mean: -14.415774\n",
            "resetting env. episode 1456.000000, reward total was -17.000000. running mean: -14.441616\n",
            "resetting env. episode 1457.000000, reward total was -11.000000. running mean: -14.407200\n",
            "resetting env. episode 1458.000000, reward total was -12.000000. running mean: -14.383128\n",
            "resetting env. episode 1459.000000, reward total was -15.000000. running mean: -14.389297\n",
            "resetting env. episode 1460.000000, reward total was -17.000000. running mean: -14.415404\n",
            "resetting env. episode 1461.000000, reward total was -11.000000. running mean: -14.381250\n",
            "resetting env. episode 1462.000000, reward total was -17.000000. running mean: -14.407437\n",
            "resetting env. episode 1463.000000, reward total was -13.000000. running mean: -14.393363\n",
            "resetting env. episode 1464.000000, reward total was -10.000000. running mean: -14.349429\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -14.405935\n",
            "resetting env. episode 1466.000000, reward total was -16.000000. running mean: -14.421876\n",
            "resetting env. episode 1467.000000, reward total was -17.000000. running mean: -14.447657\n",
            "resetting env. episode 1468.000000, reward total was -18.000000. running mean: -14.483180\n",
            "resetting env. episode 1469.000000, reward total was -15.000000. running mean: -14.488348\n",
            "resetting env. episode 1470.000000, reward total was -16.000000. running mean: -14.503465\n",
            "resetting env. episode 1471.000000, reward total was -16.000000. running mean: -14.518430\n",
            "resetting env. episode 1472.000000, reward total was -13.000000. running mean: -14.503246\n",
            "resetting env. episode 1473.000000, reward total was -12.000000. running mean: -14.478214\n",
            "resetting env. episode 1474.000000, reward total was -16.000000. running mean: -14.493431\n",
            "resetting env. episode 1475.000000, reward total was -16.000000. running mean: -14.508497\n",
            "resetting env. episode 1476.000000, reward total was -14.000000. running mean: -14.503412\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -14.548378\n",
            "resetting env. episode 1478.000000, reward total was -13.000000. running mean: -14.532894\n",
            "resetting env. episode 1479.000000, reward total was -10.000000. running mean: -14.487565\n",
            "resetting env. episode 1480.000000, reward total was -9.000000. running mean: -14.432690\n",
            "resetting env. episode 1481.000000, reward total was -7.000000. running mean: -14.358363\n",
            "resetting env. episode 1482.000000, reward total was -15.000000. running mean: -14.364779\n",
            "resetting env. episode 1483.000000, reward total was -13.000000. running mean: -14.351131\n",
            "resetting env. episode 1484.000000, reward total was -17.000000. running mean: -14.377620\n",
            "resetting env. episode 1485.000000, reward total was -9.000000. running mean: -14.323844\n",
            "resetting env. episode 1486.000000, reward total was -17.000000. running mean: -14.350605\n",
            "resetting env. episode 1487.000000, reward total was -17.000000. running mean: -14.377099\n",
            "resetting env. episode 1488.000000, reward total was -17.000000. running mean: -14.403328\n",
            "resetting env. episode 1489.000000, reward total was -10.000000. running mean: -14.359295\n",
            "resetting env. episode 1490.000000, reward total was -13.000000. running mean: -14.345702\n",
            "resetting env. episode 1491.000000, reward total was -15.000000. running mean: -14.352245\n",
            "resetting env. episode 1492.000000, reward total was -11.000000. running mean: -14.318723\n",
            "resetting env. episode 1493.000000, reward total was -13.000000. running mean: -14.305535\n",
            "resetting env. episode 1494.000000, reward total was -18.000000. running mean: -14.342480\n",
            "resetting env. episode 1495.000000, reward total was -14.000000. running mean: -14.339055\n",
            "resetting env. episode 1496.000000, reward total was -13.000000. running mean: -14.325665\n",
            "resetting env. episode 1497.000000, reward total was -15.000000. running mean: -14.332408\n",
            "resetting env. episode 1498.000000, reward total was -15.000000. running mean: -14.339084\n",
            "resetting env. episode 1499.000000, reward total was -12.000000. running mean: -14.315693\n",
            "resetting env. episode 1500.000000, reward total was -17.000000. running mean: -14.342536\n",
            "CPU times: user 2h 27min 21s, sys: 1h 9min 36s, total: 3h 36min 57s\n",
            "Wall time: 1h 53min 22s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "w2NblmwDsL3y",
        "outputId": "b2de95b7-b172-4310-fd6b-70a6fc88f8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "H=200_le_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}