{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "7c3d2d29-9f4f-4fdf-cefa-f2db01934f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=dce0553cb746577ce90c75806d222713093e749e5e18f81e1a611304288f316a\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "ce61eb60-5987-499e-f8e7-e16788cf3bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "c32419e9-65a1-4c72-b897-db14179433c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "891d3e7d-8d5e-497d-ecea-dd91a7c1a541"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "206ec5d7-4f8b-45f3-8cc4-6988f568b0b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -19.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-2\n",
        "learning_rate = 1e-2\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "5dc79373-9bab-475e-d99a-c1876c9d58f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.039404\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.049010\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.048520\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.058035\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.047454\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.056980\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.066410\n",
            "resetting env. episode 12.000000, reward total was -18.000000. running mean: -20.045746\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.055288\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.064736\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.074088\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.083347\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.092514\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.091589\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.090673\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.099766\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.108768\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.117681\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.126504\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.135239\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.143886\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.152448\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.160923\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.169314\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.177621\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.185845\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.193986\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.202046\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.210026\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.217926\n",
            "resetting env. episode 35.000000, reward total was -18.000000. running mean: -20.195746\n",
            "resetting env. episode 36.000000, reward total was -17.000000. running mean: -20.163789\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.172151\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.160429\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.168825\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.177137\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.185366\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.193512\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.201577\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.209561\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.217465\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.225291\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.233038\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.240707\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.248300\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.255817\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.253259\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.260727\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.258119\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.265538\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.272883\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.280154\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.287352\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.294479\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.301534\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.308519\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.315434\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.322279\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.329056\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.335766\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.342408\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.348984\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.355494\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.361939\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.368320\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.364637\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.370990\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.377280\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.383508\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.389673\n",
            "resetting env. episode 75.000000, reward total was -18.000000. running mean: -20.365776\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.372118\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.378397\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.384613\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.390767\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.396859\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.402891\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.408862\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.414773\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.420625\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.426419\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.432155\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.437833\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.443455\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -20.429020\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.434730\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.440383\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.445979\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.451519\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.457004\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.462434\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.467810\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.473132\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.478400\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.483616\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.488780\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.493892\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.498953\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.503964\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.508924\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.513835\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.518697\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.523510\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.528275\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.532992\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.537662\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.542285\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.546862\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.551394\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.555880\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.560321\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.564718\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.569071\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.573380\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.577646\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.581870\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.586051\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.590191\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.594289\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.598346\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.602362\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.606339\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.610275\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.614173\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.618031\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.621850\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.625632\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.629376\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.633082\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.636751\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.620384\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.624180\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.627938\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.631659\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.635342\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.638989\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.642599\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.646173\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.649711\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.653214\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.656682\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.660115\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.663514\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.656879\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.660310\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.653707\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.657170\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.650598\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.654092\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.637551\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.641176\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.644764\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.638316\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.631933\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.625614\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.619358\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.623164\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.616932\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.610763\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.614655\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.618509\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.622324\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.616100\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.619939\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.613740\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.617603\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.621427\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.625212\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.628960\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.632671\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.636344\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.639981\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.633581\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.627245\n",
            "resetting env. episode 179.000000, reward total was -18.000000. running mean: -20.600972\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.594963\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.599013\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.603023\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.606993\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.600923\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.604914\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.608864\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.612776\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.606648\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.600582\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.604576\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.598530\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.602545\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.606519\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.590454\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.594550\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.598604\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.592618\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.596692\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.600725\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.604718\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.608670\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.612584\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.616458\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.620293\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.624090\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.617849\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.611671\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.605554\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.609499\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.603404\n",
            "resetting env. episode 211.000000, reward total was -18.000000. running mean: -20.577370\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.571596\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.555880\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.550321\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.544818\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.539370\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.543976\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.548536\n",
            "resetting env. episode 219.000000, reward total was -17.000000. running mean: -20.513051\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.517921\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.522741\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.527514\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.532239\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.536916\n",
            "resetting env. episode 225.000000, reward total was -18.000000. running mean: -20.511547\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.516432\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.501267\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.506255\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.501192\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.506180\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.511118\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.506007\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.500947\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.505938\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.510878\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.515770\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.510612\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.515506\n",
            "resetting env. episode 239.000000, reward total was -15.000000. running mean: -20.460351\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.465747\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.461090\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.466479\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.471814\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.477096\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.482325\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.477502\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.482727\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.487899\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.483020\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.488190\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.493308\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.488375\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.483491\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.488657\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.483770\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.478932\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.464143\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.469502\n",
            "resetting env. episode 259.000000, reward total was -18.000000. running mean: -20.444807\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.440358\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.445955\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.441495\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.427080\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.422810\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.418581\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.424396\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.420152\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.425950\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.431691\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.427374\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.433100\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.438769\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.424381\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.430138\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.435836\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.441478\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.427063\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.432792\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.438464\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.444080\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.449639\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.445143\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.450691\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.446184\n",
            "resetting env. episode 285.000000, reward total was -18.000000. running mean: -20.421722\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.417505\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.423330\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.429097\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.434806\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.430458\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.426153\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.431892\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.437573\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.443197\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.448765\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.454277\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.439735\n",
            "resetting env. episode 298.000000, reward total was -18.000000. running mean: -20.415337\n",
            "resetting env. episode 299.000000, reward total was -18.000000. running mean: -20.391184\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.397272\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.383299\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.389466\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.375572\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.381816\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.387998\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.384118\n",
            "resetting env. episode 307.000000, reward total was -18.000000. running mean: -20.360277\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.346674\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.353207\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.359675\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.346078\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.342618\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.349191\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.355700\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.352143\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.358621\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.355035\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.361485\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.367870\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.364191\n",
            "resetting env. episode 321.000000, reward total was -18.000000. running mean: -20.340549\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.347144\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.343672\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.330235\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.326933\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.323664\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.330427\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.327123\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.323852\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.320613\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.317407\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.324233\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.320991\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.317781\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.314603\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.311457\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.318342\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.325159\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.331907\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.328588\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.325302\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.322049\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.328829\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.335540\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.342185\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.348763\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.355276\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.351723\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.338206\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.334824\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.331475\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.338161\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.344779\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.341331\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.337918\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.334539\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.331193\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -20.317881\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.314703\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.321556\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.328340\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.335057\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.341706\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.348289\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.354806\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.361258\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.367645\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.373969\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.380229\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.386427\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.382563\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.378737\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.384950\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.391100\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.387189\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.393317\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.399384\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.395390\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.391436\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.397522\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.393547\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.399611\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.405615\n",
            "resetting env. episode 384.000000, reward total was -18.000000. running mean: -20.381559\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.377743\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.373966\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.370226\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.376524\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.372759\n",
            "resetting env. episode 390.000000, reward total was -18.000000. running mean: -20.349031\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.355541\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.351986\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.358466\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.364881\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.371232\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.377520\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.373745\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.370007\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.376307\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.382544\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.388719\n",
            "resetting env. episode 402.000000, reward total was -17.000000. running mean: -20.354832\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.341283\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.347870\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.334392\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.331048\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.337737\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.344360\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.350916\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.357407\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.353833\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.350295\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.346792\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.353324\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.349791\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.346293\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.332830\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.319501\n",
            "resetting env. episode 419.000000, reward total was -18.000000. running mean: -20.296306\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.293343\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.300410\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.297406\n",
            "resetting env. episode 423.000000, reward total was -15.000000. running mean: -20.244432\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.231988\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.239668\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.247271\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.254798\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -20.242250\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.239828\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.247429\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.254955\n",
            "resetting env. episode 432.000000, reward total was -18.000000. running mean: -20.232406\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.230082\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.227781\n",
            "resetting env. episode 435.000000, reward total was -18.000000. running mean: -20.205503\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.213448\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.211313\n",
            "resetting env. episode 438.000000, reward total was -17.000000. running mean: -20.179200\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.177408\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.185634\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.183778\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.181940\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.170121\n",
            "resetting env. episode 444.000000, reward total was -18.000000. running mean: -20.148419\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.156935\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.165366\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.163712\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.152075\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.160554\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.168949\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.157259\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.145687\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.144230\n",
            "resetting env. episode 454.000000, reward total was -18.000000. running mean: -20.122788\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.121560\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.130344\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.139041\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.127650\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.136374\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.135010\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.133660\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.132323\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.131000\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.129690\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.128393\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -20.117109\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.115938\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.104779\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.093731\n",
            "resetting env. episode 470.000000, reward total was -18.000000. running mean: -20.072794\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.062066\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.071445\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.060731\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.060123\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.069522\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.068827\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.058139\n",
            "resetting env. episode 478.000000, reward total was -17.000000. running mean: -20.027557\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.027282\n",
            "resetting env. episode 480.000000, reward total was -14.000000. running mean: -19.967009\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -19.967339\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -19.977665\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -19.987889\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -19.978010\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -19.988230\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -19.988347\n",
            "resetting env. episode 487.000000, reward total was -17.000000. running mean: -19.958464\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -19.968879\n",
            "resetting env. episode 489.000000, reward total was -16.000000. running mean: -19.929191\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -19.939899\n",
            "resetting env. episode 491.000000, reward total was -17.000000. running mean: -19.910500\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -19.921395\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -19.922181\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -19.922959\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -19.933729\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -19.934392\n",
            "resetting env. episode 497.000000, reward total was -17.000000. running mean: -19.905048\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -19.905998\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -19.906938\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -19.897868\n",
            "CPU times: user 23min 21s, sys: 10min 42s, total: 34min 4s\n",
            "Wall time: 17min 41s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "a61f4bf2-57d1-4157-ad0d-8953953c87f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -17.000000. running mean: -17.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -17.020000\n",
            "resetting env. episode 3.000000, reward total was -17.000000. running mean: -17.019800\n",
            "resetting env. episode 4.000000, reward total was -18.000000. running mean: -17.029602\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -17.049306\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -17.068813\n",
            "resetting env. episode 7.000000, reward total was -18.000000. running mean: -17.078125\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -17.097344\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -17.136370\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -17.175006\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -17.203256\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -17.231224\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -17.268912\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -17.306222\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -17.343160\n",
            "resetting env. episode 16.000000, reward total was -18.000000. running mean: -17.349729\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -17.376231\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -17.402469\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -17.438444\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -17.474060\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -17.509319\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -17.524226\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -17.558984\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -17.573394\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -17.587660\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -17.611783\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -17.635666\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -17.649309\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -17.682816\n",
            "resetting env. episode 30.000000, reward total was -17.000000. running mean: -17.675988\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -17.699228\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -17.732236\n",
            "resetting env. episode 33.000000, reward total was -18.000000. running mean: -17.734913\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -17.767564\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -17.789888\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -17.811990\n",
            "resetting env. episode 37.000000, reward total was -18.000000. running mean: -17.813870\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -17.825731\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -17.857474\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -17.878899\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -17.910110\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -17.921009\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -17.941799\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -17.972381\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -17.982657\n",
            "resetting env. episode 46.000000, reward total was -16.000000. running mean: -17.962830\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -17.993202\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -18.013270\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -18.043137\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -18.062706\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -18.082079\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -18.111258\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -18.120146\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -18.148944\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -18.177455\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -18.195680\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -18.223723\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -18.251486\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -18.278971\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -18.306181\n",
            "resetting env. episode 61.000000, reward total was -18.000000. running mean: -18.303120\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -18.330088\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -18.336788\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -18.363420\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -18.369786\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -18.376088\n",
            "resetting env. episode 67.000000, reward total was -16.000000. running mean: -18.352327\n",
            "resetting env. episode 68.000000, reward total was -15.000000. running mean: -18.318804\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -18.335615\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -18.352259\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -18.358737\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -18.375149\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -18.391398\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -18.417484\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -18.443309\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -18.448876\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -18.454387\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -18.469843\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -18.485145\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -18.490293\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -18.515391\n",
            "resetting env. episode 82.000000, reward total was -18.000000. running mean: -18.510237\n",
            "resetting env. episode 83.000000, reward total was -17.000000. running mean: -18.495134\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -18.510183\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -18.515081\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -18.529930\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -18.554631\n",
            "resetting env. episode 88.000000, reward total was -18.000000. running mean: -18.549085\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -18.573594\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -18.587858\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -18.611979\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -18.625859\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -18.639601\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -18.663205\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -18.666573\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -18.689907\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -18.713008\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -18.735878\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -18.758519\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -18.780934\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -18.803125\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -18.815093\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -18.816942\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -18.828773\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -18.850485\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -18.861980\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -18.873361\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -18.894627\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -18.915681\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -18.936524\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -18.947159\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -18.967687\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -18.978010\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -18.998230\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -19.008248\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.018165\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.037984\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -19.037604\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -19.037228\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.056856\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.076287\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.085524\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -19.084669\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.093822\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.112884\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -19.111755\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.130638\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -19.139331\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -19.147938\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -19.146459\n",
            "resetting env. episode 131.000000, reward total was -19.000000. running mean: -19.144994\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -19.153544\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.172009\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -19.170288\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -19.168586\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.186900\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -19.195031\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -19.203080\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.221050\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.238839\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.256451\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -19.263886\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -19.271247\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -19.278535\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.295750\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.312792\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -19.329664\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -19.336367\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -19.333004\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -19.339674\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -19.356277\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -19.352714\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -19.369187\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -19.385495\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -19.391640\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -19.407724\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -19.423647\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.439410\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.445016\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -19.460566\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -19.465960\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -19.481301\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -19.496488\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -19.501523\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -19.516508\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -19.521342\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -19.516129\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -19.520968\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -19.515758\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -19.530601\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.545295\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -19.549842\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -19.554343\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -19.568800\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -19.583112\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -19.597281\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.611308\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -19.615195\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -19.629043\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -19.632752\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -19.646425\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -19.649961\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -19.653461\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -19.666926\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -19.670257\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -19.673555\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -19.686819\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -19.699951\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -19.712951\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -19.725822\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -19.738564\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -19.751178\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -19.763666\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -19.746029\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -19.748569\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -19.761083\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -19.763473\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -19.765838\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -19.778180\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -19.790398\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -19.782494\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -19.794669\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -19.806722\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -19.808655\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -19.820568\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -19.832363\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -19.844039\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -19.855599\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -19.867043\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -19.868372\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -19.869689\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -19.880992\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -19.882182\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -19.873360\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -19.874626\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -19.885880\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -19.897021\n",
            "resetting env. episode 218.000000, reward total was -18.000000. running mean: -19.878051\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -19.879271\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -19.870478\n",
            "resetting env. episode 221.000000, reward total was -17.000000. running mean: -19.841773\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -19.853355\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -19.844822\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -19.836374\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -19.838010\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -19.839630\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -19.831233\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -19.822921\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -19.814692\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -19.816545\n",
            "resetting env. episode 231.000000, reward total was -17.000000. running mean: -19.788380\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -19.800496\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -19.802491\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -19.804466\n",
            "resetting env. episode 235.000000, reward total was -18.000000. running mean: -19.786421\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -19.788557\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -19.790671\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -19.802765\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -19.794737\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -19.796790\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -19.798822\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -19.810834\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -19.812725\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -19.814598\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -19.806452\n",
            "resetting env. episode 246.000000, reward total was -18.000000. running mean: -19.788387\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -19.770504\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -19.772799\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -19.765071\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -19.767420\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -19.779746\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -19.791948\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -19.784029\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -19.796188\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -19.808227\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -19.820144\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -19.811943\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -19.813823\n",
            "resetting env. episode 259.000000, reward total was -18.000000. running mean: -19.795685\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -19.787728\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -19.799851\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -19.801853\n",
            "resetting env. episode 263.000000, reward total was -14.000000. running mean: -19.743834\n",
            "resetting env. episode 264.000000, reward total was -17.000000. running mean: -19.716396\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -19.729232\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -19.741939\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -19.754520\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -19.766975\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -19.779305\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -19.791512\n",
            "resetting env. episode 271.000000, reward total was -18.000000. running mean: -19.773597\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -19.785861\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -19.788002\n",
            "resetting env. episode 274.000000, reward total was -18.000000. running mean: -19.770122\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -19.782421\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -19.794597\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -19.806651\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -19.818584\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -19.830399\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -19.832095\n",
            "resetting env. episode 281.000000, reward total was -18.000000. running mean: -19.813774\n",
            "resetting env. episode 282.000000, reward total was -18.000000. running mean: -19.795636\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -19.797680\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -19.799703\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -19.791706\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -19.793789\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -19.795851\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -19.807892\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -19.809813\n",
            "resetting env. episode 290.000000, reward total was -18.000000. running mean: -19.791715\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -19.803798\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -19.805760\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -19.807702\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -19.799625\n",
            "resetting env. episode 295.000000, reward total was -18.000000. running mean: -19.781629\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -19.793813\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -19.785875\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -19.798016\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -19.800036\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -19.792035\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -19.794115\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -19.806174\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -19.818112\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -19.819931\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -19.811732\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -19.813614\n",
            "resetting env. episode 307.000000, reward total was -16.000000. running mean: -19.775478\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -19.787724\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -19.779846\n",
            "resetting env. episode 310.000000, reward total was -17.000000. running mean: -19.752048\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -19.754527\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -19.756982\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -19.769412\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -19.771718\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -19.784001\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -19.786161\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -19.778299\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -19.770516\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -19.772811\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -19.765083\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -19.767432\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -19.769758\n",
            "resetting env. episode 323.000000, reward total was -16.000000. running mean: -19.732060\n",
            "resetting env. episode 324.000000, reward total was -18.000000. running mean: -19.714740\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -19.727592\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -19.730316\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -19.723013\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -19.725783\n",
            "resetting env. episode 329.000000, reward total was -18.000000. running mean: -19.708525\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -19.711440\n",
            "resetting env. episode 331.000000, reward total was -19.000000. running mean: -19.704326\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -19.697282\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -19.690310\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -19.703406\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -19.716372\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -19.729209\n",
            "resetting env. episode 337.000000, reward total was -17.000000. running mean: -19.701917\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -19.714897\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -19.707748\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -19.710671\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -19.693564\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -19.696629\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -19.689662\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -19.692766\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -19.695838\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -19.678880\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -19.672091\n",
            "resetting env. episode 348.000000, reward total was -18.000000. running mean: -19.655370\n",
            "resetting env. episode 349.000000, reward total was -17.000000. running mean: -19.628816\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -19.622528\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -19.626303\n",
            "resetting env. episode 352.000000, reward total was -18.000000. running mean: -19.610040\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -19.603939\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -19.617900\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -19.631721\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -19.635404\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -19.639050\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -19.642659\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -19.636233\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -19.639870\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -19.643472\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -19.647037\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -19.650567\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -19.654061\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -19.657520\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -19.670945\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -19.674236\n",
            "resetting env. episode 368.000000, reward total was -18.000000. running mean: -19.657493\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -19.660918\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -19.654309\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -19.647766\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -19.651288\n",
            "resetting env. episode 373.000000, reward total was -18.000000. running mean: -19.634776\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -19.628428\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -19.632143\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -19.635822\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -19.639464\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -19.653069\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -19.666538\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -19.669873\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -19.683174\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -19.696343\n",
            "resetting env. episode 383.000000, reward total was -18.000000. running mean: -19.679379\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -19.692585\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -19.705660\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -19.718603\n",
            "resetting env. episode 387.000000, reward total was -16.000000. running mean: -19.681417\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -19.674603\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -19.687857\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -19.680978\n",
            "resetting env. episode 391.000000, reward total was -18.000000. running mean: -19.664168\n",
            "resetting env. episode 392.000000, reward total was -18.000000. running mean: -19.647527\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -19.651051\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -19.654541\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -19.667996\n",
            "resetting env. episode 396.000000, reward total was -18.000000. running mean: -19.651316\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -19.664802\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -19.658154\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -19.651573\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -19.655057\n",
            "resetting env. episode 401.000000, reward total was -16.000000. running mean: -19.618507\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -19.622321\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -19.626098\n",
            "resetting env. episode 404.000000, reward total was -19.000000. running mean: -19.619837\n",
            "resetting env. episode 405.000000, reward total was -17.000000. running mean: -19.593639\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -19.597703\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -19.611725\n",
            "resetting env. episode 408.000000, reward total was -17.000000. running mean: -19.585608\n",
            "resetting env. episode 409.000000, reward total was -18.000000. running mean: -19.569752\n",
            "resetting env. episode 410.000000, reward total was -18.000000. running mean: -19.554055\n",
            "resetting env. episode 411.000000, reward total was -17.000000. running mean: -19.528514\n",
            "resetting env. episode 412.000000, reward total was -17.000000. running mean: -19.503229\n",
            "resetting env. episode 413.000000, reward total was -18.000000. running mean: -19.488197\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -19.483315\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -19.488482\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -19.503597\n",
            "resetting env. episode 417.000000, reward total was -18.000000. running mean: -19.488561\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -19.503675\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -19.518638\n",
            "resetting env. episode 420.000000, reward total was -18.000000. running mean: -19.503452\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -19.518417\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -19.523233\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -19.538001\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -19.542621\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -19.547195\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -19.541723\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -19.536306\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -19.530943\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -19.535633\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -19.540277\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -19.554874\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -19.569325\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -19.573632\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -19.577896\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -19.572117\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -19.586396\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -19.580532\n",
            "resetting env. episode 438.000000, reward total was -18.000000. running mean: -19.564726\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -19.559079\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -19.573488\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -19.567753\n",
            "resetting env. episode 442.000000, reward total was -18.000000. running mean: -19.552076\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -19.546555\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -19.541090\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -19.535679\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -19.530322\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -19.535019\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -19.529668\n",
            "resetting env. episode 449.000000, reward total was -18.000000. running mean: -19.514372\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -19.529228\n",
            "resetting env. episode 451.000000, reward total was -16.000000. running mean: -19.493936\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -19.508996\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -19.513906\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -19.518767\n",
            "resetting env. episode 455.000000, reward total was -17.000000. running mean: -19.493580\n",
            "resetting env. episode 456.000000, reward total was -16.000000. running mean: -19.458644\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -19.454057\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -19.459517\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -19.464922\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -19.480272\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -19.495470\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -19.500515\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -19.495510\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -19.500555\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -19.505549\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -19.510494\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -19.515389\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -19.520235\n",
            "resetting env. episode 469.000000, reward total was -17.000000. running mean: -19.495033\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -19.510082\n",
            "resetting env. episode 471.000000, reward total was -16.000000. running mean: -19.474981\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -19.480232\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -19.495429\n",
            "resetting env. episode 474.000000, reward total was -15.000000. running mean: -19.450475\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -19.455970\n",
            "resetting env. episode 476.000000, reward total was -18.000000. running mean: -19.441411\n",
            "resetting env. episode 477.000000, reward total was -17.000000. running mean: -19.416996\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -19.402827\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -19.408798\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -19.414710\n",
            "resetting env. episode 481.000000, reward total was -15.000000. running mean: -19.370563\n",
            "resetting env. episode 482.000000, reward total was -18.000000. running mean: -19.356858\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -19.353289\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -19.349756\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -19.346259\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -19.352796\n",
            "resetting env. episode 487.000000, reward total was -18.000000. running mean: -19.339268\n",
            "resetting env. episode 488.000000, reward total was -11.000000. running mean: -19.255875\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -19.263317\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -19.260683\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -19.278077\n",
            "resetting env. episode 492.000000, reward total was -18.000000. running mean: -19.265296\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -19.272643\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -19.259916\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -19.257317\n",
            "resetting env. episode 496.000000, reward total was -17.000000. running mean: -19.234744\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -19.232397\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -19.240073\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -19.237672\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -19.255295\n",
            "CPU times: user 27min 42s, sys: 12min 34s, total: 40min 17s\n",
            "Wall time: 20min 47s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "631bc752-c6f7-4a3c-ad9b-97cafccced33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHWklEQVR4nO3dv25bZRzH4deQksR2kzR/aElbAgWK+LOB2DrBAHfByoA6s7GwIsHQgUvgBnoJMCI2FKSCQE3c1ClpmjgpRWZAIBGX4u9JyrHN80zRic/JL1L8kd9XsU+j3+8XgMQTdQ8AjB/hAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQm6p64rsvzg79ttonGqVcWZsuzVOPr1PnlpdKc2Z24Hin2y17vd7Q11lamC/z7dPHnufu3r1y+84vx74OJ29nbbnsPXPm2NdpdnbKwo1bJzBRfa5e325UOa9yON57afBJWqdzKytl5czgH8NerxeGY6Gsra4ee56fNzvCMaJ2nnu63Hrj+WNfZ/nbH8c+HFVZqgAx4QBiwgHEhAOIVd4cnVR37u6WRtkY+vGn261yZm7uMU7Ef6W1cae0NgY3tPfPzpd75xdrmGh0CccRW9vbZWt7e+jHr62uCseEmL+xVVa/Xh84vvnmJeE4wlIFiAkHEBMOICYcQMzm6JBON5ul3WoNHJ9rDx6DSSccQ1pZWiwvXLxY9xgwEixVgJhwADHhAGLCAcRsjg5pb79XOt3uwPHWzGxpt5o1TAT1EY4hdbrdh4ZjbXW1XG6t1TAR1MdSBYgJBxATDiAmHEDM5ugRzZmZMjM9HT2eyXA43yx3n10aOH6w4P1IRwnHERfOnT2R+6owfrqvXSjd1y7UPcZYsFQBYsIBxIQDiAkHEJuYzdH9Xq/sTA3+Or8+eBBd5+DwftnZ3T32PL3Dg2Nfg8djerf30PunxNfZGf5m5pOm0e/3K5342XuL1U6Emp3kH27jBK9Vh6vXtyv9ChPzigOGNe5P9lFgjwOICQcQq7xUufLh5yc5BzBGKm+Odrtdm6Mw5paWlipt+ViqADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQq/y2+m++/PQk5wBq8PYHn1Q6z2eOwv9Y1c8ctVQBYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcTcrR5qdr81XXYunR04PtU7LAvfd0ql/wl/zIQDanaw2C4/vvN6KY2/J6K18UtZ+L5T01SPZqkCxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcACxkf08jlcuPV+as7MDx7+78UO5t79fw0TAn0Y2HHPtdplrt/92rN/vl1NTIzsyxBYuvlxaz50vD9ovlJsHN8vBbwd1jzQUz0Ko0Vvvf1xWLr9RSinl2vq1sn5vveaJhiMcUKtGaTRG8VNFH83mKBATDiAmHEDMHgfUaLfzQ5ma+ePfDp7sbJXZvZ2/vjd9Z6+usf6VcECNvvrio/LnHZda/X559cj3R3XbVDigVv1S+n98NaqReBh7HEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiI3s2+p/2tws06eeGjjeOxiPj4+HSTay4bh5a6vuEYB/YKkCxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQm6p64srlN09yDmCMNPr9fqUTb9++Xe1EYGQsLy83qpxX+RVHo1Hp5wETwB4HEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYpXvqwL8f3nFAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxA7HcupMyEyA+D+QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "9be0b8e1-1e1e-4be1-cbe8-979189f94c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.970299\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.960596\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.940990\n",
            "resetting env. episode 8.000000, reward total was -18.000000. running mean: -20.911580\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.892464\n",
            "resetting env. episode 10.000000, reward total was -17.000000. running mean: -20.853540\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.845004\n",
            "resetting env. episode 12.000000, reward total was -18.000000. running mean: -20.816554\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.798389\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.790405\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.772501\n",
            "resetting env. episode 16.000000, reward total was -18.000000. running mean: -20.744776\n",
            "resetting env. episode 17.000000, reward total was -18.000000. running mean: -20.717328\n",
            "resetting env. episode 18.000000, reward total was -16.000000. running mean: -20.670155\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.663453\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.646819\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.640350\n",
            "resetting env. episode 22.000000, reward total was -17.000000. running mean: -20.603947\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.597907\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.581928\n",
            "resetting env. episode 25.000000, reward total was -17.000000. running mean: -20.546109\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.540648\n",
            "resetting env. episode 27.000000, reward total was -18.000000. running mean: -20.515242\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.510089\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.504988\n",
            "resetting env. episode 30.000000, reward total was -16.000000. running mean: -20.459938\n",
            "resetting env. episode 31.000000, reward total was -19.000000. running mean: -20.445339\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -20.420886\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.406677\n",
            "resetting env. episode 34.000000, reward total was -17.000000. running mean: -20.372610\n",
            "resetting env. episode 35.000000, reward total was -17.000000. running mean: -20.338884\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.335495\n",
            "resetting env. episode 37.000000, reward total was -16.000000. running mean: -20.292140\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.299219\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.296227\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.303264\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -20.290232\n",
            "resetting env. episode 42.000000, reward total was -14.000000. running mean: -20.227329\n",
            "resetting env. episode 43.000000, reward total was -17.000000. running mean: -20.195056\n",
            "resetting env. episode 44.000000, reward total was -18.000000. running mean: -20.173105\n",
            "resetting env. episode 45.000000, reward total was -17.000000. running mean: -20.141374\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.139961\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.138561\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.137175\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.135804\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.124446\n",
            "resetting env. episode 51.000000, reward total was -18.000000. running mean: -20.103201\n",
            "resetting env. episode 52.000000, reward total was -18.000000. running mean: -20.082169\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.081347\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.080534\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.089729\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -20.078831\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.078043\n",
            "resetting env. episode 58.000000, reward total was -16.000000. running mean: -20.037263\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.036890\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.036521\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.036156\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -20.025794\n",
            "resetting env. episode 63.000000, reward total was -18.000000. running mean: -20.005536\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.015481\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.025326\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -20.015073\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.014922\n",
            "resetting env. episode 68.000000, reward total was -18.000000. running mean: -19.994773\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.004825\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -19.994777\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.994829\n",
            "resetting env. episode 72.000000, reward total was -16.000000. running mean: -19.954881\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -19.955332\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -19.945779\n",
            "resetting env. episode 75.000000, reward total was -18.000000. running mean: -19.926321\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.937058\n",
            "resetting env. episode 77.000000, reward total was -18.000000. running mean: -19.917687\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -19.908510\n",
            "resetting env. episode 79.000000, reward total was -18.000000. running mean: -19.889425\n",
            "resetting env. episode 80.000000, reward total was -17.000000. running mean: -19.860531\n",
            "resetting env. episode 81.000000, reward total was -17.000000. running mean: -19.831926\n",
            "resetting env. episode 82.000000, reward total was -19.000000. running mean: -19.823606\n",
            "resetting env. episode 83.000000, reward total was -16.000000. running mean: -19.785370\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -19.787517\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.789642\n",
            "resetting env. episode 86.000000, reward total was -19.000000. running mean: -19.781745\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.793928\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -19.785988\n",
            "resetting env. episode 89.000000, reward total was -14.000000. running mean: -19.728128\n",
            "resetting env. episode 90.000000, reward total was -17.000000. running mean: -19.700847\n",
            "resetting env. episode 91.000000, reward total was -16.000000. running mean: -19.663839\n",
            "resetting env. episode 92.000000, reward total was -16.000000. running mean: -19.627200\n",
            "resetting env. episode 93.000000, reward total was -18.000000. running mean: -19.610928\n",
            "resetting env. episode 94.000000, reward total was -17.000000. running mean: -19.584819\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -19.578971\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.583181\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.597349\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.601376\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.615362\n",
            "resetting env. episode 100.000000, reward total was -18.000000. running mean: -19.599208\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -19.593216\n",
            "resetting env. episode 102.000000, reward total was -18.000000. running mean: -19.577284\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -19.571511\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.585796\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.599938\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -19.603939\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.607900\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -19.601821\n",
            "resetting env. episode 109.000000, reward total was -18.000000. running mean: -19.585802\n",
            "resetting env. episode 110.000000, reward total was -18.000000. running mean: -19.569944\n",
            "resetting env. episode 111.000000, reward total was -18.000000. running mean: -19.554245\n",
            "resetting env. episode 112.000000, reward total was -18.000000. running mean: -19.538702\n",
            "resetting env. episode 113.000000, reward total was -18.000000. running mean: -19.523315\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.538082\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -19.532701\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.537374\n",
            "resetting env. episode 117.000000, reward total was -19.000000. running mean: -19.532001\n",
            "resetting env. episode 118.000000, reward total was -17.000000. running mean: -19.506681\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.511614\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.516498\n",
            "resetting env. episode 121.000000, reward total was -16.000000. running mean: -19.481333\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.486519\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.491654\n",
            "resetting env. episode 124.000000, reward total was -18.000000. running mean: -19.476738\n",
            "resetting env. episode 125.000000, reward total was -17.000000. running mean: -19.451970\n",
            "resetting env. episode 126.000000, reward total was -16.000000. running mean: -19.417451\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -19.413276\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -19.409143\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -19.405052\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.421001\n",
            "resetting env. episode 131.000000, reward total was -18.000000. running mean: -19.406791\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -19.402723\n",
            "resetting env. episode 133.000000, reward total was -18.000000. running mean: -19.388696\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.404809\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -19.410761\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -19.416654\n",
            "resetting env. episode 137.000000, reward total was -18.000000. running mean: -19.402487\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -19.398462\n",
            "resetting env. episode 139.000000, reward total was -16.000000. running mean: -19.364478\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.380833\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.397024\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -19.413054\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.428924\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.444634\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -19.440188\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.455786\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -19.451228\n",
            "resetting env. episode 148.000000, reward total was -17.000000. running mean: -19.426716\n",
            "resetting env. episode 149.000000, reward total was -18.000000. running mean: -19.412449\n",
            "resetting env. episode 150.000000, reward total was -15.000000. running mean: -19.368324\n",
            "resetting env. episode 151.000000, reward total was -17.000000. running mean: -19.344641\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.361195\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -19.367583\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -19.373907\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -19.380168\n",
            "resetting env. episode 156.000000, reward total was -17.000000. running mean: -19.356366\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -19.352803\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -19.359275\n",
            "resetting env. episode 159.000000, reward total was -18.000000. running mean: -19.345682\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -19.342225\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -19.348803\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -19.345315\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -19.351862\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -19.358343\n",
            "resetting env. episode 165.000000, reward total was -18.000000. running mean: -19.344759\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -19.341312\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -19.347899\n",
            "resetting env. episode 168.000000, reward total was -18.000000. running mean: -19.334420\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -19.331076\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -19.327765\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -19.334487\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -19.341142\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -19.337731\n",
            "resetting env. episode 174.000000, reward total was -18.000000. running mean: -19.324354\n",
            "resetting env. episode 175.000000, reward total was -18.000000. running mean: -19.311110\n",
            "resetting env. episode 176.000000, reward total was -16.000000. running mean: -19.277999\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.295219\n",
            "resetting env. episode 178.000000, reward total was -18.000000. running mean: -19.282267\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -19.289444\n",
            "resetting env. episode 180.000000, reward total was -14.000000. running mean: -19.236550\n",
            "resetting env. episode 181.000000, reward total was -18.000000. running mean: -19.224184\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -19.231942\n",
            "resetting env. episode 183.000000, reward total was -18.000000. running mean: -19.219623\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -19.217427\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -19.225252\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -19.223000\n",
            "resetting env. episode 187.000000, reward total was -18.000000. running mean: -19.210770\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -19.218662\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -19.226476\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -19.244211\n",
            "resetting env. episode 191.000000, reward total was -16.000000. running mean: -19.211769\n",
            "resetting env. episode 192.000000, reward total was -18.000000. running mean: -19.199651\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -19.217654\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -19.205478\n",
            "resetting env. episode 195.000000, reward total was -19.000000. running mean: -19.203423\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -19.221389\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -19.229175\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -19.236883\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -19.234514\n",
            "resetting env. episode 200.000000, reward total was -16.000000. running mean: -19.202169\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -19.200148\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -19.198146\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -19.196165\n",
            "resetting env. episode 204.000000, reward total was -18.000000. running mean: -19.184203\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -19.182361\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -19.200537\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -19.218532\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -19.236347\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -19.243983\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -19.261543\n",
            "resetting env. episode 211.000000, reward total was -17.000000. running mean: -19.238928\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -19.236539\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -19.234173\n",
            "resetting env. episode 214.000000, reward total was -18.000000. running mean: -19.221832\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -19.239613\n",
            "resetting env. episode 216.000000, reward total was -15.000000. running mean: -19.197217\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -19.215245\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -19.223092\n",
            "resetting env. episode 219.000000, reward total was -17.000000. running mean: -19.200862\n",
            "resetting env. episode 220.000000, reward total was -15.000000. running mean: -19.158853\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -19.177264\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -19.185492\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -19.183637\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -19.181800\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -19.199982\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -19.197983\n",
            "resetting env. episode 227.000000, reward total was -18.000000. running mean: -19.186003\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -19.184143\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -19.192301\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -19.210378\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -19.218275\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -19.226092\n",
            "resetting env. episode 233.000000, reward total was -17.000000. running mean: -19.203831\n",
            "resetting env. episode 234.000000, reward total was -18.000000. running mean: -19.191793\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -19.209875\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -19.227776\n",
            "resetting env. episode 237.000000, reward total was -18.000000. running mean: -19.215498\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -19.223343\n",
            "resetting env. episode 239.000000, reward total was -16.000000. running mean: -19.191110\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -19.199199\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -19.207207\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -19.205135\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -19.223083\n",
            "resetting env. episode 244.000000, reward total was -18.000000. running mean: -19.210852\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -19.218744\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -19.226556\n",
            "resetting env. episode 247.000000, reward total was -17.000000. running mean: -19.204291\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -19.212248\n",
            "resetting env. episode 249.000000, reward total was -18.000000. running mean: -19.200126\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -19.208124\n",
            "resetting env. episode 251.000000, reward total was -18.000000. running mean: -19.196043\n",
            "resetting env. episode 252.000000, reward total was -18.000000. running mean: -19.184083\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -19.192242\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -19.210319\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -19.218216\n",
            "resetting env. episode 256.000000, reward total was -18.000000. running mean: -19.206034\n",
            "resetting env. episode 257.000000, reward total was -17.000000. running mean: -19.183974\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -19.202134\n",
            "resetting env. episode 259.000000, reward total was -16.000000. running mean: -19.170113\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -19.168411\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -19.166727\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -19.165060\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -19.173409\n",
            "resetting env. episode 264.000000, reward total was -17.000000. running mean: -19.151675\n",
            "resetting env. episode 265.000000, reward total was -17.000000. running mean: -19.130159\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -19.138857\n",
            "resetting env. episode 267.000000, reward total was -17.000000. running mean: -19.117468\n",
            "resetting env. episode 268.000000, reward total was -17.000000. running mean: -19.096294\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -19.105331\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -19.124278\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -19.123035\n",
            "resetting env. episode 272.000000, reward total was -17.000000. running mean: -19.101804\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -19.100786\n",
            "resetting env. episode 274.000000, reward total was -18.000000. running mean: -19.089778\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -19.088881\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -19.097992\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -19.097012\n",
            "resetting env. episode 278.000000, reward total was -18.000000. running mean: -19.086042\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -19.095181\n",
            "resetting env. episode 280.000000, reward total was -18.000000. running mean: -19.084230\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -19.083387\n",
            "resetting env. episode 282.000000, reward total was -18.000000. running mean: -19.072553\n",
            "resetting env. episode 283.000000, reward total was -19.000000. running mean: -19.071828\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -19.091110\n",
            "resetting env. episode 285.000000, reward total was -18.000000. running mean: -19.080199\n",
            "resetting env. episode 286.000000, reward total was -18.000000. running mean: -19.069397\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -19.078703\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -19.087916\n",
            "resetting env. episode 289.000000, reward total was -16.000000. running mean: -19.057036\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -19.066466\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -19.065801\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -19.055143\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -19.054592\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -19.064046\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -19.073406\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -19.082672\n",
            "resetting env. episode 297.000000, reward total was -17.000000. running mean: -19.061845\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -19.081226\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -19.080414\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -19.079610\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -19.098814\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -19.117826\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -19.116647\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -19.135481\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -19.134126\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -19.152785\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -19.171257\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -19.179544\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -19.187749\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -19.185872\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -19.194013\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -19.212073\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -19.229952\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -19.237652\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -19.255276\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -19.262723\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -19.260096\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -19.267495\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -19.274820\n",
            "resetting env. episode 320.000000, reward total was -16.000000. running mean: -19.242072\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -19.239651\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -19.247255\n",
            "resetting env. episode 323.000000, reward total was -18.000000. running mean: -19.234782\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -19.232434\n",
            "resetting env. episode 325.000000, reward total was -18.000000. running mean: -19.220110\n",
            "resetting env. episode 326.000000, reward total was -18.000000. running mean: -19.207909\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -19.215830\n",
            "resetting env. episode 328.000000, reward total was -15.000000. running mean: -19.173671\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -19.181935\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -19.180115\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -19.188314\n",
            "resetting env. episode 332.000000, reward total was -18.000000. running mean: -19.176431\n",
            "resetting env. episode 333.000000, reward total was -17.000000. running mean: -19.154667\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -19.153120\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -19.151589\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -19.170073\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -19.178372\n",
            "resetting env. episode 338.000000, reward total was -17.000000. running mean: -19.156589\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -19.165023\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -19.183372\n",
            "resetting env. episode 341.000000, reward total was -16.000000. running mean: -19.151539\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -19.150023\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -19.158523\n",
            "resetting env. episode 344.000000, reward total was -17.000000. running mean: -19.136938\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -19.135568\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -19.134213\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -19.132871\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -19.141542\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -19.150127\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -19.148625\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -19.157139\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -19.155568\n",
            "resetting env. episode 353.000000, reward total was -18.000000. running mean: -19.144012\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -19.152572\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -19.161046\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -19.169436\n",
            "resetting env. episode 357.000000, reward total was -16.000000. running mean: -19.137741\n",
            "resetting env. episode 358.000000, reward total was -17.000000. running mean: -19.116364\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -19.135200\n",
            "resetting env. episode 360.000000, reward total was -18.000000. running mean: -19.123848\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -19.142610\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -19.141184\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -19.139772\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -19.148374\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -19.146890\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -19.145421\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -19.143967\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -19.142528\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -19.151102\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -19.169591\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -19.167895\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -19.186216\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -19.194354\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -19.212411\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -19.220287\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -19.238084\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -19.245703\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -19.263246\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -19.270613\n",
            "resetting env. episode 380.000000, reward total was -17.000000. running mean: -19.247907\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -19.265428\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -19.282774\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -19.279946\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -19.287147\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -19.304275\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -19.311232\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -19.328120\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -19.324839\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -19.331591\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -19.328275\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -19.334992\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -19.351642\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -19.348126\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -19.344644\n",
            "resetting env. episode 395.000000, reward total was -15.000000. running mean: -19.301198\n",
            "resetting env. episode 396.000000, reward total was -17.000000. running mean: -19.278186\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -19.295404\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -19.302450\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -19.319426\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -19.316231\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -19.333069\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -19.349738\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -19.346241\n",
            "resetting env. episode 404.000000, reward total was -17.000000. running mean: -19.322778\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -19.319551\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -19.336355\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -19.332992\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -19.349662\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -19.356165\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -19.352603\n",
            "resetting env. episode 411.000000, reward total was -18.000000. running mean: -19.339077\n",
            "resetting env. episode 412.000000, reward total was -18.000000. running mean: -19.325687\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -19.332430\n",
            "resetting env. episode 414.000000, reward total was -17.000000. running mean: -19.309105\n",
            "resetting env. episode 415.000000, reward total was -18.000000. running mean: -19.296014\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -19.293054\n",
            "resetting env. episode 417.000000, reward total was -17.000000. running mean: -19.270124\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -19.277422\n",
            "resetting env. episode 419.000000, reward total was -17.000000. running mean: -19.254648\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -19.272102\n",
            "resetting env. episode 421.000000, reward total was -16.000000. running mean: -19.239381\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -19.236987\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -19.244617\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -19.242171\n",
            "resetting env. episode 425.000000, reward total was -17.000000. running mean: -19.219749\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -19.237552\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -19.245176\n",
            "resetting env. episode 428.000000, reward total was -18.000000. running mean: -19.232724\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -19.250397\n",
            "resetting env. episode 430.000000, reward total was -18.000000. running mean: -19.237893\n",
            "resetting env. episode 431.000000, reward total was -17.000000. running mean: -19.215514\n",
            "resetting env. episode 432.000000, reward total was -17.000000. running mean: -19.193359\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -19.201426\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -19.199411\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -19.207417\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -19.215343\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -19.233190\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -19.230858\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -19.238549\n",
            "resetting env. episode 440.000000, reward total was -18.000000. running mean: -19.226164\n",
            "resetting env. episode 441.000000, reward total was -16.000000. running mean: -19.193902\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -19.211963\n",
            "resetting env. episode 443.000000, reward total was -15.000000. running mean: -19.169843\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -19.188145\n",
            "resetting env. episode 445.000000, reward total was -18.000000. running mean: -19.176263\n",
            "resetting env. episode 446.000000, reward total was -18.000000. running mean: -19.164501\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -19.172856\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -19.181127\n",
            "resetting env. episode 449.000000, reward total was -14.000000. running mean: -19.129316\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -19.148023\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -19.156543\n",
            "resetting env. episode 452.000000, reward total was -18.000000. running mean: -19.144977\n",
            "resetting env. episode 453.000000, reward total was -16.000000. running mean: -19.113527\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -19.112392\n",
            "resetting env. episode 455.000000, reward total was -16.000000. running mean: -19.081268\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -19.080456\n",
            "resetting env. episode 457.000000, reward total was -18.000000. running mean: -19.069651\n",
            "resetting env. episode 458.000000, reward total was -18.000000. running mean: -19.058954\n",
            "resetting env. episode 459.000000, reward total was -18.000000. running mean: -19.048365\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -19.067881\n",
            "resetting env. episode 461.000000, reward total was -18.000000. running mean: -19.057202\n",
            "resetting env. episode 462.000000, reward total was -17.000000. running mean: -19.036630\n",
            "resetting env. episode 463.000000, reward total was -18.000000. running mean: -19.026264\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -19.026001\n",
            "resetting env. episode 465.000000, reward total was -16.000000. running mean: -18.995741\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -18.995784\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -19.005826\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -19.025768\n",
            "resetting env. episode 469.000000, reward total was -18.000000. running mean: -19.015510\n",
            "resetting env. episode 470.000000, reward total was -18.000000. running mean: -19.005355\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -18.995302\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -19.015349\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -19.025195\n",
            "resetting env. episode 474.000000, reward total was -17.000000. running mean: -19.004943\n",
            "resetting env. episode 475.000000, reward total was -17.000000. running mean: -18.984894\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -18.985045\n",
            "resetting env. episode 477.000000, reward total was -16.000000. running mean: -18.955194\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -18.975642\n",
            "resetting env. episode 479.000000, reward total was -17.000000. running mean: -18.955886\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -18.956327\n",
            "resetting env. episode 481.000000, reward total was -17.000000. running mean: -18.936764\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -18.937396\n",
            "resetting env. episode 483.000000, reward total was -18.000000. running mean: -18.928022\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -18.928742\n",
            "resetting env. episode 485.000000, reward total was -15.000000. running mean: -18.889455\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -18.880560\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -18.901754\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -18.902737\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -18.923710\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -18.934472\n",
            "resetting env. episode 491.000000, reward total was -15.000000. running mean: -18.895128\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -18.906176\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -18.927115\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -18.947844\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -18.968365\n",
            "resetting env. episode 496.000000, reward total was -17.000000. running mean: -18.948681\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -18.959195\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -18.969603\n",
            "resetting env. episode 499.000000, reward total was -17.000000. running mean: -18.949907\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -18.960408\n",
            "resetting env. episode 501.000000, reward total was -19.000000. running mean: -18.960804\n",
            "resetting env. episode 502.000000, reward total was -17.000000. running mean: -18.941195\n",
            "resetting env. episode 503.000000, reward total was -17.000000. running mean: -18.921784\n",
            "resetting env. episode 504.000000, reward total was -19.000000. running mean: -18.922566\n",
            "resetting env. episode 505.000000, reward total was -18.000000. running mean: -18.913340\n",
            "resetting env. episode 506.000000, reward total was -18.000000. running mean: -18.904207\n",
            "resetting env. episode 507.000000, reward total was -19.000000. running mean: -18.905165\n",
            "resetting env. episode 508.000000, reward total was -16.000000. running mean: -18.876113\n",
            "resetting env. episode 509.000000, reward total was -17.000000. running mean: -18.857352\n",
            "resetting env. episode 510.000000, reward total was -18.000000. running mean: -18.848778\n",
            "resetting env. episode 511.000000, reward total was -16.000000. running mean: -18.820290\n",
            "resetting env. episode 512.000000, reward total was -19.000000. running mean: -18.822088\n",
            "resetting env. episode 513.000000, reward total was -19.000000. running mean: -18.823867\n",
            "resetting env. episode 514.000000, reward total was -18.000000. running mean: -18.815628\n",
            "resetting env. episode 515.000000, reward total was -18.000000. running mean: -18.807472\n",
            "resetting env. episode 516.000000, reward total was -18.000000. running mean: -18.799397\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -18.821403\n",
            "resetting env. episode 518.000000, reward total was -16.000000. running mean: -18.793189\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -18.805257\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -18.817205\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -18.829033\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -18.840742\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -18.862335\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -18.873711\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -18.884974\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -18.906125\n",
            "resetting env. episode 527.000000, reward total was -16.000000. running mean: -18.877063\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -18.888293\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -18.899410\n",
            "resetting env. episode 530.000000, reward total was -18.000000. running mean: -18.890416\n",
            "resetting env. episode 531.000000, reward total was -18.000000. running mean: -18.881512\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -18.892696\n",
            "resetting env. episode 533.000000, reward total was -19.000000. running mean: -18.893769\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -18.914832\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -18.935683\n",
            "resetting env. episode 536.000000, reward total was -19.000000. running mean: -18.936327\n",
            "resetting env. episode 537.000000, reward total was -19.000000. running mean: -18.936963\n",
            "resetting env. episode 538.000000, reward total was -19.000000. running mean: -18.937594\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -18.948218\n",
            "resetting env. episode 540.000000, reward total was -17.000000. running mean: -18.928736\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -18.949448\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -18.959954\n",
            "resetting env. episode 543.000000, reward total was -16.000000. running mean: -18.930354\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -18.951051\n",
            "resetting env. episode 545.000000, reward total was -18.000000. running mean: -18.941540\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -18.942125\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -18.952704\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -18.973176\n",
            "resetting env. episode 549.000000, reward total was -13.000000. running mean: -18.913445\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -18.924310\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -18.945067\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -18.955616\n",
            "resetting env. episode 553.000000, reward total was -17.000000. running mean: -18.936060\n",
            "resetting env. episode 554.000000, reward total was -17.000000. running mean: -18.916700\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -18.927533\n",
            "resetting env. episode 556.000000, reward total was -19.000000. running mean: -18.928257\n",
            "resetting env. episode 557.000000, reward total was -14.000000. running mean: -18.878975\n",
            "resetting env. episode 558.000000, reward total was -16.000000. running mean: -18.850185\n",
            "resetting env. episode 559.000000, reward total was -19.000000. running mean: -18.851683\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -18.873166\n",
            "resetting env. episode 561.000000, reward total was -18.000000. running mean: -18.864435\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -18.885790\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -18.906932\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -18.917863\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -18.938685\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -18.959298\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -18.969705\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -18.980008\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -19.000208\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -19.020205\n",
            "resetting env. episode 571.000000, reward total was -18.000000. running mean: -19.010003\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -19.019903\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -19.029704\n",
            "resetting env. episode 574.000000, reward total was -18.000000. running mean: -19.019407\n",
            "resetting env. episode 575.000000, reward total was -18.000000. running mean: -19.009213\n",
            "resetting env. episode 576.000000, reward total was -17.000000. running mean: -18.989121\n",
            "resetting env. episode 577.000000, reward total was -17.000000. running mean: -18.969230\n",
            "resetting env. episode 578.000000, reward total was -17.000000. running mean: -18.949538\n",
            "resetting env. episode 579.000000, reward total was -18.000000. running mean: -18.940042\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -18.940642\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -18.951235\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -18.971723\n",
            "resetting env. episode 583.000000, reward total was -15.000000. running mean: -18.932006\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -18.942686\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -18.963259\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -18.973626\n",
            "resetting env. episode 587.000000, reward total was -18.000000. running mean: -18.963890\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -18.964251\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -18.974609\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -18.984863\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -19.005014\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -19.024964\n",
            "resetting env. episode 593.000000, reward total was -17.000000. running mean: -19.004714\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -19.014667\n",
            "resetting env. episode 595.000000, reward total was -19.000000. running mean: -19.014520\n",
            "resetting env. episode 596.000000, reward total was -19.000000. running mean: -19.014375\n",
            "resetting env. episode 597.000000, reward total was -16.000000. running mean: -18.984231\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -18.994389\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -19.014445\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -19.034301\n",
            "resetting env. episode 601.000000, reward total was -19.000000. running mean: -19.033958\n",
            "resetting env. episode 602.000000, reward total was -17.000000. running mean: -19.013618\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -19.023482\n",
            "resetting env. episode 604.000000, reward total was -17.000000. running mean: -19.003247\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -19.023215\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -19.032983\n",
            "resetting env. episode 607.000000, reward total was -19.000000. running mean: -19.032653\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -19.052326\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -19.071803\n",
            "resetting env. episode 610.000000, reward total was -17.000000. running mean: -19.051085\n",
            "resetting env. episode 611.000000, reward total was -19.000000. running mean: -19.050574\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -19.070068\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -19.079368\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -19.088574\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -19.107688\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -19.116611\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -19.135445\n",
            "resetting env. episode 618.000000, reward total was -19.000000. running mean: -19.134091\n",
            "resetting env. episode 619.000000, reward total was -17.000000. running mean: -19.112750\n",
            "resetting env. episode 620.000000, reward total was -19.000000. running mean: -19.111622\n",
            "resetting env. episode 621.000000, reward total was -17.000000. running mean: -19.090506\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -19.099601\n",
            "resetting env. episode 623.000000, reward total was -19.000000. running mean: -19.098605\n",
            "resetting env. episode 624.000000, reward total was -20.000000. running mean: -19.107619\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -19.116543\n",
            "resetting env. episode 626.000000, reward total was -19.000000. running mean: -19.115377\n",
            "resetting env. episode 627.000000, reward total was -19.000000. running mean: -19.114224\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -19.133081\n",
            "resetting env. episode 629.000000, reward total was -18.000000. running mean: -19.121751\n",
            "resetting env. episode 630.000000, reward total was -17.000000. running mean: -19.100533\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -19.119528\n",
            "resetting env. episode 632.000000, reward total was -13.000000. running mean: -19.058332\n",
            "resetting env. episode 633.000000, reward total was -17.000000. running mean: -19.037749\n",
            "resetting env. episode 634.000000, reward total was -19.000000. running mean: -19.037372\n",
            "resetting env. episode 635.000000, reward total was -19.000000. running mean: -19.036998\n",
            "resetting env. episode 636.000000, reward total was -18.000000. running mean: -19.026628\n",
            "resetting env. episode 637.000000, reward total was -17.000000. running mean: -19.006362\n",
            "resetting env. episode 638.000000, reward total was -15.000000. running mean: -18.966298\n",
            "resetting env. episode 639.000000, reward total was -20.000000. running mean: -18.976635\n",
            "resetting env. episode 640.000000, reward total was -19.000000. running mean: -18.976869\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -18.987100\n",
            "resetting env. episode 642.000000, reward total was -18.000000. running mean: -18.977229\n",
            "resetting env. episode 643.000000, reward total was -19.000000. running mean: -18.977457\n",
            "resetting env. episode 644.000000, reward total was -19.000000. running mean: -18.977682\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -18.977905\n",
            "resetting env. episode 646.000000, reward total was -19.000000. running mean: -18.978126\n",
            "resetting env. episode 647.000000, reward total was -20.000000. running mean: -18.988345\n",
            "resetting env. episode 648.000000, reward total was -20.000000. running mean: -18.998462\n",
            "resetting env. episode 649.000000, reward total was -18.000000. running mean: -18.988477\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -18.998592\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -19.008606\n",
            "resetting env. episode 652.000000, reward total was -19.000000. running mean: -19.008520\n",
            "resetting env. episode 653.000000, reward total was -17.000000. running mean: -18.988435\n",
            "resetting env. episode 654.000000, reward total was -19.000000. running mean: -18.988551\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -18.988665\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -18.998778\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -19.008791\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -19.018703\n",
            "resetting env. episode 659.000000, reward total was -19.000000. running mean: -19.018516\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -19.038331\n",
            "resetting env. episode 661.000000, reward total was -18.000000. running mean: -19.027947\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -19.027668\n",
            "resetting env. episode 663.000000, reward total was -17.000000. running mean: -19.007391\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -19.017317\n",
            "resetting env. episode 665.000000, reward total was -19.000000. running mean: -19.017144\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -19.036973\n",
            "resetting env. episode 667.000000, reward total was -17.000000. running mean: -19.016603\n",
            "resetting env. episode 668.000000, reward total was -16.000000. running mean: -18.986437\n",
            "resetting env. episode 669.000000, reward total was -19.000000. running mean: -18.986573\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -18.996707\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -18.996740\n",
            "resetting env. episode 672.000000, reward total was -19.000000. running mean: -18.996772\n",
            "resetting env. episode 673.000000, reward total was -17.000000. running mean: -18.976805\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -18.987037\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -18.997166\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -18.997195\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -19.017223\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -19.017050\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -19.026880\n",
            "resetting env. episode 680.000000, reward total was -17.000000. running mean: -19.006611\n",
            "resetting env. episode 681.000000, reward total was -16.000000. running mean: -18.976545\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -18.996779\n",
            "resetting env. episode 683.000000, reward total was -19.000000. running mean: -18.996812\n",
            "resetting env. episode 684.000000, reward total was -19.000000. running mean: -18.996844\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -19.016875\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -19.036706\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -19.056339\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -19.065776\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -19.085118\n",
            "resetting env. episode 690.000000, reward total was -18.000000. running mean: -19.074267\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -19.083524\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -19.092689\n",
            "resetting env. episode 693.000000, reward total was -18.000000. running mean: -19.081762\n",
            "resetting env. episode 694.000000, reward total was -19.000000. running mean: -19.080945\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -19.080135\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -19.089334\n",
            "resetting env. episode 697.000000, reward total was -19.000000. running mean: -19.088440\n",
            "resetting env. episode 698.000000, reward total was -19.000000. running mean: -19.087556\n",
            "resetting env. episode 699.000000, reward total was -16.000000. running mean: -19.056680\n",
            "resetting env. episode 700.000000, reward total was -15.000000. running mean: -19.016114\n",
            "resetting env. episode 701.000000, reward total was -18.000000. running mean: -19.005953\n",
            "resetting env. episode 702.000000, reward total was -13.000000. running mean: -18.945893\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -18.966434\n",
            "resetting env. episode 704.000000, reward total was -19.000000. running mean: -18.966770\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -18.977102\n",
            "resetting env. episode 706.000000, reward total was -18.000000. running mean: -18.967331\n",
            "resetting env. episode 707.000000, reward total was -19.000000. running mean: -18.967658\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -18.977981\n",
            "resetting env. episode 709.000000, reward total was -17.000000. running mean: -18.958201\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -18.968619\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -18.968933\n",
            "resetting env. episode 712.000000, reward total was -17.000000. running mean: -18.949244\n",
            "resetting env. episode 713.000000, reward total was -19.000000. running mean: -18.949751\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -18.960254\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -18.970651\n",
            "resetting env. episode 716.000000, reward total was -19.000000. running mean: -18.970945\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -18.991235\n",
            "resetting env. episode 718.000000, reward total was -19.000000. running mean: -18.991323\n",
            "resetting env. episode 719.000000, reward total was -16.000000. running mean: -18.961410\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -18.971796\n",
            "resetting env. episode 721.000000, reward total was -17.000000. running mean: -18.952078\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -18.972557\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -18.992831\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -19.002903\n",
            "resetting env. episode 725.000000, reward total was -19.000000. running mean: -19.002874\n",
            "resetting env. episode 726.000000, reward total was -15.000000. running mean: -18.962845\n",
            "resetting env. episode 727.000000, reward total was -17.000000. running mean: -18.943217\n",
            "resetting env. episode 728.000000, reward total was -17.000000. running mean: -18.923785\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -18.944547\n",
            "resetting env. episode 730.000000, reward total was -18.000000. running mean: -18.935101\n",
            "resetting env. episode 731.000000, reward total was -17.000000. running mean: -18.915750\n",
            "resetting env. episode 732.000000, reward total was -17.000000. running mean: -18.896593\n",
            "resetting env. episode 733.000000, reward total was -18.000000. running mean: -18.887627\n",
            "resetting env. episode 734.000000, reward total was -18.000000. running mean: -18.878751\n",
            "resetting env. episode 735.000000, reward total was -15.000000. running mean: -18.839963\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -18.851563\n",
            "resetting env. episode 737.000000, reward total was -18.000000. running mean: -18.843048\n",
            "resetting env. episode 738.000000, reward total was -18.000000. running mean: -18.834617\n",
            "resetting env. episode 739.000000, reward total was -18.000000. running mean: -18.826271\n",
            "resetting env. episode 740.000000, reward total was -19.000000. running mean: -18.828008\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -18.849728\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -18.851231\n",
            "resetting env. episode 743.000000, reward total was -18.000000. running mean: -18.842719\n",
            "resetting env. episode 744.000000, reward total was -18.000000. running mean: -18.834292\n",
            "resetting env. episode 745.000000, reward total was -20.000000. running mean: -18.845949\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -18.857489\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -18.878914\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -18.900125\n",
            "resetting env. episode 749.000000, reward total was -19.000000. running mean: -18.901124\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -18.922113\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -18.932892\n",
            "resetting env. episode 752.000000, reward total was -14.000000. running mean: -18.883563\n",
            "resetting env. episode 753.000000, reward total was -14.000000. running mean: -18.834727\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -18.846380\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -18.857916\n",
            "resetting env. episode 756.000000, reward total was -18.000000. running mean: -18.849337\n",
            "resetting env. episode 757.000000, reward total was -17.000000. running mean: -18.830843\n",
            "resetting env. episode 758.000000, reward total was -16.000000. running mean: -18.802535\n",
            "resetting env. episode 759.000000, reward total was -18.000000. running mean: -18.794510\n",
            "resetting env. episode 760.000000, reward total was -17.000000. running mean: -18.776565\n",
            "resetting env. episode 761.000000, reward total was -16.000000. running mean: -18.748799\n",
            "resetting env. episode 762.000000, reward total was -19.000000. running mean: -18.751311\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -18.753798\n",
            "resetting env. episode 764.000000, reward total was -18.000000. running mean: -18.746260\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -18.768797\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -18.781109\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -18.803298\n",
            "resetting env. episode 768.000000, reward total was -18.000000. running mean: -18.795265\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -18.797313\n",
            "resetting env. episode 770.000000, reward total was -18.000000. running mean: -18.789339\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -18.791446\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -18.813532\n",
            "resetting env. episode 773.000000, reward total was -18.000000. running mean: -18.805396\n",
            "resetting env. episode 774.000000, reward total was -19.000000. running mean: -18.807342\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -18.829269\n",
            "resetting env. episode 776.000000, reward total was -18.000000. running mean: -18.820976\n",
            "resetting env. episode 777.000000, reward total was -18.000000. running mean: -18.812766\n",
            "resetting env. episode 778.000000, reward total was -15.000000. running mean: -18.774639\n",
            "resetting env. episode 779.000000, reward total was -16.000000. running mean: -18.746892\n",
            "resetting env. episode 780.000000, reward total was -17.000000. running mean: -18.729423\n",
            "resetting env. episode 781.000000, reward total was -17.000000. running mean: -18.712129\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -18.735008\n",
            "resetting env. episode 783.000000, reward total was -16.000000. running mean: -18.707658\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -18.730581\n",
            "resetting env. episode 785.000000, reward total was -15.000000. running mean: -18.693275\n",
            "resetting env. episode 786.000000, reward total was -18.000000. running mean: -18.686343\n",
            "resetting env. episode 787.000000, reward total was -15.000000. running mean: -18.649479\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -18.662984\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -18.666355\n",
            "resetting env. episode 790.000000, reward total was -15.000000. running mean: -18.629691\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -18.633394\n",
            "resetting env. episode 792.000000, reward total was -18.000000. running mean: -18.627060\n",
            "resetting env. episode 793.000000, reward total was -16.000000. running mean: -18.600790\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -18.624782\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -18.648534\n",
            "resetting env. episode 796.000000, reward total was -19.000000. running mean: -18.652049\n",
            "resetting env. episode 797.000000, reward total was -19.000000. running mean: -18.655528\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -18.668973\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -18.692283\n",
            "resetting env. episode 800.000000, reward total was -10.000000. running mean: -18.605360\n",
            "resetting env. episode 801.000000, reward total was -14.000000. running mean: -18.559307\n",
            "resetting env. episode 802.000000, reward total was -19.000000. running mean: -18.563714\n",
            "resetting env. episode 803.000000, reward total was -18.000000. running mean: -18.558076\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -18.582496\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -18.586671\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -18.610804\n",
            "resetting env. episode 807.000000, reward total was -17.000000. running mean: -18.594696\n",
            "resetting env. episode 808.000000, reward total was -18.000000. running mean: -18.588749\n",
            "resetting env. episode 809.000000, reward total was -17.000000. running mean: -18.572862\n",
            "resetting env. episode 810.000000, reward total was -18.000000. running mean: -18.567133\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -18.581462\n",
            "resetting env. episode 812.000000, reward total was -18.000000. running mean: -18.575647\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -18.589890\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -18.593992\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -18.608052\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -18.621971\n",
            "resetting env. episode 817.000000, reward total was -14.000000. running mean: -18.575751\n",
            "resetting env. episode 818.000000, reward total was -19.000000. running mean: -18.579994\n",
            "resetting env. episode 819.000000, reward total was -18.000000. running mean: -18.574194\n",
            "resetting env. episode 820.000000, reward total was -19.000000. running mean: -18.578452\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -18.592668\n",
            "resetting env. episode 822.000000, reward total was -18.000000. running mean: -18.586741\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -18.610873\n",
            "resetting env. episode 824.000000, reward total was -16.000000. running mean: -18.584765\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -18.598917\n",
            "resetting env. episode 826.000000, reward total was -19.000000. running mean: -18.602928\n",
            "resetting env. episode 827.000000, reward total was -16.000000. running mean: -18.576899\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -18.591130\n",
            "resetting env. episode 829.000000, reward total was -18.000000. running mean: -18.585218\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -18.609366\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -18.623272\n",
            "resetting env. episode 832.000000, reward total was -18.000000. running mean: -18.617040\n",
            "resetting env. episode 833.000000, reward total was -16.000000. running mean: -18.590869\n",
            "resetting env. episode 834.000000, reward total was -17.000000. running mean: -18.574961\n",
            "resetting env. episode 835.000000, reward total was -18.000000. running mean: -18.569211\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -18.593519\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -18.597584\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -18.621608\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -18.635392\n",
            "resetting env. episode 840.000000, reward total was -18.000000. running mean: -18.629038\n",
            "resetting env. episode 841.000000, reward total was -16.000000. running mean: -18.602748\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -18.626720\n",
            "resetting env. episode 843.000000, reward total was -19.000000. running mean: -18.630453\n",
            "resetting env. episode 844.000000, reward total was -19.000000. running mean: -18.634148\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -18.657807\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -18.681229\n",
            "resetting env. episode 847.000000, reward total was -16.000000. running mean: -18.654416\n",
            "resetting env. episode 848.000000, reward total was -17.000000. running mean: -18.637872\n",
            "resetting env. episode 849.000000, reward total was -19.000000. running mean: -18.641494\n",
            "resetting env. episode 850.000000, reward total was -17.000000. running mean: -18.625079\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -18.638828\n",
            "resetting env. episode 852.000000, reward total was -18.000000. running mean: -18.632440\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -18.646115\n",
            "resetting env. episode 854.000000, reward total was -19.000000. running mean: -18.649654\n",
            "resetting env. episode 855.000000, reward total was -16.000000. running mean: -18.623158\n",
            "resetting env. episode 856.000000, reward total was -17.000000. running mean: -18.606926\n",
            "resetting env. episode 857.000000, reward total was -17.000000. running mean: -18.590857\n",
            "resetting env. episode 858.000000, reward total was -19.000000. running mean: -18.594948\n",
            "resetting env. episode 859.000000, reward total was -12.000000. running mean: -18.528999\n",
            "resetting env. episode 860.000000, reward total was -19.000000. running mean: -18.533709\n",
            "resetting env. episode 861.000000, reward total was -16.000000. running mean: -18.508372\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -18.523288\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -18.538055\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -18.542674\n",
            "resetting env. episode 865.000000, reward total was -18.000000. running mean: -18.537248\n",
            "resetting env. episode 866.000000, reward total was -19.000000. running mean: -18.541875\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -18.556456\n",
            "resetting env. episode 868.000000, reward total was -17.000000. running mean: -18.540892\n",
            "resetting env. episode 869.000000, reward total was -16.000000. running mean: -18.515483\n",
            "resetting env. episode 870.000000, reward total was -18.000000. running mean: -18.510328\n",
            "resetting env. episode 871.000000, reward total was -18.000000. running mean: -18.505225\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -18.520173\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -18.544971\n",
            "resetting env. episode 874.000000, reward total was -18.000000. running mean: -18.539521\n",
            "resetting env. episode 875.000000, reward total was -19.000000. running mean: -18.544126\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -18.558685\n",
            "resetting env. episode 877.000000, reward total was -18.000000. running mean: -18.553098\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -18.557567\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -18.561991\n",
            "resetting env. episode 880.000000, reward total was -19.000000. running mean: -18.566371\n",
            "resetting env. episode 881.000000, reward total was -18.000000. running mean: -18.560708\n",
            "resetting env. episode 882.000000, reward total was -13.000000. running mean: -18.505100\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -18.520049\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -18.544849\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -18.549401\n",
            "resetting env. episode 886.000000, reward total was -13.000000. running mean: -18.493907\n",
            "resetting env. episode 887.000000, reward total was -15.000000. running mean: -18.458967\n",
            "resetting env. episode 888.000000, reward total was -19.000000. running mean: -18.464378\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -18.479734\n",
            "resetting env. episode 890.000000, reward total was -16.000000. running mean: -18.454937\n",
            "resetting env. episode 891.000000, reward total was -19.000000. running mean: -18.460387\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -18.475783\n",
            "resetting env. episode 893.000000, reward total was -18.000000. running mean: -18.471026\n",
            "resetting env. episode 894.000000, reward total was -19.000000. running mean: -18.476315\n",
            "resetting env. episode 895.000000, reward total was -15.000000. running mean: -18.441552\n",
            "resetting env. episode 896.000000, reward total was -14.000000. running mean: -18.397137\n",
            "resetting env. episode 897.000000, reward total was -17.000000. running mean: -18.383165\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -18.409334\n",
            "resetting env. episode 899.000000, reward total was -17.000000. running mean: -18.395240\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -18.411288\n",
            "resetting env. episode 901.000000, reward total was -17.000000. running mean: -18.397175\n",
            "resetting env. episode 902.000000, reward total was -15.000000. running mean: -18.363203\n",
            "resetting env. episode 903.000000, reward total was -19.000000. running mean: -18.369571\n",
            "resetting env. episode 904.000000, reward total was -19.000000. running mean: -18.375876\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -18.402117\n",
            "resetting env. episode 906.000000, reward total was -14.000000. running mean: -18.358096\n",
            "resetting env. episode 907.000000, reward total was -19.000000. running mean: -18.364515\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -18.380869\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -18.397061\n",
            "resetting env. episode 910.000000, reward total was -17.000000. running mean: -18.383090\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -18.399259\n",
            "resetting env. episode 912.000000, reward total was -18.000000. running mean: -18.395267\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -18.401314\n",
            "resetting env. episode 914.000000, reward total was -18.000000. running mean: -18.397301\n",
            "resetting env. episode 915.000000, reward total was -17.000000. running mean: -18.383328\n",
            "resetting env. episode 916.000000, reward total was -19.000000. running mean: -18.389495\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -18.405600\n",
            "resetting env. episode 918.000000, reward total was -19.000000. running mean: -18.411544\n",
            "resetting env. episode 919.000000, reward total was -16.000000. running mean: -18.387428\n",
            "resetting env. episode 920.000000, reward total was -10.000000. running mean: -18.303554\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -18.320518\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -18.347313\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -18.373840\n",
            "resetting env. episode 924.000000, reward total was -15.000000. running mean: -18.340102\n",
            "resetting env. episode 925.000000, reward total was -19.000000. running mean: -18.346701\n",
            "resetting env. episode 926.000000, reward total was -17.000000. running mean: -18.333234\n",
            "resetting env. episode 927.000000, reward total was -18.000000. running mean: -18.329901\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -18.346602\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -18.373136\n",
            "resetting env. episode 930.000000, reward total was -18.000000. running mean: -18.369405\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -18.385711\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -18.391854\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -18.417935\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -18.443756\n",
            "resetting env. episode 935.000000, reward total was -17.000000. running mean: -18.429318\n",
            "resetting env. episode 936.000000, reward total was -17.000000. running mean: -18.415025\n",
            "resetting env. episode 937.000000, reward total was -18.000000. running mean: -18.410875\n",
            "resetting env. episode 938.000000, reward total was -12.000000. running mean: -18.346766\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -18.353298\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -18.379765\n",
            "resetting env. episode 941.000000, reward total was -19.000000. running mean: -18.385968\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -18.412108\n",
            "resetting env. episode 943.000000, reward total was -17.000000. running mean: -18.397987\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -18.414007\n",
            "resetting env. episode 945.000000, reward total was -18.000000. running mean: -18.409867\n",
            "resetting env. episode 946.000000, reward total was -17.000000. running mean: -18.395768\n",
            "resetting env. episode 947.000000, reward total was -16.000000. running mean: -18.371811\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -18.388093\n",
            "resetting env. episode 949.000000, reward total was -17.000000. running mean: -18.374212\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -18.390470\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -18.416565\n",
            "resetting env. episode 952.000000, reward total was -19.000000. running mean: -18.422399\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -18.438175\n",
            "resetting env. episode 954.000000, reward total was -19.000000. running mean: -18.443794\n",
            "resetting env. episode 955.000000, reward total was -18.000000. running mean: -18.439356\n",
            "resetting env. episode 956.000000, reward total was -15.000000. running mean: -18.404962\n",
            "resetting env. episode 957.000000, reward total was -16.000000. running mean: -18.380912\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -18.397103\n",
            "resetting env. episode 959.000000, reward total was -16.000000. running mean: -18.373132\n",
            "resetting env. episode 960.000000, reward total was -19.000000. running mean: -18.379401\n",
            "resetting env. episode 961.000000, reward total was -16.000000. running mean: -18.355607\n",
            "resetting env. episode 962.000000, reward total was -18.000000. running mean: -18.352051\n",
            "resetting env. episode 963.000000, reward total was -17.000000. running mean: -18.338530\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -18.345145\n",
            "resetting env. episode 965.000000, reward total was -17.000000. running mean: -18.331694\n",
            "resetting env. episode 966.000000, reward total was -18.000000. running mean: -18.328377\n",
            "resetting env. episode 967.000000, reward total was -13.000000. running mean: -18.275093\n",
            "resetting env. episode 968.000000, reward total was -16.000000. running mean: -18.252342\n",
            "resetting env. episode 969.000000, reward total was -14.000000. running mean: -18.209819\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -18.237720\n",
            "resetting env. episode 971.000000, reward total was -16.000000. running mean: -18.215343\n",
            "resetting env. episode 972.000000, reward total was -16.000000. running mean: -18.193190\n",
            "resetting env. episode 973.000000, reward total was -17.000000. running mean: -18.181258\n",
            "resetting env. episode 974.000000, reward total was -19.000000. running mean: -18.189445\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -18.207551\n",
            "resetting env. episode 976.000000, reward total was -17.000000. running mean: -18.195475\n",
            "resetting env. episode 977.000000, reward total was -17.000000. running mean: -18.183521\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -18.211685\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -18.229568\n",
            "resetting env. episode 980.000000, reward total was -19.000000. running mean: -18.237273\n",
            "resetting env. episode 981.000000, reward total was -17.000000. running mean: -18.224900\n",
            "resetting env. episode 982.000000, reward total was -19.000000. running mean: -18.232651\n",
            "resetting env. episode 983.000000, reward total was -13.000000. running mean: -18.180325\n",
            "resetting env. episode 984.000000, reward total was -16.000000. running mean: -18.158521\n",
            "resetting env. episode 985.000000, reward total was -19.000000. running mean: -18.166936\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -18.175267\n",
            "resetting env. episode 987.000000, reward total was -15.000000. running mean: -18.143514\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -18.172079\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -18.200358\n",
            "resetting env. episode 990.000000, reward total was -16.000000. running mean: -18.178355\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -18.186571\n",
            "resetting env. episode 992.000000, reward total was -19.000000. running mean: -18.194705\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -18.212758\n",
            "resetting env. episode 994.000000, reward total was -17.000000. running mean: -18.200631\n",
            "resetting env. episode 995.000000, reward total was -14.000000. running mean: -18.158624\n",
            "resetting env. episode 996.000000, reward total was -16.000000. running mean: -18.137038\n",
            "resetting env. episode 997.000000, reward total was -18.000000. running mean: -18.135668\n",
            "resetting env. episode 998.000000, reward total was -15.000000. running mean: -18.104311\n",
            "resetting env. episode 999.000000, reward total was -19.000000. running mean: -18.113268\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -18.132135\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -18.150814\n",
            "resetting env. episode 1002.000000, reward total was -19.000000. running mean: -18.159306\n",
            "resetting env. episode 1003.000000, reward total was -16.000000. running mean: -18.137713\n",
            "resetting env. episode 1004.000000, reward total was -14.000000. running mean: -18.096336\n",
            "resetting env. episode 1005.000000, reward total was -14.000000. running mean: -18.055372\n",
            "resetting env. episode 1006.000000, reward total was -16.000000. running mean: -18.034819\n",
            "resetting env. episode 1007.000000, reward total was -15.000000. running mean: -18.004470\n",
            "resetting env. episode 1008.000000, reward total was -19.000000. running mean: -18.014426\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -18.034281\n",
            "resetting env. episode 1010.000000, reward total was -16.000000. running mean: -18.013939\n",
            "resetting env. episode 1011.000000, reward total was -19.000000. running mean: -18.023799\n",
            "resetting env. episode 1012.000000, reward total was -19.000000. running mean: -18.033561\n",
            "resetting env. episode 1013.000000, reward total was -17.000000. running mean: -18.023226\n",
            "resetting env. episode 1014.000000, reward total was -19.000000. running mean: -18.032993\n",
            "resetting env. episode 1015.000000, reward total was -17.000000. running mean: -18.022663\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -18.052437\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -18.081912\n",
            "resetting env. episode 1018.000000, reward total was -18.000000. running mean: -18.081093\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -18.100282\n",
            "resetting env. episode 1020.000000, reward total was -17.000000. running mean: -18.089279\n",
            "resetting env. episode 1021.000000, reward total was -14.000000. running mean: -18.048387\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -18.067903\n",
            "resetting env. episode 1023.000000, reward total was -13.000000. running mean: -18.017224\n",
            "resetting env. episode 1024.000000, reward total was -18.000000. running mean: -18.017052\n",
            "resetting env. episode 1025.000000, reward total was -18.000000. running mean: -18.016881\n",
            "resetting env. episode 1026.000000, reward total was -17.000000. running mean: -18.006712\n",
            "resetting env. episode 1027.000000, reward total was -15.000000. running mean: -17.976645\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -17.986879\n",
            "resetting env. episode 1029.000000, reward total was -18.000000. running mean: -17.987010\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -18.017140\n",
            "resetting env. episode 1031.000000, reward total was -15.000000. running mean: -17.986968\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -18.007099\n",
            "resetting env. episode 1033.000000, reward total was -14.000000. running mean: -17.967028\n",
            "resetting env. episode 1034.000000, reward total was -19.000000. running mean: -17.977357\n",
            "resetting env. episode 1035.000000, reward total was -18.000000. running mean: -17.977584\n",
            "resetting env. episode 1036.000000, reward total was -17.000000. running mean: -17.967808\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -17.978130\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -17.998349\n",
            "resetting env. episode 1039.000000, reward total was -19.000000. running mean: -18.008365\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -18.028282\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -18.037999\n",
            "resetting env. episode 1042.000000, reward total was -18.000000. running mean: -18.037619\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -18.067243\n",
            "resetting env. episode 1044.000000, reward total was -19.000000. running mean: -18.076570\n",
            "resetting env. episode 1045.000000, reward total was -17.000000. running mean: -18.065804\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -18.075146\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -18.104395\n",
            "resetting env. episode 1048.000000, reward total was -19.000000. running mean: -18.113351\n",
            "resetting env. episode 1049.000000, reward total was -16.000000. running mean: -18.092217\n",
            "resetting env. episode 1050.000000, reward total was -17.000000. running mean: -18.081295\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -18.100482\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -18.119477\n",
            "resetting env. episode 1053.000000, reward total was -19.000000. running mean: -18.128283\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -18.157000\n",
            "resetting env. episode 1055.000000, reward total was -17.000000. running mean: -18.145430\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -18.163976\n",
            "resetting env. episode 1057.000000, reward total was -18.000000. running mean: -18.162336\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -18.190712\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -18.208805\n",
            "resetting env. episode 1060.000000, reward total was -18.000000. running mean: -18.206717\n",
            "resetting env. episode 1061.000000, reward total was -18.000000. running mean: -18.204650\n",
            "resetting env. episode 1062.000000, reward total was -17.000000. running mean: -18.192604\n",
            "resetting env. episode 1063.000000, reward total was -17.000000. running mean: -18.180678\n",
            "resetting env. episode 1064.000000, reward total was -20.000000. running mean: -18.198871\n",
            "resetting env. episode 1065.000000, reward total was -19.000000. running mean: -18.206882\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -18.224813\n",
            "resetting env. episode 1067.000000, reward total was -15.000000. running mean: -18.192565\n",
            "resetting env. episode 1068.000000, reward total was -18.000000. running mean: -18.190639\n",
            "resetting env. episode 1069.000000, reward total was -19.000000. running mean: -18.198733\n",
            "resetting env. episode 1070.000000, reward total was -19.000000. running mean: -18.206746\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -18.224678\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -18.232432\n",
            "resetting env. episode 1073.000000, reward total was -14.000000. running mean: -18.190107\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -18.208206\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -18.236124\n",
            "resetting env. episode 1076.000000, reward total was -16.000000. running mean: -18.213763\n",
            "resetting env. episode 1077.000000, reward total was -20.000000. running mean: -18.231625\n",
            "resetting env. episode 1078.000000, reward total was -17.000000. running mean: -18.219309\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -18.237116\n",
            "resetting env. episode 1080.000000, reward total was -18.000000. running mean: -18.234745\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -18.262397\n",
            "resetting env. episode 1082.000000, reward total was -19.000000. running mean: -18.269773\n",
            "resetting env. episode 1083.000000, reward total was -14.000000. running mean: -18.227076\n",
            "resetting env. episode 1084.000000, reward total was -17.000000. running mean: -18.214805\n",
            "resetting env. episode 1085.000000, reward total was -18.000000. running mean: -18.212657\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -18.240530\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -18.268125\n",
            "resetting env. episode 1088.000000, reward total was -17.000000. running mean: -18.255444\n",
            "resetting env. episode 1089.000000, reward total was -17.000000. running mean: -18.242889\n",
            "resetting env. episode 1090.000000, reward total was -19.000000. running mean: -18.250460\n",
            "resetting env. episode 1091.000000, reward total was -19.000000. running mean: -18.257956\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -18.265376\n",
            "resetting env. episode 1093.000000, reward total was -19.000000. running mean: -18.272722\n",
            "resetting env. episode 1094.000000, reward total was -18.000000. running mean: -18.269995\n",
            "resetting env. episode 1095.000000, reward total was -18.000000. running mean: -18.267295\n",
            "resetting env. episode 1096.000000, reward total was -18.000000. running mean: -18.264622\n",
            "resetting env. episode 1097.000000, reward total was -19.000000. running mean: -18.271976\n",
            "resetting env. episode 1098.000000, reward total was -19.000000. running mean: -18.279256\n",
            "resetting env. episode 1099.000000, reward total was -17.000000. running mean: -18.266464\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -18.283799\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -18.290961\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -18.288051\n",
            "resetting env. episode 1103.000000, reward total was -18.000000. running mean: -18.285171\n",
            "resetting env. episode 1104.000000, reward total was -19.000000. running mean: -18.292319\n",
            "resetting env. episode 1105.000000, reward total was -18.000000. running mean: -18.289396\n",
            "resetting env. episode 1106.000000, reward total was -17.000000. running mean: -18.276502\n",
            "resetting env. episode 1107.000000, reward total was -19.000000. running mean: -18.283737\n",
            "resetting env. episode 1108.000000, reward total was -17.000000. running mean: -18.270900\n",
            "resetting env. episode 1109.000000, reward total was -16.000000. running mean: -18.248191\n",
            "resetting env. episode 1110.000000, reward total was -19.000000. running mean: -18.255709\n",
            "resetting env. episode 1111.000000, reward total was -19.000000. running mean: -18.263152\n",
            "resetting env. episode 1112.000000, reward total was -15.000000. running mean: -18.230520\n",
            "resetting env. episode 1113.000000, reward total was -18.000000. running mean: -18.228215\n",
            "resetting env. episode 1114.000000, reward total was -17.000000. running mean: -18.215933\n",
            "resetting env. episode 1115.000000, reward total was -15.000000. running mean: -18.183774\n",
            "resetting env. episode 1116.000000, reward total was -18.000000. running mean: -18.181936\n",
            "resetting env. episode 1117.000000, reward total was -17.000000. running mean: -18.170116\n",
            "resetting env. episode 1118.000000, reward total was -14.000000. running mean: -18.128415\n",
            "resetting env. episode 1119.000000, reward total was -18.000000. running mean: -18.127131\n",
            "resetting env. episode 1120.000000, reward total was -19.000000. running mean: -18.135860\n",
            "resetting env. episode 1121.000000, reward total was -14.000000. running mean: -18.094501\n",
            "resetting env. episode 1122.000000, reward total was -19.000000. running mean: -18.103556\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -18.132521\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -18.151195\n",
            "resetting env. episode 1125.000000, reward total was -16.000000. running mean: -18.129683\n",
            "resetting env. episode 1126.000000, reward total was -18.000000. running mean: -18.128387\n",
            "resetting env. episode 1127.000000, reward total was -18.000000. running mean: -18.127103\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -18.135832\n",
            "resetting env. episode 1129.000000, reward total was -15.000000. running mean: -18.104473\n",
            "resetting env. episode 1130.000000, reward total was -17.000000. running mean: -18.093429\n",
            "resetting env. episode 1131.000000, reward total was -14.000000. running mean: -18.052494\n",
            "resetting env. episode 1132.000000, reward total was -17.000000. running mean: -18.041969\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -18.061550\n",
            "resetting env. episode 1134.000000, reward total was -18.000000. running mean: -18.060934\n",
            "resetting env. episode 1135.000000, reward total was -16.000000. running mean: -18.040325\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -18.049922\n",
            "resetting env. episode 1137.000000, reward total was -17.000000. running mean: -18.039422\n",
            "resetting env. episode 1138.000000, reward total was -15.000000. running mean: -18.009028\n",
            "resetting env. episode 1139.000000, reward total was -19.000000. running mean: -18.018938\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -18.038749\n",
            "resetting env. episode 1141.000000, reward total was -18.000000. running mean: -18.038361\n",
            "resetting env. episode 1142.000000, reward total was -18.000000. running mean: -18.037977\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -18.057598\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -18.087022\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -18.116151\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -18.134990\n",
            "resetting env. episode 1147.000000, reward total was -19.000000. running mean: -18.143640\n",
            "resetting env. episode 1148.000000, reward total was -18.000000. running mean: -18.142204\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -18.170782\n",
            "resetting env. episode 1150.000000, reward total was -19.000000. running mean: -18.179074\n",
            "resetting env. episode 1151.000000, reward total was -16.000000. running mean: -18.157283\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -18.185710\n",
            "resetting env. episode 1153.000000, reward total was -17.000000. running mean: -18.173853\n",
            "resetting env. episode 1154.000000, reward total was -16.000000. running mean: -18.152115\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -18.180593\n",
            "resetting env. episode 1156.000000, reward total was -19.000000. running mean: -18.188788\n",
            "resetting env. episode 1157.000000, reward total was -14.000000. running mean: -18.146900\n",
            "resetting env. episode 1158.000000, reward total was -20.000000. running mean: -18.165431\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -18.183776\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -18.201939\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -18.209919\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -18.217820\n",
            "resetting env. episode 1163.000000, reward total was -19.000000. running mean: -18.225642\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -18.233385\n",
            "resetting env. episode 1165.000000, reward total was -19.000000. running mean: -18.241052\n",
            "resetting env. episode 1166.000000, reward total was -15.000000. running mean: -18.208641\n",
            "resetting env. episode 1167.000000, reward total was -17.000000. running mean: -18.196555\n",
            "resetting env. episode 1168.000000, reward total was -17.000000. running mean: -18.184589\n",
            "resetting env. episode 1169.000000, reward total was -17.000000. running mean: -18.172743\n",
            "resetting env. episode 1170.000000, reward total was -20.000000. running mean: -18.191016\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -18.219106\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -18.236915\n",
            "resetting env. episode 1173.000000, reward total was -19.000000. running mean: -18.244545\n",
            "resetting env. episode 1174.000000, reward total was -17.000000. running mean: -18.232100\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -18.259779\n",
            "resetting env. episode 1176.000000, reward total was -16.000000. running mean: -18.237181\n",
            "resetting env. episode 1177.000000, reward total was -18.000000. running mean: -18.234809\n",
            "resetting env. episode 1178.000000, reward total was -18.000000. running mean: -18.232461\n",
            "resetting env. episode 1179.000000, reward total was -19.000000. running mean: -18.240137\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -18.247735\n",
            "resetting env. episode 1181.000000, reward total was -15.000000. running mean: -18.215258\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -18.233105\n",
            "resetting env. episode 1183.000000, reward total was -16.000000. running mean: -18.210774\n",
            "resetting env. episode 1184.000000, reward total was -19.000000. running mean: -18.218667\n",
            "resetting env. episode 1185.000000, reward total was -17.000000. running mean: -18.206480\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -18.224415\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -18.252171\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -18.279649\n",
            "resetting env. episode 1189.000000, reward total was -19.000000. running mean: -18.286853\n",
            "resetting env. episode 1190.000000, reward total was -18.000000. running mean: -18.283984\n",
            "resetting env. episode 1191.000000, reward total was -17.000000. running mean: -18.271144\n",
            "resetting env. episode 1192.000000, reward total was -18.000000. running mean: -18.268433\n",
            "resetting env. episode 1193.000000, reward total was -19.000000. running mean: -18.275749\n",
            "resetting env. episode 1194.000000, reward total was -19.000000. running mean: -18.282991\n",
            "resetting env. episode 1195.000000, reward total was -18.000000. running mean: -18.280161\n",
            "resetting env. episode 1196.000000, reward total was -17.000000. running mean: -18.267360\n",
            "resetting env. episode 1197.000000, reward total was -15.000000. running mean: -18.234686\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -18.252339\n",
            "resetting env. episode 1199.000000, reward total was -13.000000. running mean: -18.199816\n",
            "resetting env. episode 1200.000000, reward total was -18.000000. running mean: -18.197818\n",
            "resetting env. episode 1201.000000, reward total was -18.000000. running mean: -18.195839\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -18.223881\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -18.241642\n",
            "resetting env. episode 1204.000000, reward total was -17.000000. running mean: -18.229226\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -18.246934\n",
            "resetting env. episode 1206.000000, reward total was -19.000000. running mean: -18.254464\n",
            "resetting env. episode 1207.000000, reward total was -18.000000. running mean: -18.251920\n",
            "resetting env. episode 1208.000000, reward total was -17.000000. running mean: -18.239400\n",
            "resetting env. episode 1209.000000, reward total was -16.000000. running mean: -18.217006\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -18.244836\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -18.262388\n",
            "resetting env. episode 1212.000000, reward total was -18.000000. running mean: -18.259764\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -18.267166\n",
            "resetting env. episode 1214.000000, reward total was -18.000000. running mean: -18.264495\n",
            "resetting env. episode 1215.000000, reward total was -17.000000. running mean: -18.251850\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -18.279331\n",
            "resetting env. episode 1217.000000, reward total was -17.000000. running mean: -18.266538\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -18.283873\n",
            "resetting env. episode 1219.000000, reward total was -18.000000. running mean: -18.281034\n",
            "resetting env. episode 1220.000000, reward total was -16.000000. running mean: -18.258224\n",
            "resetting env. episode 1221.000000, reward total was -19.000000. running mean: -18.265641\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -18.272985\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -18.280255\n",
            "resetting env. episode 1224.000000, reward total was -18.000000. running mean: -18.277452\n",
            "resetting env. episode 1225.000000, reward total was -19.000000. running mean: -18.284678\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -18.301831\n",
            "resetting env. episode 1227.000000, reward total was -15.000000. running mean: -18.268813\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -18.286125\n",
            "resetting env. episode 1229.000000, reward total was -18.000000. running mean: -18.283263\n",
            "resetting env. episode 1230.000000, reward total was -18.000000. running mean: -18.280431\n",
            "resetting env. episode 1231.000000, reward total was -19.000000. running mean: -18.287627\n",
            "resetting env. episode 1232.000000, reward total was -17.000000. running mean: -18.274750\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -18.292003\n",
            "resetting env. episode 1234.000000, reward total was -15.000000. running mean: -18.259083\n",
            "resetting env. episode 1235.000000, reward total was -20.000000. running mean: -18.276492\n",
            "resetting env. episode 1236.000000, reward total was -17.000000. running mean: -18.263727\n",
            "resetting env. episode 1237.000000, reward total was -18.000000. running mean: -18.261090\n",
            "resetting env. episode 1238.000000, reward total was -18.000000. running mean: -18.258479\n",
            "resetting env. episode 1239.000000, reward total was -18.000000. running mean: -18.255894\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -18.273335\n",
            "resetting env. episode 1241.000000, reward total was -17.000000. running mean: -18.260602\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -18.267996\n",
            "resetting env. episode 1243.000000, reward total was -17.000000. running mean: -18.255316\n",
            "resetting env. episode 1244.000000, reward total was -19.000000. running mean: -18.262763\n",
            "resetting env. episode 1245.000000, reward total was -19.000000. running mean: -18.270135\n",
            "resetting env. episode 1246.000000, reward total was -19.000000. running mean: -18.277434\n",
            "resetting env. episode 1247.000000, reward total was -16.000000. running mean: -18.254659\n",
            "resetting env. episode 1248.000000, reward total was -15.000000. running mean: -18.222113\n",
            "resetting env. episode 1249.000000, reward total was -18.000000. running mean: -18.219892\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -18.227693\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -18.255416\n",
            "resetting env. episode 1252.000000, reward total was -17.000000. running mean: -18.242862\n",
            "resetting env. episode 1253.000000, reward total was -17.000000. running mean: -18.230433\n",
            "resetting env. episode 1254.000000, reward total was -18.000000. running mean: -18.228129\n",
            "resetting env. episode 1255.000000, reward total was -17.000000. running mean: -18.215847\n",
            "resetting env. episode 1256.000000, reward total was -18.000000. running mean: -18.213689\n",
            "resetting env. episode 1257.000000, reward total was -19.000000. running mean: -18.221552\n",
            "resetting env. episode 1258.000000, reward total was -18.000000. running mean: -18.219336\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -18.247143\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -18.264672\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -18.282025\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -18.289205\n",
            "resetting env. episode 1263.000000, reward total was -20.000000. running mean: -18.306313\n",
            "resetting env. episode 1264.000000, reward total was -17.000000. running mean: -18.293250\n",
            "resetting env. episode 1265.000000, reward total was -17.000000. running mean: -18.280317\n",
            "resetting env. episode 1266.000000, reward total was -20.000000. running mean: -18.297514\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -18.314539\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -18.341393\n",
            "resetting env. episode 1269.000000, reward total was -19.000000. running mean: -18.347979\n",
            "resetting env. episode 1270.000000, reward total was -17.000000. running mean: -18.334500\n",
            "resetting env. episode 1271.000000, reward total was -18.000000. running mean: -18.331155\n",
            "resetting env. episode 1272.000000, reward total was -16.000000. running mean: -18.307843\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -18.324765\n",
            "resetting env. episode 1274.000000, reward total was -18.000000. running mean: -18.321517\n",
            "resetting env. episode 1275.000000, reward total was -18.000000. running mean: -18.318302\n",
            "resetting env. episode 1276.000000, reward total was -19.000000. running mean: -18.325119\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -18.341868\n",
            "resetting env. episode 1278.000000, reward total was -19.000000. running mean: -18.348449\n",
            "resetting env. episode 1279.000000, reward total was -17.000000. running mean: -18.334964\n",
            "resetting env. episode 1280.000000, reward total was -17.000000. running mean: -18.321615\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -18.328399\n",
            "resetting env. episode 1282.000000, reward total was -19.000000. running mean: -18.335115\n",
            "resetting env. episode 1283.000000, reward total was -15.000000. running mean: -18.301764\n",
            "resetting env. episode 1284.000000, reward total was -19.000000. running mean: -18.308746\n",
            "resetting env. episode 1285.000000, reward total was -18.000000. running mean: -18.305658\n",
            "resetting env. episode 1286.000000, reward total was -17.000000. running mean: -18.292602\n",
            "resetting env. episode 1287.000000, reward total was -18.000000. running mean: -18.289676\n",
            "resetting env. episode 1288.000000, reward total was -17.000000. running mean: -18.276779\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -18.294011\n",
            "resetting env. episode 1290.000000, reward total was -19.000000. running mean: -18.301071\n",
            "resetting env. episode 1291.000000, reward total was -18.000000. running mean: -18.298060\n",
            "resetting env. episode 1292.000000, reward total was -17.000000. running mean: -18.285080\n",
            "resetting env. episode 1293.000000, reward total was -19.000000. running mean: -18.292229\n",
            "resetting env. episode 1294.000000, reward total was -18.000000. running mean: -18.289307\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -18.306414\n",
            "resetting env. episode 1296.000000, reward total was -19.000000. running mean: -18.313350\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -18.330216\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -18.346914\n",
            "resetting env. episode 1299.000000, reward total was -15.000000. running mean: -18.313445\n",
            "resetting env. episode 1300.000000, reward total was -17.000000. running mean: -18.300310\n",
            "resetting env. episode 1301.000000, reward total was -17.000000. running mean: -18.287307\n",
            "resetting env. episode 1302.000000, reward total was -15.000000. running mean: -18.254434\n",
            "resetting env. episode 1303.000000, reward total was -18.000000. running mean: -18.251890\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -18.279371\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -18.296577\n",
            "resetting env. episode 1306.000000, reward total was -15.000000. running mean: -18.263611\n",
            "resetting env. episode 1307.000000, reward total was -18.000000. running mean: -18.260975\n",
            "resetting env. episode 1308.000000, reward total was -18.000000. running mean: -18.258366\n",
            "resetting env. episode 1309.000000, reward total was -18.000000. running mean: -18.255782\n",
            "resetting env. episode 1310.000000, reward total was -15.000000. running mean: -18.223224\n",
            "resetting env. episode 1311.000000, reward total was -16.000000. running mean: -18.200992\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -18.218982\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -18.226792\n",
            "resetting env. episode 1314.000000, reward total was -20.000000. running mean: -18.244524\n",
            "resetting env. episode 1315.000000, reward total was -19.000000. running mean: -18.252079\n",
            "resetting env. episode 1316.000000, reward total was -16.000000. running mean: -18.229558\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -18.247263\n",
            "resetting env. episode 1318.000000, reward total was -18.000000. running mean: -18.244790\n",
            "resetting env. episode 1319.000000, reward total was -19.000000. running mean: -18.252342\n",
            "resetting env. episode 1320.000000, reward total was -17.000000. running mean: -18.239819\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -18.257420\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -18.274846\n",
            "resetting env. episode 1323.000000, reward total was -19.000000. running mean: -18.282098\n",
            "resetting env. episode 1324.000000, reward total was -16.000000. running mean: -18.259277\n",
            "resetting env. episode 1325.000000, reward total was -19.000000. running mean: -18.266684\n",
            "resetting env. episode 1326.000000, reward total was -18.000000. running mean: -18.264017\n",
            "resetting env. episode 1327.000000, reward total was -18.000000. running mean: -18.261377\n",
            "resetting env. episode 1328.000000, reward total was -18.000000. running mean: -18.258763\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -18.276176\n",
            "resetting env. episode 1330.000000, reward total was -16.000000. running mean: -18.253414\n",
            "resetting env. episode 1331.000000, reward total was -18.000000. running mean: -18.250880\n",
            "resetting env. episode 1332.000000, reward total was -19.000000. running mean: -18.258371\n",
            "resetting env. episode 1333.000000, reward total was -19.000000. running mean: -18.265787\n",
            "resetting env. episode 1334.000000, reward total was -19.000000. running mean: -18.273129\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -18.290398\n",
            "resetting env. episode 1336.000000, reward total was -19.000000. running mean: -18.297494\n",
            "resetting env. episode 1337.000000, reward total was -19.000000. running mean: -18.304519\n",
            "resetting env. episode 1338.000000, reward total was -17.000000. running mean: -18.291474\n",
            "resetting env. episode 1339.000000, reward total was -18.000000. running mean: -18.288559\n",
            "resetting env. episode 1340.000000, reward total was -19.000000. running mean: -18.295674\n",
            "resetting env. episode 1341.000000, reward total was -18.000000. running mean: -18.292717\n",
            "resetting env. episode 1342.000000, reward total was -19.000000. running mean: -18.299790\n",
            "resetting env. episode 1343.000000, reward total was -18.000000. running mean: -18.296792\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -18.313824\n",
            "resetting env. episode 1345.000000, reward total was -17.000000. running mean: -18.300686\n",
            "resetting env. episode 1346.000000, reward total was -19.000000. running mean: -18.307679\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -18.334602\n",
            "resetting env. episode 1348.000000, reward total was -17.000000. running mean: -18.321256\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -18.338043\n",
            "resetting env. episode 1350.000000, reward total was -17.000000. running mean: -18.324663\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -18.341416\n",
            "resetting env. episode 1352.000000, reward total was -17.000000. running mean: -18.328002\n",
            "resetting env. episode 1353.000000, reward total was -19.000000. running mean: -18.334722\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -18.351375\n",
            "resetting env. episode 1355.000000, reward total was -17.000000. running mean: -18.337861\n",
            "resetting env. episode 1356.000000, reward total was -19.000000. running mean: -18.344483\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -18.361038\n",
            "resetting env. episode 1358.000000, reward total was -19.000000. running mean: -18.367427\n",
            "resetting env. episode 1359.000000, reward total was -19.000000. running mean: -18.373753\n",
            "resetting env. episode 1360.000000, reward total was -16.000000. running mean: -18.350016\n",
            "resetting env. episode 1361.000000, reward total was -11.000000. running mean: -18.276515\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -18.293750\n",
            "resetting env. episode 1363.000000, reward total was -19.000000. running mean: -18.300813\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -18.307805\n",
            "resetting env. episode 1365.000000, reward total was -18.000000. running mean: -18.304727\n",
            "resetting env. episode 1366.000000, reward total was -18.000000. running mean: -18.301679\n",
            "resetting env. episode 1367.000000, reward total was -18.000000. running mean: -18.298663\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -18.315676\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -18.332519\n",
            "resetting env. episode 1370.000000, reward total was -18.000000. running mean: -18.329194\n",
            "resetting env. episode 1371.000000, reward total was -15.000000. running mean: -18.295902\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -18.322943\n",
            "resetting env. episode 1373.000000, reward total was -16.000000. running mean: -18.299714\n",
            "resetting env. episode 1374.000000, reward total was -19.000000. running mean: -18.306716\n",
            "resetting env. episode 1375.000000, reward total was -17.000000. running mean: -18.293649\n",
            "resetting env. episode 1376.000000, reward total was -18.000000. running mean: -18.290713\n",
            "resetting env. episode 1377.000000, reward total was -17.000000. running mean: -18.277806\n",
            "resetting env. episode 1378.000000, reward total was -18.000000. running mean: -18.275028\n",
            "resetting env. episode 1379.000000, reward total was -18.000000. running mean: -18.272277\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -18.279555\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -18.286759\n",
            "resetting env. episode 1382.000000, reward total was -14.000000. running mean: -18.243891\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -18.251452\n",
            "resetting env. episode 1384.000000, reward total was -18.000000. running mean: -18.248938\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -18.266449\n",
            "resetting env. episode 1386.000000, reward total was -17.000000. running mean: -18.253784\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -18.281246\n",
            "resetting env. episode 1388.000000, reward total was -19.000000. running mean: -18.288434\n",
            "resetting env. episode 1389.000000, reward total was -18.000000. running mean: -18.285549\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -18.302694\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -18.319667\n",
            "resetting env. episode 1392.000000, reward total was -18.000000. running mean: -18.316470\n",
            "resetting env. episode 1393.000000, reward total was -16.000000. running mean: -18.293306\n",
            "resetting env. episode 1394.000000, reward total was -18.000000. running mean: -18.290373\n",
            "resetting env. episode 1395.000000, reward total was -18.000000. running mean: -18.287469\n",
            "resetting env. episode 1396.000000, reward total was -17.000000. running mean: -18.274594\n",
            "resetting env. episode 1397.000000, reward total was -17.000000. running mean: -18.261848\n",
            "resetting env. episode 1398.000000, reward total was -19.000000. running mean: -18.269230\n",
            "resetting env. episode 1399.000000, reward total was -17.000000. running mean: -18.256537\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -18.273972\n",
            "resetting env. episode 1401.000000, reward total was -18.000000. running mean: -18.271232\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -18.298520\n",
            "resetting env. episode 1403.000000, reward total was -17.000000. running mean: -18.285535\n",
            "resetting env. episode 1404.000000, reward total was -17.000000. running mean: -18.272679\n",
            "resetting env. episode 1405.000000, reward total was -18.000000. running mean: -18.269953\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -18.287253\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -18.314381\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -18.341237\n",
            "resetting env. episode 1409.000000, reward total was -19.000000. running mean: -18.347824\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -18.364346\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -18.390703\n",
            "resetting env. episode 1412.000000, reward total was -17.000000. running mean: -18.376796\n",
            "resetting env. episode 1413.000000, reward total was -18.000000. running mean: -18.373028\n",
            "resetting env. episode 1414.000000, reward total was -14.000000. running mean: -18.329297\n",
            "resetting env. episode 1415.000000, reward total was -16.000000. running mean: -18.306005\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -18.332944\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -18.349615\n",
            "resetting env. episode 1418.000000, reward total was -12.000000. running mean: -18.286119\n",
            "resetting env. episode 1419.000000, reward total was -18.000000. running mean: -18.283258\n",
            "resetting env. episode 1420.000000, reward total was -17.000000. running mean: -18.270425\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -18.287721\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -18.314844\n",
            "resetting env. episode 1423.000000, reward total was -13.000000. running mean: -18.261695\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -18.279078\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -18.296287\n",
            "resetting env. episode 1426.000000, reward total was -17.000000. running mean: -18.283325\n",
            "resetting env. episode 1427.000000, reward total was -18.000000. running mean: -18.280491\n",
            "resetting env. episode 1428.000000, reward total was -13.000000. running mean: -18.227686\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -18.255410\n",
            "resetting env. episode 1430.000000, reward total was -18.000000. running mean: -18.252855\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -18.280327\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -18.307524\n",
            "resetting env. episode 1433.000000, reward total was -19.000000. running mean: -18.314448\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -18.341304\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -18.367891\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -18.394212\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -18.410270\n",
            "resetting env. episode 1438.000000, reward total was -20.000000. running mean: -18.426167\n",
            "resetting env. episode 1439.000000, reward total was -14.000000. running mean: -18.381906\n",
            "resetting env. episode 1440.000000, reward total was -17.000000. running mean: -18.368086\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -18.384406\n",
            "resetting env. episode 1442.000000, reward total was -19.000000. running mean: -18.390562\n",
            "resetting env. episode 1443.000000, reward total was -11.000000. running mean: -18.316656\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -18.323489\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -18.350254\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -18.376752\n",
            "resetting env. episode 1447.000000, reward total was -19.000000. running mean: -18.382984\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -18.389155\n",
            "resetting env. episode 1449.000000, reward total was -18.000000. running mean: -18.385263\n",
            "resetting env. episode 1450.000000, reward total was -18.000000. running mean: -18.381410\n",
            "resetting env. episode 1451.000000, reward total was -18.000000. running mean: -18.377596\n",
            "resetting env. episode 1452.000000, reward total was -16.000000. running mean: -18.353820\n",
            "resetting env. episode 1453.000000, reward total was -17.000000. running mean: -18.340282\n",
            "resetting env. episode 1454.000000, reward total was -15.000000. running mean: -18.306879\n",
            "resetting env. episode 1455.000000, reward total was -19.000000. running mean: -18.313810\n",
            "resetting env. episode 1456.000000, reward total was -16.000000. running mean: -18.290672\n",
            "resetting env. episode 1457.000000, reward total was -17.000000. running mean: -18.277766\n",
            "resetting env. episode 1458.000000, reward total was -14.000000. running mean: -18.234988\n",
            "resetting env. episode 1459.000000, reward total was -14.000000. running mean: -18.192638\n",
            "resetting env. episode 1460.000000, reward total was -17.000000. running mean: -18.180712\n",
            "resetting env. episode 1461.000000, reward total was -18.000000. running mean: -18.178905\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -18.207116\n",
            "resetting env. episode 1463.000000, reward total was -19.000000. running mean: -18.215044\n",
            "resetting env. episode 1464.000000, reward total was -17.000000. running mean: -18.202894\n",
            "resetting env. episode 1465.000000, reward total was -18.000000. running mean: -18.200865\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -18.228856\n",
            "resetting env. episode 1467.000000, reward total was -18.000000. running mean: -18.226568\n",
            "resetting env. episode 1468.000000, reward total was -19.000000. running mean: -18.234302\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -18.241959\n",
            "resetting env. episode 1470.000000, reward total was -17.000000. running mean: -18.229540\n",
            "resetting env. episode 1471.000000, reward total was -17.000000. running mean: -18.217244\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -18.235072\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -18.252721\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -18.270194\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -18.287492\n",
            "resetting env. episode 1476.000000, reward total was -16.000000. running mean: -18.264617\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -18.291971\n",
            "resetting env. episode 1478.000000, reward total was -20.000000. running mean: -18.309051\n",
            "resetting env. episode 1479.000000, reward total was -17.000000. running mean: -18.295961\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -18.313001\n",
            "resetting env. episode 1481.000000, reward total was -19.000000. running mean: -18.319871\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -18.346672\n",
            "resetting env. episode 1483.000000, reward total was -17.000000. running mean: -18.333205\n",
            "resetting env. episode 1484.000000, reward total was -15.000000. running mean: -18.299873\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -18.326875\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -18.353606\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -18.380070\n",
            "resetting env. episode 1488.000000, reward total was -15.000000. running mean: -18.346269\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -18.362806\n",
            "resetting env. episode 1490.000000, reward total was -19.000000. running mean: -18.369178\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -18.395487\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -18.421532\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -18.447316\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -18.472843\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -18.498115\n",
            "resetting env. episode 1496.000000, reward total was -19.000000. running mean: -18.503134\n",
            "resetting env. episode 1497.000000, reward total was -19.000000. running mean: -18.508102\n",
            "resetting env. episode 1498.000000, reward total was -17.000000. running mean: -18.493021\n",
            "resetting env. episode 1499.000000, reward total was -16.000000. running mean: -18.468091\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -18.473410\n",
            "CPU times: user 1h 39min 16s, sys: 45min 24s, total: 2h 24min 41s\n",
            "Wall time: 1h 14min 40s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "w2NblmwDsL3y",
        "outputId": "7d4a638c-14b9-49e8-82b0-f36e5b5fe314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "H=200_le_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}