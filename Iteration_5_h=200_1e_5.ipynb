{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "e3677806-061a-47c4-f986-6fd37dcbaabc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 12.1 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=b362bf69c008ed7b73c1e9a7c369db8bbff850da07a8fdca226045cec276200d\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "c70a1a9b-3a3e-486a-a896-88d2a58d91c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "e78b6a42-901b-4629-8397-308ac93836bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "1c4802aa-ed3e-4c83-cccb-2e242f8e009c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "af8b8237-16d9-42d0-c320-742246840e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -15.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-5\n",
        "learning_rate = 1e-5\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "76013d88-8387-482f-c75e-f51f196c954a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -19.999900\n",
            "resetting env. episode 4.000000, reward total was -19.000000. running mean: -19.989901\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.000002\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.000002\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.010002\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.019902\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.029703\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.039406\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.049012\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.048522\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.058036\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.067456\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.056782\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.056214\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.065652\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.074995\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.084245\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.093403\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.102469\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.111444\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.120330\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.129126\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.137835\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.136457\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.135092\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.143741\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.152304\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.150781\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.159273\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.167680\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.166003\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.174343\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.182600\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.190774\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.188866\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.176977\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.185208\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.193356\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.201422\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.199408\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.207414\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.215340\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.223186\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.230954\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.238645\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.246258\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.243796\n",
            "resetting env. episode 50.000000, reward total was -18.000000. running mean: -20.221358\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.229144\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.236853\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.244484\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.252039\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -20.229519\n",
            "resetting env. episode 56.000000, reward total was -18.000000. running mean: -20.207224\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.215152\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.213000\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.210870\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.218761\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.216574\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.224408\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.222164\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.229942\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.237643\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.235266\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.242914\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.230485\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.228180\n",
            "resetting env. episode 70.000000, reward total was -18.000000. running mean: -20.205898\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.213839\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.221701\n",
            "resetting env. episode 73.000000, reward total was -18.000000. running mean: -20.199484\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.207489\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.205414\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.213360\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.221226\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.229014\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.236724\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.244357\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.251913\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.259394\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.266800\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.254132\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.261591\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.268975\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.266285\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.273622\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.280886\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.278077\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.265296\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.262643\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.260017\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.267417\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.274743\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.281995\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.289175\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.296283\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.303321\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.310287\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.317184\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.324013\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.320773\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.327565\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.334289\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.340946\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.337537\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.344161\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.350720\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.357213\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.363640\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.360004\n",
            "resetting env. episode 113.000000, reward total was -19.000000. running mean: -20.346404\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.352940\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.359411\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.365816\n",
            "resetting env. episode 117.000000, reward total was -18.000000. running mean: -20.342158\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.348737\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.355249\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.361697\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.348080\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.354599\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.351053\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.357543\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.363967\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.360327\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.366724\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.373057\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.379326\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.385533\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.391678\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.397761\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.403783\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.409746\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.415648\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.421492\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.427277\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.423004\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.428774\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.434486\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.430141\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.425840\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.421582\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.427366\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.413092\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.408961\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.394872\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.400923\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -20.386914\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.383044\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.389214\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.385322\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.391469\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.397554\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.403578\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.399543\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.405547\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.411492\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.417377\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.423203\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -20.398971\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.394981\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.391031\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.397121\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.403150\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.409118\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.395027\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.401077\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.407066\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.392996\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.399066\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.395075\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.391124\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.397213\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -20.383241\n",
            "resetting env. episode 176.000000, reward total was -17.000000. running mean: -20.349408\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.345914\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -20.332455\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.339131\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.345739\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.352282\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.358759\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.365172\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.371520\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.367805\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.374127\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.380385\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.376581\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.382816\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.378987\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.385198\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.391346\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.397432\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.393458\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.399523\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.405528\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.411473\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.407358\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.403284\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.409252\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.405159\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.411108\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.416996\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.412826\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.418698\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.404511\n",
            "resetting env. episode 207.000000, reward total was -17.000000. running mean: -20.370466\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.366761\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.373094\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.359363\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.365769\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.372112\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -20.348390\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.354907\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.361357\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.367744\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.364066\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.370426\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.376722\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.382954\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.389125\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.385234\n",
            "resetting env. episode 223.000000, reward total was -18.000000. running mean: -20.361381\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.367767\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.374090\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.360349\n",
            "resetting env. episode 227.000000, reward total was -18.000000. running mean: -20.336745\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.333378\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.330044\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.326744\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.333476\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.340141\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.336740\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.343373\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.349939\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.336440\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.343075\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.349644\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.356148\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.352586\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.359061\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.365470\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -20.351815\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.358297\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.354714\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.361167\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.357555\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.363980\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.360340\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.356737\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.353169\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.359638\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.356041\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.352481\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.348956\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.355466\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.351912\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.348393\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.354909\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.361360\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.367746\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.374069\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.380328\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.386525\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.392659\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.378733\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.364945\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.371296\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.377583\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.383807\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.389969\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.396069\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.392109\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.398188\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.404206\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.410164\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.406062\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.412001\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.417881\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.423703\n",
            "resetting env. episode 281.000000, reward total was -18.000000. running mean: -20.399466\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.405471\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.411416\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.407302\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.413229\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.409097\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.405006\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.410956\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.416846\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.412678\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.408551\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.414465\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.420321\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.426118\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.431856\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.427538\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.413262\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.399130\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.405139\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.411087\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.416976\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.412806\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.418678\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.424492\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.430247\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.435944\n",
            "resetting env. episode 307.000000, reward total was -17.000000. running mean: -20.401585\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.407569\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.403493\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.409458\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.405364\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.411310\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.417197\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.423025\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.428795\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.434507\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.440162\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.445760\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.451303\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.456790\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.452222\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.437699\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.443322\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.448889\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.454400\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.449856\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.445358\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.450904\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.456395\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.461831\n",
            "resetting env. episode 331.000000, reward total was -19.000000. running mean: -20.447213\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.452741\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.448213\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.453731\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.459194\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.464602\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.469956\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.475256\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.460504\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.455899\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.461340\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.466726\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.462059\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.467439\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.472764\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.458036\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.453456\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.448922\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.454432\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.449888\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.445389\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.450935\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.456426\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.451862\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.457343\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.462770\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.468142\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.473460\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.478726\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.483939\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.489099\n",
            "resetting env. episode 362.000000, reward total was -18.000000. running mean: -20.464208\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.469566\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.474871\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.480122\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.465321\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.470667\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.475961\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.471201\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.476489\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.481724\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.486907\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.492038\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.497118\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.502146\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.507125\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.512054\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.516933\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.521764\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.526546\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.511281\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.516168\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.521006\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.515796\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.500638\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.505632\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.500575\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.495570\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.500614\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.505608\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.510552\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.495446\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.500492\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.505487\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.510432\n",
            "resetting env. episode 396.000000, reward total was -18.000000. running mean: -20.485328\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.490474\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.495570\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.480614\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.485808\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.480950\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.486140\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.491279\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.486366\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.481502\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.486687\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.491821\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.496902\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.491933\n",
            "resetting env. episode 410.000000, reward total was -15.000000. running mean: -20.437014\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.432644\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.438317\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.433934\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.439595\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.445199\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.440747\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.426339\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.432076\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.437755\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.443378\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.448944\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.454455\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.459910\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.465311\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.450658\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.456151\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.461590\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.466974\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.462304\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.457681\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.463104\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.458473\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.453888\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.459350\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.464756\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.460108\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.455507\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.460952\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.456343\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.461779\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.467162\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.472490\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.477765\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.462987\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.458358\n",
            "resetting env. episode 446.000000, reward total was -18.000000. running mean: -20.433774\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.439436\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.435042\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.440691\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.446285\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.451822\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.447303\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.452830\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.458302\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.463719\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.469082\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.474391\n",
            "resetting env. episode 458.000000, reward total was -18.000000. running mean: -20.449647\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.445151\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.450699\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.456192\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.461630\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.467014\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.472344\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.477620\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.482844\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.478016\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.473236\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.478503\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -20.463718\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.469081\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.474390\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.469646\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.474950\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.470200\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.475498\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.480743\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.475936\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.481177\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.476365\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.481601\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.486785\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.491917\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.496998\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.492028\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.477108\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.472337\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.477613\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.482837\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.488009\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.493129\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.498198\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.503216\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.508183\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.513102\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.517971\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.512791\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.497663\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.492686\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.497759\n",
            "CPU times: user 30min 56s, sys: 12min 57s, total: 43min 54s\n",
            "Wall time: 22min 58s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "e462457c-42e2-43b9-b667-ec0bfc1bf17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.029800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049502\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.069007\n",
            "resetting env. episode 6.000000, reward total was -18.000000. running mean: -19.058317\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.077734\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.086956\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -19.096087\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -19.095126\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -19.104175\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.123133\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.141902\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.150483\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.168978\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.187288\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.195415\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.213461\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.231326\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.249013\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.266523\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.283858\n",
            "resetting env. episode 23.000000, reward total was -18.000000. running mean: -19.271019\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.278309\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.295526\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -19.302571\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.319545\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.336349\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.352986\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.369456\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.375762\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -19.362004\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -19.368384\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.384700\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -19.380853\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.397045\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -19.393074\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -19.389143\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -19.385252\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -19.381399\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.397585\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.403610\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.419573\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -19.425378\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -19.431124\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -19.436813\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.452445\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.467920\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.483241\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.498409\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.513424\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -19.518290\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -19.523107\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.537876\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.552497\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.566972\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.581303\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.585490\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -19.589635\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -19.593738\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.607801\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.621723\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -19.625506\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -19.619251\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.623058\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.636828\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -19.630459\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.644155\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.657713\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.671136\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.684425\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -19.677581\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.690805\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.703897\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -19.696858\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.699889\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.712890\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -19.715761\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.728604\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.731318\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.744005\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.756564\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -19.758999\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -19.761409\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.773795\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.786057\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.788196\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.800314\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -19.802311\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.804288\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.816245\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.828083\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.839802\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -19.831404\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.843090\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.844659\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.846212\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.857750\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.869173\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.880481\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.891676\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.902759\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.913732\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.924594\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.925349\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -19.926095\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.936834\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.947466\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.957991\n",
            "resetting env. episode 110.000000, reward total was -18.000000. running mean: -19.938411\n",
            "resetting env. episode 111.000000, reward total was -18.000000. running mean: -19.919027\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.929837\n",
            "resetting env. episode 113.000000, reward total was -18.000000. running mean: -19.910538\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.921433\n",
            "resetting env. episode 115.000000, reward total was -18.000000. running mean: -19.902219\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.913197\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.924065\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.934824\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.935476\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.946121\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.956660\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.957093\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -19.947522\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.958047\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -19.948467\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.958982\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -19.959392\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.969798\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -19.960100\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.970499\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -19.970794\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.981086\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -19.981275\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.991463\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.001548\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.001532\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.011517\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.021402\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.031188\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.030876\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.030567\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.040262\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.049859\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.039360\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.038967\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.048577\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.038091\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.037710\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.037333\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.036960\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.046590\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.056125\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.045563\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.045108\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.054657\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.064110\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.073469\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.082734\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.081907\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.091088\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.100177\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.089175\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.078283\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.087501\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.096626\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.105659\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.104603\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.113557\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.102421\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.111397\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.120283\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.129080\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.127789\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.136511\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.145146\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.153695\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.162158\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.160536\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.158931\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.167342\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.175668\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.183912\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.182072\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.190252\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.188349\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.186466\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.184601\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.192755\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.200827\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.188819\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.196931\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.204962\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.212912\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.220783\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.218575\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.226389\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.234125\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.231784\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.239466\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.247072\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.254601\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.262055\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.269434\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.276740\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.283973\n",
            "resetting env. episode 206.000000, reward total was -18.000000. running mean: -20.261133\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.248522\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.256036\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.253476\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.250941\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.248432\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.255948\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.263388\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.270754\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.268047\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.275366\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.282613\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.289786\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.276889\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.284120\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.291278\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.298366\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.295382\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.292428\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.289504\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.286609\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.283743\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.290905\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.287996\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.295116\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.292165\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.299244\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.296251\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.303289\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.310256\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.317153\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.313982\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.320842\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.327633\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.334357\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.331013\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.337703\n",
            "resetting env. episode 243.000000, reward total was -18.000000. running mean: -20.314326\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.321183\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.317971\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.314792\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.321644\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.328427\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.335143\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.341791\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.348374\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.354890\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.351341\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.357828\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.364249\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.370607\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.376901\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.383132\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.389300\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.385407\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.391553\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.397638\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.403661\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.409625\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.415528\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.411373\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.397259\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.403287\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.409254\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.405161\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.411110\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.416999\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.422829\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.428600\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.424314\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.430071\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.435771\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.441413\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.446999\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.452529\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.458004\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.463423\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.468789\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.474101\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.479360\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.474567\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.459821\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.465223\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.470571\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.475865\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.481106\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.486295\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.491432\n",
            "resetting env. episode 294.000000, reward total was -18.000000. running mean: -20.466518\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.471853\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.467134\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.462463\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.457838\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.453260\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.458727\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.454140\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.459599\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.455003\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.450453\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.445948\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.451489\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.446974\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.432504\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.438179\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.443797\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.449359\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.454866\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.460317\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.465714\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.471057\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.476346\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.471583\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.456867\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.462298\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.467675\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.462998\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.458368\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.453785\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.459247\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.454654\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.440108\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.445707\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.451250\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.446737\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.452270\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.447747\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.453270\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.458737\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.464150\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.469508\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.474813\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.480065\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.485264\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.490412\n",
            "resetting env. episode 340.000000, reward total was -18.000000. running mean: -20.465507\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.460852\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.466244\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.471581\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.466866\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.462197\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.467575\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.452899\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.448370\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.443887\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.449448\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.454953\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.460404\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.465800\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.451142\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.456630\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.452064\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.447543\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.453068\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.448537\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.454052\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.459511\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.454916\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.460367\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.455763\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.461206\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.466594\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.461928\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.467308\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.472635\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.477909\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.473130\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.478399\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.483615\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.488778\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.493891\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.498952\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.503962\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.498923\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.503933\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.508894\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.513805\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.518667\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.523480\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.528246\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.532963\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.537634\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.522257\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.507035\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.511964\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.516845\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.521676\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.516459\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.511295\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.516182\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.511020\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.515910\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.500751\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.505743\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.510686\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.515579\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.500423\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.505419\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.510365\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.515261\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.520108\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.514907\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.519758\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.514561\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.519415\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.514221\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.519079\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.523888\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.528649\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.533363\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.528029\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.532749\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.527421\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.522147\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.506926\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.501856\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.486838\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.491969\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.497050\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.492079\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.497158\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.502187\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.507165\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.512093\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.516972\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.521803\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.526585\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.531319\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.536006\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.540645\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.535239\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.539887\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.544488\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.549043\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.533552\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.538217\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.542835\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.527406\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.532132\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.536811\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.541443\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.546029\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.550568\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.555063\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.549512\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.544017\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.538577\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.543191\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.547759\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.542281\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.546859\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.551390\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.555876\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.560317\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.554714\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.559167\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.553575\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.558040\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.562459\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.566835\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.571166\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.575455\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.579700\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.583903\n",
            "resetting env. episode 469.000000, reward total was -18.000000. running mean: -20.558064\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.562483\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.566859\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.561190\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.555578\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.560022\n",
            "resetting env. episode 475.000000, reward total was -18.000000. running mean: -20.534422\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.529078\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.533787\n",
            "resetting env. episode 478.000000, reward total was -19.000000. running mean: -20.518449\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.523265\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.528032\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.522752\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.527524\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.532249\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.536926\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.541557\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -20.516142\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.520980\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.515770\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.520613\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.525407\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.520153\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.524951\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.529701\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.534404\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.539060\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.543670\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.538233\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.542851\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.547422\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.551948\n",
            "CPU times: user 29min 48s, sys: 12min 41s, total: 42min 29s\n",
            "Wall time: 22min 9s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "4d9f145c-ca2c-4b6e-920c-35e6401b8c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAG1ElEQVR4nO3dz24dZx2A4TkhTRzbcZr4T4MVlA0NUiVYIaEuumJDL4UF6lWwRYLLQGLda0AICYnCpoJFG5pG2E1qO7FDKx0WbAgHit9x7LHj51l+0Rz/rCSv5vukMzObz+cDQHFl6gGAi0c4gEw4gEw4gEw4gEw4gEw4gEw4gEw4gEw4gEw4gEw4gEw4gOzq2At/8t0bx/5a7ZXZMLx3//qw/MbpderuxvqwvHRjYf3x7u7w7PDw2J+z/uat4dbqzRPPs/fsYNh58vTEn8PZ2b93Z9i/t76wvvrZk2Htk50JJjp9H3z4xWzMdaPD8f7bi/9Jp3R3c3PYvH17Yf3Z4WEMx5vD/e3tE8/z8PPHwnHB7H1nfXj07oOF9bd+95fXNhxj2aoAmXAAmXAAmXAA2ejD0cvm6f7+sLd/sLB+c3VluL22NsFEMB3hOKbdJ0+Hvz58uLB+f3tbOLh0bFWATDiATDiATDiAzOHoMd1cWR6+vbm5sL62ujLBNDAt4TimrfX1YWt98QtQcBnZqgCZcACZcACZcACZw9FjOnj+/L8+EGhl6cawurI8wUQwHeE4psc7u//zuyoPVu5PMBFMx1YFyIQDyIQDyIQDyByOHtONpevDnVu3FtaXl5YmmAamJRzHtL21NWxvbU09BpwLtipAJhxAJhxAJhxA9tocjj4/PBy+vLr463z19dfpc45e/GP4cn//xPMcvjg68Wdwtq4dHA0rjxZfFH5t39/lf5rN5/NRF/7y/TvjLoRz6pv+Qc/ObIqz9cGHX4z61V6bOw44qdc1DqfBGQeQCQeQjd6qvPezX73KOYALZPTh6O7ursNRuODW19dHHe3YqgCZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcADZ6K/V/+HXv3iVc7xsNgz33ro7LF2/tvBHf3v8eDg8enF6PxsukR//9Oejrju3zxz90Q++P6ytrr60Np/Ph9//6c/Dk7290/zRcGmMfeaorQqQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQjX504Gmbz//1xK+F9QlmAV52bsPxx48/Hr51ZfGG6PDoaIJpgH93bsMhEHB+OeMAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAsqtTDwCX3Yu1G8PuO/cW1t84OBo2Pvp0mE0w0/8jHDCxF7eWh8/efXsYZi8nYuXR02Hjo08nmuqb2aoAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmdcjwMSuHn013Pxkd2F96cnBBNMcj3DAxJb/vjd87ze/nXqMxFYFyEbfcWw++OGrnAO4QGbz+XzUhTs7O+MuBM6NjY2NUa+mHX3HMZudx1fhAmfBGQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQjX6vCnB5ueMAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAMuEAsn8CHte1O3OvWzcAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "a39008e8-6ce1-49c5-93df-c514774655e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -18.000000. running mean: -19.980000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.990200\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.000298\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.010295\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.010192\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.020090\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.029889\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.039590\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.039194\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.038803\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.048414\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.047930\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.047451\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.056977\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.066407\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.065743\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.075085\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.074334\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.073591\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.062855\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.072227\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.081504\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.090689\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.099782\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.108785\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.107697\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.106620\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.115554\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.124398\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.123154\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.131923\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.130603\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.129297\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.138004\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.136624\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.145258\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.153805\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.152267\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.150745\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -20.139237\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.147845\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.156366\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.164803\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.173155\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.181423\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.189609\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.197713\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.205736\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.213678\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.201542\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.199526\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.207531\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.215456\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.213301\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.221168\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.228956\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.236667\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.234300\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.241957\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.239538\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.247142\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.234671\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.242324\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.249901\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.247402\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -20.234928\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.242579\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.240153\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.247751\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.255274\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.262721\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.260094\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.267493\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.264818\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.272170\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.279448\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.286654\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.293787\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.300849\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.297841\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.304862\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.311814\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.318695\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.305508\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.302453\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.309429\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.306335\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.303271\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.310239\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.307136\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.314065\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.320924\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.317715\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.324538\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.331292\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.337979\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.344600\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.351154\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.357642\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.344066\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.350625\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.357119\n",
            "resetting env. episode 104.000000, reward total was -18.000000. running mean: -20.333548\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.330212\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.316910\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.323741\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.320503\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.327298\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.334025\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.340685\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.327278\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.324006\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.330766\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.327458\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.334183\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.340841\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.347433\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.353959\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.360419\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.356815\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -20.343247\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.339814\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.336416\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.343052\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.329621\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.316325\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.313162\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.320030\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.326830\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.333562\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.340226\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.346824\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.343356\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.349922\n",
            "resetting env. episode 136.000000, reward total was -18.000000. running mean: -20.326423\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.323159\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.329927\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.316628\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.323462\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.320227\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.327025\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.333754\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.340417\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.347013\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.353543\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.350007\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.356507\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.362942\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.359313\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.355719\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.342162\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.348741\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.355253\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.361701\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.358084\n",
            "resetting env. episode 157.000000, reward total was -18.000000. running mean: -20.334503\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.341158\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.347746\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.354269\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.350726\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.357219\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.343647\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.340210\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.346808\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.353340\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.359807\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.366209\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.372546\n",
            "resetting env. episode 170.000000, reward total was -18.000000. running mean: -20.348821\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.355333\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.361779\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.358162\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.364580\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.360934\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.367325\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.373652\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -20.359915\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.356316\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.342753\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.349325\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.345832\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.352374\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.348850\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.335361\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.342008\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.348588\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.355102\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.361551\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.357935\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.364356\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.360712\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.357105\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.353534\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.359999\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.366399\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.372735\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.359008\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.365418\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.361763\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.368146\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.374464\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.370720\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.377012\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.383242\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.379410\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.385616\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.391760\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.397842\n",
            "resetting env. episode 210.000000, reward total was -18.000000. running mean: -20.373864\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.380125\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.386324\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.392460\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.388536\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.394651\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.390704\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.396797\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.402829\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.398801\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.394813\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.400865\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.386856\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.392987\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.389058\n",
            "resetting env. episode 225.000000, reward total was -18.000000. running mean: -20.365167\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.371515\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.377800\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.384022\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.380182\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.366380\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.372716\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.368989\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.365299\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.371646\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.357930\n",
            "resetting env. episode 236.000000, reward total was -17.000000. running mean: -20.324350\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.321107\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.327896\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.324617\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.331371\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.338057\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.334676\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.341330\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.337916\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.334537\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.331192\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.337880\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.334501\n",
            "resetting env. episode 249.000000, reward total was -18.000000. running mean: -20.311156\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.318045\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.324864\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.321616\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.328399\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.335115\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.341764\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.348347\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.344863\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.351414\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.357900\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.354321\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.340778\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.347370\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.353897\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.340358\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.346954\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.343485\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.350050\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.336549\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.343184\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.329752\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.316454\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.323290\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.320057\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.316856\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.323688\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.320451\n",
            "resetting env. episode 277.000000, reward total was -16.000000. running mean: -20.277246\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.284474\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.281629\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.288813\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.295925\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.292966\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.300036\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.297035\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.304065\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.311024\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.317914\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.324735\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.331488\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.338173\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.334791\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.341443\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.348029\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.344549\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.351103\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.357592\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.364016\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.350376\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.356872\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.363303\n",
            "resetting env. episode 301.000000, reward total was -17.000000. running mean: -20.329670\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.336374\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.343010\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.349580\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.346084\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.352623\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.359097\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.365506\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.371851\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.378132\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.384351\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.390508\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.396603\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.402637\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.408610\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.414524\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.420379\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.426175\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -20.411913\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.417794\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.423616\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.429380\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.425086\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.420835\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.426627\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.432361\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.428037\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.423757\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.419519\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.425324\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.431071\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.426760\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.432492\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.438168\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.433786\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.419448\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.425254\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.431001\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.426691\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.422424\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.428200\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.433918\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.439579\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.435183\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.440831\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.446423\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.451958\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.457439\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.452865\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.448336\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.453853\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.439314\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.444921\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.450472\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.455967\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.461407\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.466793\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.462125\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.457504\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.462929\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.458300\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.463717\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.459080\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.454489\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.459944\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.455344\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.460791\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.466183\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.471521\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.476806\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.482038\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.477218\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.482445\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.477621\n",
            "resetting env. episode 375.000000, reward total was -18.000000. running mean: -20.452845\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.458316\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.463733\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.469096\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.454405\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.439861\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.445462\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.451008\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.456497\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.441932\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.447513\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.453038\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.458508\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.463923\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.449283\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.434791\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.440443\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.436038\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.431678\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.427361\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.433087\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.428757\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.434469\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.440124\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.435723\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.421366\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.417152\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.422981\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.428751\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.434463\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.430119\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.435817\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.431459\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.437145\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.432773\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.428446\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.434161\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.439819\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.445421\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.440967\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.436557\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.442192\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.437770\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.443392\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.448958\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.454469\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.459924\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.455325\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.460772\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.456164\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.461602\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.456986\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.462416\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.467792\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.473114\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.478383\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.483599\n",
            "resetting env. episode 432.000000, reward total was -18.000000. running mean: -20.458763\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.454176\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.439634\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.445238\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.450785\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.446277\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.441815\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.447396\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.452922\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.448393\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.453909\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.439370\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.444976\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.450527\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.446021\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.431561\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.417246\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.423073\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.428842\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.414554\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.410408\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.396304\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.402341\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.408318\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.394235\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.400292\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.406289\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.392227\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.398304\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.394321\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.400378\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.396374\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.392411\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.378486\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.384702\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -20.360855\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.367246\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.373574\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.379838\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.366039\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.372379\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.368655\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.364969\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.371319\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.377606\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.383830\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.389991\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.386092\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.392231\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.378308\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.374525\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.370780\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.367072\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.343401\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.339967\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.346568\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.343102\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.349671\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.346174\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.352713\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.359185\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.365594\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.371938\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.358218\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.364636\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.370990\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.377280\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.373507\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.369772\n",
            "resetting env. episode 501.000000, reward total was -18.000000. running mean: -20.346074\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.352614\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.359087\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.365497\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.371842\n",
            "resetting env. episode 506.000000, reward total was -19.000000. running mean: -20.358123\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.354542\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.360997\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.367387\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.363713\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.360076\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.366475\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.372810\n",
            "resetting env. episode 514.000000, reward total was -18.000000. running mean: -20.349082\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.355591\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.352035\n",
            "resetting env. episode 517.000000, reward total was -19.000000. running mean: -20.338515\n",
            "resetting env. episode 518.000000, reward total was -19.000000. running mean: -20.325130\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.331878\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.338560\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.335174\n",
            "resetting env. episode 522.000000, reward total was -19.000000. running mean: -20.321822\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -20.308604\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.315518\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.312363\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.319239\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.316047\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.322886\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.329657\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.336361\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.342997\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.349567\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.346072\n",
            "resetting env. episode 534.000000, reward total was -17.000000. running mean: -20.312611\n",
            "resetting env. episode 535.000000, reward total was -19.000000. running mean: -20.299485\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.306490\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.313425\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.310291\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.317188\n",
            "resetting env. episode 540.000000, reward total was -19.000000. running mean: -20.304016\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.310976\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.307866\n",
            "resetting env. episode 543.000000, reward total was -20.000000. running mean: -20.304787\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.311740\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.308622\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.315536\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.312381\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.319257\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.326064\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -20.322804\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.329576\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.336280\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.332917\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.329588\n",
            "resetting env. episode 555.000000, reward total was -18.000000. running mean: -20.306292\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.303229\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.310197\n",
            "resetting env. episode 558.000000, reward total was -19.000000. running mean: -20.297095\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -20.294124\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.291183\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.298271\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.295288\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.302335\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.299312\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.296319\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.303356\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.310322\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.317219\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.324047\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.330806\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.327498\n",
            "resetting env. episode 572.000000, reward total was -17.000000. running mean: -20.294223\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.291281\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.298368\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.305384\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.302330\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.309307\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.316214\n",
            "resetting env. episode 579.000000, reward total was -19.000000. running mean: -20.303052\n",
            "resetting env. episode 580.000000, reward total was -18.000000. running mean: -20.280021\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.287221\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.294349\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.301406\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.308391\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.315308\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.312154\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.309033\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.315943\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.312783\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.319655\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.326459\n",
            "resetting env. episode 592.000000, reward total was -19.000000. running mean: -20.313194\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.310062\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.316962\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.323792\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.330554\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.327249\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.333976\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.340636\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.347230\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.353758\n",
            "resetting env. episode 602.000000, reward total was -19.000000. running mean: -20.340220\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.346818\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.353350\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.359816\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.356218\n",
            "resetting env. episode 607.000000, reward total was -19.000000. running mean: -20.342656\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.349229\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.345737\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.352280\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.358757\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -20.345169\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.351718\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.358200\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.364618\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.360972\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.367363\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.373689\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.379952\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.386152\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.392291\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.398368\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.404384\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.410341\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.416237\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.422075\n",
            "resetting env. episode 627.000000, reward total was -19.000000. running mean: -20.407854\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.403775\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.409738\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.415640\n",
            "resetting env. episode 631.000000, reward total was -19.000000. running mean: -20.401484\n",
            "resetting env. episode 632.000000, reward total was -19.000000. running mean: -20.387469\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.383594\n",
            "resetting env. episode 634.000000, reward total was -18.000000. running mean: -20.359758\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.366161\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.362499\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.368874\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.365186\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.371534\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.377818\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.384040\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.390200\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.386298\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.392435\n",
            "resetting env. episode 645.000000, reward total was -20.000000. running mean: -20.388510\n",
            "resetting env. episode 646.000000, reward total was -17.000000. running mean: -20.354625\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.361079\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.367468\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.373794\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.380056\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.376255\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.382493\n",
            "resetting env. episode 653.000000, reward total was -16.000000. running mean: -20.338668\n",
            "resetting env. episode 654.000000, reward total was -19.000000. running mean: -20.325281\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.332028\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.328708\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.335421\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.342067\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.348646\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.345159\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.351708\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.358191\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.364609\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.370963\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.367253\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.373581\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.369845\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -20.366146\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.372485\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.378760\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.384972\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.391123\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.397211\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.393239\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.399307\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -20.385314\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.391461\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -20.377546\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.383771\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.389933\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.396034\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.402073\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.398053\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.404072\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.400031\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.406031\n",
            "resetting env. episode 687.000000, reward total was -19.000000. running mean: -20.391971\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.398051\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.404071\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.410030\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.405930\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.401870\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.407852\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.413773\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.419635\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.415439\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.421285\n",
            "resetting env. episode 698.000000, reward total was -19.000000. running mean: -20.407072\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.413001\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.418871\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -20.414682\n",
            "resetting env. episode 702.000000, reward total was -19.000000. running mean: -20.400535\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.406530\n",
            "resetting env. episode 704.000000, reward total was -19.000000. running mean: -20.392465\n",
            "resetting env. episode 705.000000, reward total was -18.000000. running mean: -20.368540\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.354855\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.361306\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.367693\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.374016\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.380276\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -20.366473\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.372809\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.379080\n",
            "resetting env. episode 714.000000, reward total was -19.000000. running mean: -20.365290\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.371637\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.377920\n",
            "resetting env. episode 717.000000, reward total was -19.000000. running mean: -20.364141\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.370500\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.366795\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.373127\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.379396\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.385602\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.391746\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.387828\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.393950\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.400010\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.406010\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.411950\n",
            "resetting env. episode 729.000000, reward total was -19.000000. running mean: -20.397831\n",
            "resetting env. episode 730.000000, reward total was -18.000000. running mean: -20.373852\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.380114\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.386313\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.392450\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.398525\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.404540\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.400494\n",
            "resetting env. episode 737.000000, reward total was -19.000000. running mean: -20.386489\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -20.382625\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.378798\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.385010\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.391160\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.387249\n",
            "resetting env. episode 743.000000, reward total was -19.000000. running mean: -20.373376\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.379642\n",
            "resetting env. episode 745.000000, reward total was -20.000000. running mean: -20.375846\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.372087\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.378367\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -20.374583\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -20.370837\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.377129\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.383357\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.389524\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.395629\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.401672\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -20.407656\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.403579\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.409543\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.415448\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.421293\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.417080\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.422910\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.418681\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -20.404494\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.410449\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.416344\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.422181\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -20.407959\n",
            "resetting env. episode 768.000000, reward total was -19.000000. running mean: -20.393879\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -20.379941\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.376141\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -20.362380\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.358756\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.355168\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -20.351617\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.348101\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.354620\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.351073\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -20.337563\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -20.334187\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.340845\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.347437\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.343962\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.340523\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.337118\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -20.323746\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.330509\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -20.327204\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.323932\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.330692\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.327386\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.334112\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.340771\n",
            "resetting env. episode 793.000000, reward total was -19.000000. running mean: -20.327363\n",
            "resetting env. episode 794.000000, reward total was -19.000000. running mean: -20.314089\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.320948\n",
            "resetting env. episode 796.000000, reward total was -20.000000. running mean: -20.317739\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.314561\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -20.301416\n",
            "resetting env. episode 799.000000, reward total was -18.000000. running mean: -20.278402\n",
            "resetting env. episode 800.000000, reward total was -20.000000. running mean: -20.275618\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.282861\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.290033\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.297133\n",
            "resetting env. episode 804.000000, reward total was -18.000000. running mean: -20.274161\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.281420\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.288605\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.295719\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.302762\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.299735\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.306737\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.303670\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.310633\n",
            "resetting env. episode 813.000000, reward total was -19.000000. running mean: -20.297527\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.304552\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.311506\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.318391\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.325207\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.331955\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.338635\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.335249\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -20.331897\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.338578\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -20.335192\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.341840\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.348422\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.354937\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.351388\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.357874\n",
            "resetting env. episode 829.000000, reward total was -19.000000. running mean: -20.344295\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.350852\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -20.337344\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.333970\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.340631\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.337224\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.343852\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.350414\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.356909\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.363340\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -20.359707\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.366110\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.362449\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.368824\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.365136\n",
            "resetting env. episode 844.000000, reward total was -19.000000. running mean: -20.351485\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.357970\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -20.354390\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.350846\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.357338\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -20.353764\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.350227\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.356725\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.363157\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.369526\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.375830\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.382072\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.388251\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.384369\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.390525\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.396620\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.392654\n",
            "resetting env. episode 861.000000, reward total was -17.000000. running mean: -20.358727\n",
            "resetting env. episode 862.000000, reward total was -19.000000. running mean: -20.345140\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.351689\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.358172\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.354590\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.361044\n",
            "resetting env. episode 867.000000, reward total was -19.000000. running mean: -20.347434\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.343959\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.350520\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.347014\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.353544\n",
            "resetting env. episode 872.000000, reward total was -19.000000. running mean: -20.340009\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.346609\n",
            "resetting env. episode 874.000000, reward total was -20.000000. running mean: -20.343143\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.349711\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.346214\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.352752\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.359225\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -20.345632\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.352176\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.358654\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.365068\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.361417\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.367803\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.374125\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.380384\n",
            "resetting env. episode 887.000000, reward total was -19.000000. running mean: -20.366580\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.372914\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.379185\n",
            "resetting env. episode 890.000000, reward total was -19.000000. running mean: -20.365393\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.371739\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -20.368022\n",
            "resetting env. episode 893.000000, reward total was -18.000000. running mean: -20.344341\n",
            "resetting env. episode 894.000000, reward total was -18.000000. running mean: -20.320898\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.327689\n",
            "resetting env. episode 896.000000, reward total was -20.000000. running mean: -20.324412\n",
            "resetting env. episode 897.000000, reward total was -20.000000. running mean: -20.321168\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.317956\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.324777\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -20.321529\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.328314\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.335031\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.341680\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.338263\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.344881\n",
            "resetting env. episode 906.000000, reward total was -17.000000. running mean: -20.311432\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.318318\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.325134\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.331883\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.338564\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.335179\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.341827\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -20.328409\n",
            "resetting env. episode 914.000000, reward total was -15.000000. running mean: -20.275125\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.282373\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.279550\n",
            "resetting env. episode 917.000000, reward total was -19.000000. running mean: -20.266754\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.264087\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.271446\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.278731\n",
            "resetting env. episode 921.000000, reward total was -19.000000. running mean: -20.265944\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.273284\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.280552\n",
            "resetting env. episode 924.000000, reward total was -19.000000. running mean: -20.267746\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.265069\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -20.262418\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.269794\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.277096\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.284325\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -20.281482\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.278667\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.285880\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -20.273021\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -20.260291\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.267688\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.265011\n",
            "resetting env. episode 937.000000, reward total was -19.000000. running mean: -20.252361\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.259838\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.257239\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.264667\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.272020\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.269300\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.276607\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.283841\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.291002\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.298092\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.305112\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.312060\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.318940\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.325750\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.332493\n",
            "resetting env. episode 952.000000, reward total was -18.000000. running mean: -20.309168\n",
            "resetting env. episode 953.000000, reward total was -19.000000. running mean: -20.296076\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.303116\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.310084\n",
            "resetting env. episode 956.000000, reward total was -19.000000. running mean: -20.296984\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.294014\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.301074\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.308063\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.304982\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.311932\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.318813\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.325625\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.322369\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -20.319145\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.325954\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.332694\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.339367\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.345973\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.352514\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.358989\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.365399\n",
            "resetting env. episode 973.000000, reward total was -18.000000. running mean: -20.341745\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.348327\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -20.344844\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.351395\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.357882\n",
            "resetting env. episode 978.000000, reward total was -19.000000. running mean: -20.344303\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -20.340860\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.347451\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.353977\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -20.350437\n",
            "resetting env. episode 983.000000, reward total was -19.000000. running mean: -20.336932\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -20.333563\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.340227\n",
            "resetting env. episode 986.000000, reward total was -20.000000. running mean: -20.336825\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.343457\n",
            "resetting env. episode 988.000000, reward total was -19.000000. running mean: -20.330022\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.336722\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.343355\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -20.329921\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.336622\n",
            "resetting env. episode 993.000000, reward total was -19.000000. running mean: -20.323256\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.330023\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.336723\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.333356\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.340022\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.346622\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.353156\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.349624\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.356128\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.362567\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.368941\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.375252\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.381499\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.377684\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.383907\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.390068\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.396168\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.402206\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.408184\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.414102\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.419961\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.425761\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.421504\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.427289\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.433016\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.438686\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.444299\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -20.439856\n",
            "resetting env. episode 1021.000000, reward total was -18.000000. running mean: -20.415457\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.421303\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.427090\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.432819\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.438491\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.444106\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.449665\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.455168\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.460616\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.466010\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.471350\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.476637\n",
            "resetting env. episode 1033.000000, reward total was -18.000000. running mean: -20.451870\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.447352\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.442878\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.448449\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.453965\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.449425\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.454931\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.460382\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.465778\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.471120\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -20.466409\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.461745\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.467127\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -20.462456\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.467831\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.473153\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.478422\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.483637\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.488801\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.483913\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.489074\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -20.474183\n",
            "resetting env. episode 1055.000000, reward total was -19.000000. running mean: -20.459441\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.464847\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.470198\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.475496\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.480741\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -20.475934\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.481175\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.486363\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.481499\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.486684\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.491817\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.496899\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.501930\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.506911\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.511842\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.516723\n",
            "resetting env. episode 1071.000000, reward total was -19.000000. running mean: -20.501556\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -20.486541\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.491675\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.496758\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.491791\n",
            "resetting env. episode 1076.000000, reward total was -19.000000. running mean: -20.476873\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.482104\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.477283\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.482510\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -20.477685\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -20.472908\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.478179\n",
            "resetting env. episode 1083.000000, reward total was -19.000000. running mean: -20.463398\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.468764\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.474076\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.479335\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.484542\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.489696\n",
            "resetting env. episode 1089.000000, reward total was -21.000000. running mean: -20.494799\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.499851\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -20.494853\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -20.489904\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.495005\n",
            "resetting env. episode 1094.000000, reward total was -19.000000. running mean: -20.480055\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.475255\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.480502\n",
            "resetting env. episode 1097.000000, reward total was -19.000000. running mean: -20.465697\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -20.461040\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -20.456430\n",
            "resetting env. episode 1100.000000, reward total was -19.000000. running mean: -20.441866\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -20.427447\n",
            "resetting env. episode 1102.000000, reward total was -19.000000. running mean: -20.413172\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.419041\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.424850\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.430602\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.436296\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.441933\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.437513\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.443138\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.438707\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.444320\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.449877\n",
            "resetting env. episode 1113.000000, reward total was -19.000000. running mean: -20.435378\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.431024\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -20.426714\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.432447\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.438122\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.433741\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.439404\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.435010\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.440660\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.446253\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.451790\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -20.447272\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.452800\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -20.448272\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.453789\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -20.449251\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.454759\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.460211\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.465609\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -20.460953\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -20.456343\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.461780\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.457162\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.452590\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.458065\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.453484\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.448949\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.454460\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.459915\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -20.445316\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.450863\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -20.456354\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -20.441791\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.437373\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -20.432999\n",
            "resetting env. episode 1148.000000, reward total was -18.000000. running mean: -20.408669\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.414582\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.420436\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.426232\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.421970\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.417750\n",
            "resetting env. episode 1154.000000, reward total was -20.000000. running mean: -20.413573\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.419437\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.425242\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -20.420990\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.426780\n",
            "resetting env. episode 1159.000000, reward total was -19.000000. running mean: -20.412512\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -20.408387\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.414303\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.420160\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -20.415959\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.411799\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -20.407681\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.403604\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -20.389568\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.395673\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.401716\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.407699\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.413622\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.419485\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.415291\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.421138\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.426926\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.432657\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.438330\n",
            "resetting env. episode 1178.000000, reward total was -19.000000. running mean: -20.423947\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -20.419708\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.425511\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -20.421256\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -20.427043\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.432773\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.438445\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.444060\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -20.439620\n",
            "resetting env. episode 1187.000000, reward total was -19.000000. running mean: -20.425224\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.430971\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -20.426662\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.432395\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.438071\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.433690\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -20.429353\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.435060\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.440709\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.446302\n",
            "resetting env. episode 1197.000000, reward total was -19.000000. running mean: -20.431839\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -20.427521\n",
            "resetting env. episode 1199.000000, reward total was -19.000000. running mean: -20.413246\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -20.409113\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.415022\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.420872\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.416663\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.422496\n",
            "resetting env. episode 1205.000000, reward total was -19.000000. running mean: -20.408271\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.404189\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.390147\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.396245\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.402283\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.408260\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.414178\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.420036\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -20.415835\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.421677\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.427460\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -20.413186\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.419054\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.424863\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.430615\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.426308\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.432045\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -20.427725\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.433448\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.439113\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.444722\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.440275\n",
            "resetting env. episode 1227.000000, reward total was -19.000000. running mean: -20.425872\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -20.421613\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.427397\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.433123\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -20.428792\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.424504\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.430259\n",
            "resetting env. episode 1234.000000, reward total was -19.000000. running mean: -20.415957\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.421797\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.427579\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -20.423303\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.429070\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.434779\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.430432\n",
            "resetting env. episode 1241.000000, reward total was -19.000000. running mean: -20.416127\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.421966\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -20.417746\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.413569\n",
            "resetting env. episode 1245.000000, reward total was -18.000000. running mean: -20.389433\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.395539\n",
            "resetting env. episode 1247.000000, reward total was -19.000000. running mean: -20.381584\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.387768\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -20.383890\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.390051\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.386151\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.382289\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.378466\n",
            "resetting env. episode 1254.000000, reward total was -17.000000. running mean: -20.344682\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.351235\n",
            "resetting env. episode 1256.000000, reward total was -18.000000. running mean: -20.327722\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.334445\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.341101\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.347690\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.344213\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.350771\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.357263\n",
            "resetting env. episode 1263.000000, reward total was -20.000000. running mean: -20.353690\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -20.340153\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.346752\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.353284\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -20.349752\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.346254\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.352791\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.349264\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.355771\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.352213\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.358691\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.365104\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.371453\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.377739\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.383961\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.390122\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.396220\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.402258\n",
            "resetting env. episode 1281.000000, reward total was -20.000000. running mean: -20.398236\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.404253\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.410211\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.416109\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.421948\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.427728\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.433451\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.429116\n",
            "resetting env. episode 1289.000000, reward total was -18.000000. running mean: -20.404825\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.410777\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -20.406669\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -20.402602\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.408576\n",
            "resetting env. episode 1294.000000, reward total was -19.000000. running mean: -20.394491\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.400546\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.406540\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.412475\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.418350\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.424167\n",
            "resetting env. episode 1300.000000, reward total was -19.000000. running mean: -20.409925\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.415826\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.411667\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -20.397551\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -20.393575\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -20.389639\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.395743\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -20.391786\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -20.387868\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.383989\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.380149\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.386348\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.392484\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -20.388559\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.394674\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.400727\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.406720\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.412653\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.418526\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.424341\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.430097\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.435796\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.441438\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.447024\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.452554\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.448028\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -20.443548\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.449113\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.454621\n",
            "resetting env. episode 1329.000000, reward total was -19.000000. running mean: -20.440075\n",
            "resetting env. episode 1330.000000, reward total was -19.000000. running mean: -20.425674\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.431418\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.437104\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.442733\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.448305\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.453822\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.459284\n",
            "resetting env. episode 1337.000000, reward total was -19.000000. running mean: -20.444691\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -20.440244\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.435842\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.441483\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.447068\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.442598\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.448172\n",
            "resetting env. episode 1344.000000, reward total was -19.000000. running mean: -20.433690\n",
            "resetting env. episode 1345.000000, reward total was -19.000000. running mean: -20.419353\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.415160\n",
            "resetting env. episode 1347.000000, reward total was -18.000000. running mean: -20.391008\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.387098\n",
            "resetting env. episode 1349.000000, reward total was -19.000000. running mean: -20.373227\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.379495\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.375700\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.381943\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.388123\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.384242\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -20.370400\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.376696\n",
            "resetting env. episode 1357.000000, reward total was -19.000000. running mean: -20.362929\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.369299\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -20.365606\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.371950\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.378231\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.374449\n",
            "resetting env. episode 1363.000000, reward total was -19.000000. running mean: -20.360704\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.367097\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -20.363426\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.369792\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -20.366094\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.372433\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.378709\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.384922\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.391072\n",
            "resetting env. episode 1372.000000, reward total was -19.000000. running mean: -20.377162\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.373390\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.379656\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -20.375860\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.382101\n",
            "resetting env. episode 1377.000000, reward total was -19.000000. running mean: -20.368280\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.374597\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.380851\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.387043\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -20.383172\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.389341\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.395447\n",
            "resetting env. episode 1384.000000, reward total was -19.000000. running mean: -20.381493\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.377678\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.383901\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.390062\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -20.386161\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.392300\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.398377\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.404393\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -20.400349\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.406345\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.412282\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.408159\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.414078\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.419937\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.425737\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -20.421480\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.427265\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.432993\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.438663\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.444276\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -20.439833\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.445435\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.450981\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -20.446471\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.452006\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.457486\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.462911\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.468282\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.473599\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.478863\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.484075\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.489234\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.494342\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.499398\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.504404\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.509360\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.504267\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -20.499224\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -20.494232\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.499289\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -20.494296\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.499353\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -20.494360\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.499416\n",
            "resetting env. episode 1428.000000, reward total was -19.000000. running mean: -20.484422\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.479578\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.484782\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.479934\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.485135\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.490284\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.495381\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.500427\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.495423\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.500469\n",
            "resetting env. episode 1438.000000, reward total was -20.000000. running mean: -20.495464\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.500509\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.505504\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.510449\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.515345\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.520191\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -20.514989\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.519839\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.524641\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -20.519395\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -20.504201\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.509159\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.504067\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.509026\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.513936\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.518797\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.523609\n",
            "resetting env. episode 1455.000000, reward total was -18.000000. running mean: -20.498373\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.503389\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -20.498355\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.503371\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.508338\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.513254\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.518122\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.522941\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.527711\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.532434\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -20.527110\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -20.521839\n",
            "resetting env. episode 1467.000000, reward total was -19.000000. running mean: -20.506620\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.511554\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -20.516439\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -20.511274\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.516161\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.521000\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -20.505790\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.510732\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -20.505625\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.510568\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.515463\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -20.520308\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.525105\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -20.519854\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.524655\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.529409\n",
            "resetting env. episode 1483.000000, reward total was -18.000000. running mean: -20.504115\n",
            "resetting env. episode 1484.000000, reward total was -19.000000. running mean: -20.489074\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -20.484183\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -20.479341\n",
            "resetting env. episode 1487.000000, reward total was -20.000000. running mean: -20.474548\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -20.469802\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -20.465104\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.470453\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.475749\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.480991\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -20.476181\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.481419\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -20.476605\n",
            "resetting env. episode 1496.000000, reward total was -19.000000. running mean: -20.461839\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.467221\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.472548\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -20.467823\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.473145\n",
            "CPU times: user 1h 28min 7s, sys: 38min 19s, total: 2h 6min 27s\n",
            "Wall time: 1h 5min 49s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "w2NblmwDsL3y",
        "outputId": "ed9bf125-ac01-41b4-d07d-27e131d5e4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHj0lEQVR4nO3dT29cVx3H4XOdSW2PndiJPSY2VU0LdNNNpXbbFQvIjhXvgQXqG0BCLNgiwbuALagV76ArhNhGIiqKVKf1xHH8Zzx2k8sKqe0k6nyvHZ2x/TzLE92bnyX7ozlHunObtm0LQGKu9gDA5SMcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiva4X/uIni1M/VjvXlPLR9nzp33x9nbq3vlb6C4sT64+Hw3I0Gk19n7XVlbKyfOvc8zw7Oiy7e0/PfR8u3v72ejnavHPu+/Qf75fVh19ewET1fPzpk6bLdZ3Dcf+nk3+kNd0bDMrgzuQvw9FoFIZjtWxvbZ17nkc7j4VjRu3/aKN8+cHb577P+r8/v/Th6MpWBYgJBxATDiAmHECs8+HodfP04KA8OzicWL+1vFTu3L5dYSIu2tIXe2Xpi8kD7eMfrJTDH96tMNHsEo4pDfeelv88ejSxvr21JRxXxMrDr8rWZw8m1nc+fEc4vsNWBYgJBxATDiAmHEDM4eiUbi31y+ZgMLF+e3mpwjRQl3BMaWNtrWysrdUeA2aCrQoQEw4gJhxATDiAmMPRKR0eH7/0C4GWFhbL8lK/wkRQj3BM6fHu8JXPqry7tF1hIqjHVgWICQcQEw4gJhxAzOHolBYX5svdlZWJ9f7CQoVpeB3GK/3y7K3JxwpOVj2P9F3CMaWtjY2ytbFRewxeo+F7b5bhe2/WHuNSsFUBYsIBxIQDiAkHELsyh6PHo1HZ703+OGdffx3d52R8WvYPDs49z2h8cu578HrMH4xe+v6U+D7707/M/Kpp2rbtdOGf7t/tdiFUdpG/uM0F3quGjz990ulHuDKfOGBal/2PfRY44wBiwgHEOm9VPvrNny9yDuAS6Xw4OhwOHY7CJbe2ttbpyMdWBYgJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAWOfH6v/11z9e5BxABT/79R86Xec7R+Ea6/qdo7YqQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOINarPcCrvLV5r8y/MT+x/mhnp4zG4woTAf83s+HYHAzK7eXlb621bVt29/aEgyrml++UZm6utG1bxodPS2lf1B6pmpkNB8ySG28slJ//7i9lcXVQnp+Nyye//WU5frJTe6xqhAOm0TSlt9AvNxeXS3OjV0rT1J6oKoejQEw4gJhwADFnHDCNti1no6NyOn9Qnp+NS2nb2hNVJRwwheenJ+Ufv/9VaebmSmlLGR/t1x6pKuGAKZ1e81h8kzMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEZvax+if7+y99DcLp2VmFaYBvmtlwPPj8v7VHAF7BVgWICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEerUHgOuuLaW0c83EelNKKS/aMvkv9QkHVHa0uVoe3n9/Yn3xq2flx3/7Z4WJvp9wQGUvejfKeKVfSvPtzxa90Vmlib6fMw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEvB4BKlvcPSjv/H3y/Sm9k9l9PYJwQGU3R6fl7oOd2mNEbFWAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiva4XDt798CLnAC6Rpm3bThfu7u52uxCYGevr602X6zp/4miaTv8fcAU44wBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHECs83tVgOvLJw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGL/A5Gq7j6zkBYpAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Iteration-5_h=200_1e_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}