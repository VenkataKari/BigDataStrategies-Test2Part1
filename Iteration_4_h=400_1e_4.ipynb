{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8ngMhg3fB9aA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b96ed6-7021-40dc-a941-4e002213ea57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 27.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=5461724ff8107c8e8cf531e385916799a059180f636db74cabeedb8ad1f58707\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MtT2GyK_6edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827fa1ae-a50b-495f-f332-d2ee9346ce52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ae7111-0de1-4513-d1b3-43175bd0f30d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d85e526-f750-411b-8558-e840931ba4fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "trwRXI-h6eeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a907b9e6-7fdb-42f5-edbc-73bf0ed7f5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -15.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 400 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d2a9cb-e578-4e92-e381-3849f99cdadc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -18.000000. running mean: -20.970000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.970300\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.960597\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.950991\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.941481\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.932066\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.932746\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.923418\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.924184\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.914942\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.915793\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.916635\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.917468\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.918294\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.919111\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.919920\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.920721\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.921513\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.922298\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.913075\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.913944\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.914805\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.905657\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.906600\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.907534\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.908459\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.899374\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.900381\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.901377\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.902363\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.893340\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.894406\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.885462\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.866607\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -20.847941\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.849462\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.850967\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.852458\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -20.833933\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -20.815594\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.817438\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.799263\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.801271\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.803258\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.795226\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.797273\n",
            "resetting env. episode 49.000000, reward total was -18.000000. running mean: -20.769301\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.761608\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.753991\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.756452\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.758887\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.761298\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.763685\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.766048\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.758388\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.750804\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.743296\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.745863\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.748404\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.750920\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.753411\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.755877\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.738318\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.730935\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.733626\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.736289\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.728927\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.731637\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.734321\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.736978\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.739608\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.742212\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.734790\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.727442\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.720167\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.722966\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.725736\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.708479\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.711394\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.714280\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.717137\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.709966\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.702866\n",
            "resetting env. episode 86.000000, reward total was -19.000000. running mean: -20.685837\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.688979\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.672089\n",
            "resetting env. episode 89.000000, reward total was -17.000000. running mean: -20.635368\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.629015\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.632725\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.636397\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.640033\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.643633\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.627197\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.620925\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.604715\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.608668\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.612582\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.606456\n",
            "resetting env. episode 101.000000, reward total was -18.000000. running mean: -20.580391\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.584587\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.578741\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.582954\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.587125\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.591253\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.595341\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.599387\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.583393\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.587560\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.591684\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.585767\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.589909\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.594010\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.598070\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.602090\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.596069\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.600108\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.584107\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.588266\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.592383\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.586459\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.580595\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.574789\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.569041\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.563350\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.547717\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.542240\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.546817\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.551349\n",
            "resetting env. episode 131.000000, reward total was -19.000000. running mean: -20.535836\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.520477\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.525273\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.520020\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.524820\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.519571\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.524376\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.519132\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.523941\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.528701\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.533414\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.538080\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.542699\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.547272\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.541800\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.546382\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.530918\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.535609\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.540253\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.524850\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.519602\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.524405\n",
            "resetting env. episode 153.000000, reward total was -18.000000. running mean: -20.499161\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.494170\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.489228\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.494336\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.499392\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.504399\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.499355\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.504361\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.509317\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.514224\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.509082\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.513991\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.508851\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.513763\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.518625\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.513439\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.498304\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.503321\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.508288\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.503205\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.508173\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.513092\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.507961\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.502881\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.507852\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.512774\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.507646\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.502570\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.497544\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.502568\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.497543\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.502567\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.497542\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.492566\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.487641\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.482764\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.467936\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.473257\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.478525\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.483739\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.488902\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.494013\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.499073\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.504082\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.489041\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.494151\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.499209\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.504217\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.499175\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.504183\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.509141\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.494050\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.489110\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.494218\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.499276\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.484283\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.479441\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.484646\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.489800\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.494902\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.499953\n",
            "resetting env. episode 214.000000, reward total was -16.000000. running mean: -20.454953\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.460404\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.465800\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.461142\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.466530\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.471865\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.477146\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.472375\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.477651\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.482875\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.488046\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.493165\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.488234\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.493351\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.488418\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.493534\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.498598\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.503612\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.498576\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.493590\n",
            "resetting env. episode 234.000000, reward total was -18.000000. running mean: -20.468655\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.463968\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.469328\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.464635\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.469989\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.465289\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.470636\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.475930\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.481170\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.476359\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.481595\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.476779\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.462011\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.457391\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.442817\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.438389\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.444005\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.449565\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.455069\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.450519\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.436014\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.441653\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.437237\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.432865\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.438536\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.444151\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.449709\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.435212\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.440860\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.436451\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.432087\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.437766\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.443388\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.448954\n",
            "resetting env. episode 268.000000, reward total was -18.000000. running mean: -20.424465\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.420220\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.406018\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.401958\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.407938\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.413859\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.419720\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.415523\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.421368\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.427154\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.422883\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.428654\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.434367\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.440024\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.445623\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.441167\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.446755\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.452288\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.447765\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.443287\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.428854\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.424566\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.430320\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.416017\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.421857\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.427638\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.433362\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.429028\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.434738\n",
            "resetting env. episode 297.000000, reward total was -18.000000. running mean: -20.410391\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.416287\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.402124\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.408103\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.414022\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.409881\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.405783\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.411725\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.407607\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.403531\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.409496\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.415401\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.401247\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.397235\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.403262\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.409230\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.415137\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.410986\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.406876\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.412807\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.408679\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.414593\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.420447\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.416242\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.422080\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.427859\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.433580\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.439245\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.434852\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.440504\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.436099\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.441738\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.437320\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.432947\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.438617\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.444231\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.449789\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.445291\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.440838\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.446430\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.441966\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.447546\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.453070\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.458540\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.453954\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.459415\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.464821\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.470172\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.475471\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.480716\n",
            "resetting env. episode 347.000000, reward total was -18.000000. running mean: -20.455909\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.461350\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.466736\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.472069\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.457348\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.452775\n",
            "resetting env. episode 353.000000, reward total was -18.000000. running mean: -20.428247\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.433964\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.429625\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.435329\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.420975\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.426766\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.422498\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.428273\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.423990\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.409750\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.405653\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.401596\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.407580\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.413504\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.419369\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.415176\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.411024\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.416914\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.422745\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.418517\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.424332\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.430089\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.425788\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.431530\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.417215\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.413042\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.418912\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.414723\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.410576\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.406470\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.412405\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.418281\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.404098\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.390057\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.376157\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.372395\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.368671\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.374985\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.381235\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.377422\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.373648\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.369912\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.376213\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.382450\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.378626\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.384840\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.380991\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.377181\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.373410\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.379675\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.385879\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.382020\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.378200\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.374418\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.370674\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.366967\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.373297\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.379564\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.375769\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.372011\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.378291\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.374508\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.380763\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.376955\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.373186\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.359454\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.345859\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.342401\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.348977\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.335487\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.332132\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.328811\n",
            "resetting env. episode 425.000000, reward total was -18.000000. running mean: -20.305523\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.312467\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.299343\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.296349\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.303386\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.310352\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.307248\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.314176\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.321034\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.317824\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.314646\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.321499\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.328284\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.315001\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.301851\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.298833\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.295844\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.292886\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.289957\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.297057\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.304087\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.301046\n",
            "resetting env. episode 447.000000, reward total was -18.000000. running mean: -20.278036\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.285255\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.282403\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.289579\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.276683\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.273916\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.261177\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.258565\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.245979\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.243520\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.241084\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.248674\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.256187\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.263625\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.260989\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.258379\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.265795\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.253137\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.250606\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.248100\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.255619\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.263063\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.250432\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.257928\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.265348\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.262695\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.260068\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.267467\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.264793\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.272145\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.279423\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.286629\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.293763\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.300825\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.307817\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.314739\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.311591\n",
            "resetting env. episode 484.000000, reward total was -18.000000. running mean: -20.288475\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.285591\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.282735\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.279907\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.287108\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.284237\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.291395\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.298481\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.305496\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.302441\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.309417\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.296322\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.303359\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.310326\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.317222\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.314050\n",
            "resetting env. episode 500.000000, reward total was -18.000000. running mean: -20.290910\n",
            "CPU times: user 54min 35s, sys: 14min 11s, total: 1h 8min 46s\n",
            "Wall time: 35min 48s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcc13f2-b802-4d55-d81a-f5ef6d574328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.039404\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.049010\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.058520\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.057935\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.067355\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.076682\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.085915\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.095056\n",
            "resetting env. episode 14.000000, reward total was -18.000000. running mean: -20.074105\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.073364\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -20.062631\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.072004\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.071284\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.080571\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.069766\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.069068\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.078377\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.087594\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.086718\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.095850\n",
            "resetting env. episode 26.000000, reward total was -17.000000. running mean: -20.064892\n",
            "resetting env. episode 27.000000, reward total was -18.000000. running mean: -20.044243\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.053801\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.063263\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.062630\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.062004\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.061384\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.060770\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.070162\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.079460\n",
            "resetting env. episode 36.000000, reward total was -17.000000. running mean: -20.048666\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.058179\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.067597\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.066921\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.076252\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.085490\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.084635\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.093788\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.102851\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.091822\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.100904\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.109895\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.098796\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.087808\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.086930\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.096060\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.105100\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.114049\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.122908\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.131679\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.140363\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.138959\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.137569\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.146194\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.144732\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.153284\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.161752\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.170134\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.178433\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.166648\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.174982\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -20.163232\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.171600\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.169884\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.168185\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.166503\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.164838\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.173190\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.181458\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.189643\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.177747\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.185969\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.184110\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.192268\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.200346\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.208342\n",
            "resetting env. episode 82.000000, reward total was -19.000000. running mean: -20.196259\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.204296\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -20.182253\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.180431\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.188627\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.186740\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.184873\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.193024\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.201094\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.209083\n",
            "resetting env. episode 92.000000, reward total was -18.000000. running mean: -20.186992\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.185122\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.193271\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.181338\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.179525\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.187730\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.175852\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.184094\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.192253\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.190330\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.188427\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.196543\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.204577\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.202532\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.210506\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.208401\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.216317\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.214154\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.222012\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.229792\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.237494\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.235119\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.242768\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.250341\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.247837\n",
            "resetting env. episode 117.000000, reward total was -18.000000. running mean: -20.225359\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.233105\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.240774\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.248366\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.255883\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.263324\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.260691\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.268084\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.265403\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.262749\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.260121\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.267520\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.274845\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.272097\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.269376\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.266682\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.264015\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.261375\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.268761\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.276074\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -20.263313\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.260680\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.268073\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.275392\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.272638\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.279912\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.267113\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.274442\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.271697\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.268980\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.266290\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.273627\n",
            "resetting env. episode 149.000000, reward total was -18.000000. running mean: -20.250891\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.238382\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.225998\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.213738\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.221601\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.229385\n",
            "resetting env. episode 155.000000, reward total was -18.000000. running mean: -20.207091\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.215020\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.222870\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.230641\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.238335\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.245952\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.243492\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.241057\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.238647\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.236260\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.243898\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.241459\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.239044\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.246654\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.254187\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.261645\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.259029\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.246438\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.243974\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.241534\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.239119\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.236728\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.234361\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.242017\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.249597\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.257101\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.264530\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.271884\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.279166\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.276374\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.283610\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.290774\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.287866\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.294988\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.292038\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.289117\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.296226\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.293264\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.300331\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.307328\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.314255\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.321112\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.327901\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.334622\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -20.321276\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.318063\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.324882\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.321634\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.318417\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.325233\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.331981\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.338661\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.325274\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.332022\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.328701\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.335414\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.342060\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.328640\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.335353\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.322000\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.328780\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.325492\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.332237\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.318915\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.315726\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.322568\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.319343\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.306149\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.303088\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.300057\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.307056\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.313986\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.310846\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.317737\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.324560\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.331314\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.338001\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.344621\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.351175\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.337663\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.344287\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.350844\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.347335\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -20.333862\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.330523\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.337218\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.343846\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.350407\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.346903\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.353434\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.359900\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.356301\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.362738\n",
            "resetting env. episode 248.000000, reward total was -18.000000. running mean: -20.339111\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.345720\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.352262\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.348740\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.345252\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.351800\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.358282\n",
            "resetting env. episode 255.000000, reward total was -18.000000. running mean: -20.334699\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.341352\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.347938\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.354459\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.360914\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.357305\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.363732\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.370095\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.366394\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.362730\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.369103\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.365412\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.361758\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.368140\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.364459\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.370814\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.377106\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.373335\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.379602\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.385805\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.391947\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.378028\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.364248\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.350605\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.337099\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.333728\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.340391\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.336987\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.343617\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.350181\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.356679\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.363112\n",
            "resetting env. episode 287.000000, reward total was -18.000000. running mean: -20.339481\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.336086\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.332726\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.319398\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.326204\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.322942\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.309713\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.296616\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.293650\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.300713\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.307706\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.314629\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.321483\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.328268\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.314985\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.321835\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.328617\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.335331\n",
            "resetting env. episode 305.000000, reward total was -18.000000. running mean: -20.311977\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.318858\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.315669\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.312512\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.319387\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.326193\n",
            "resetting env. episode 311.000000, reward total was -18.000000. running mean: -20.302931\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.289902\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.287003\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.294133\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.301192\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.308180\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.315098\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.311947\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.308828\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.315739\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.322582\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.319356\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.316163\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.323001\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.329771\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.336473\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.343108\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.349677\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.346181\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.352719\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.359192\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.365600\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.361944\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.368324\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.374641\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.380895\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.377086\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.383315\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.389482\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.375587\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.381831\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.368013\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.374333\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.380589\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.376783\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.383015\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.389185\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.385293\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.371441\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.377726\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.383949\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.390109\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.396208\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.382246\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.388424\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.394539\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.390594\n",
            "resetting env. episode 358.000000, reward total was -18.000000. running mean: -20.366688\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.373021\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.369291\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.375598\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.381842\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.388024\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.394144\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.400202\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.406200\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.412138\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.418017\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.423837\n",
            "resetting env. episode 370.000000, reward total was -18.000000. running mean: -20.399598\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.405602\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.401546\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.397531\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.393555\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.379620\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.365824\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.362165\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.348544\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.345058\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.351608\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.358092\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.354511\n",
            "resetting env. episode 383.000000, reward total was -18.000000. running mean: -20.330966\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.337656\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.324279\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.331037\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.337726\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.344349\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.350905\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.347396\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.353922\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.350383\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.356879\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.343311\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.349878\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.336379\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.343015\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.349585\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.356089\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.352528\n",
            "resetting env. episode 401.000000, reward total was -18.000000. running mean: -20.329003\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.315713\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.322556\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.319330\n",
            "resetting env. episode 405.000000, reward total was -18.000000. running mean: -20.296137\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.303175\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.310144\n",
            "resetting env. episode 408.000000, reward total was -17.000000. running mean: -20.277042\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.284272\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.281429\n",
            "resetting env. episode 411.000000, reward total was -18.000000. running mean: -20.258615\n",
            "resetting env. episode 412.000000, reward total was -18.000000. running mean: -20.236029\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.243668\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.251232\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.258719\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.256132\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.263571\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.270935\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.268226\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.275544\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.272788\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.260060\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.267460\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.274785\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.282037\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.289217\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.286325\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.293461\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.290527\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.287621\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.284745\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.291898\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.298979\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.295989\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.293029\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.290099\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.287198\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.274326\n",
            "resetting env. episode 439.000000, reward total was -18.000000. running mean: -20.251583\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.259067\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.256476\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.253911\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.261372\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.268759\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.276071\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.273310\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.270577\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.267871\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.265193\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.252541\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.260015\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.267415\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.274741\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.281994\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.279174\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.266382\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.273718\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.280981\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.288171\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.285289\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.292437\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.289512\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.296617\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.303651\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.310614\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.307508\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.314433\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.311289\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.318176\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.314994\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.311844\n",
            "resetting env. episode 472.000000, reward total was -18.000000. running mean: -20.288726\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.285839\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.282980\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.290150\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.297249\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.304276\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.311234\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.298121\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.305140\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.312089\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.308968\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.315878\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.312719\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.309592\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.306496\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.303431\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.310397\n",
            "resetting env. episode 489.000000, reward total was -18.000000. running mean: -20.287293\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.294420\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.301476\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.298461\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.285476\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.282622\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.269795\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.277097\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.274327\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.281583\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.288767\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.285880\n",
            "CPU times: user 52min 30s, sys: 13min 59s, total: 1h 6min 30s\n",
            "Wall time: 34min 30s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "a7055647-af91-4dbf-adea-a8bf904b859d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9AxOcQhIsKow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83933aab-b696-49ce-ec8a-d9a51512a23c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.980199\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980397\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.970593\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -20.950887\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.951378\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.951864\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.952346\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.952822\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.953294\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.933761\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.924424\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.925179\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.925928\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.916668\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.917502\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.918327\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.909143\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.900052\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.891051\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.892141\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.873219\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.874487\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.875742\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.866985\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.848315\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.849832\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.851334\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.832820\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.834492\n",
            "resetting env. episode 34.000000, reward total was -18.000000. running mean: -20.806147\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.808086\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.800005\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.802005\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.793985\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.786045\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.788184\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.790303\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.792400\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.794476\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.776531\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.778766\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.770978\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.763268\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.765635\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.757979\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.760399\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.752795\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.755267\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.747715\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.750237\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.752735\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.745208\n",
            "resetting env. episode 57.000000, reward total was -18.000000. running mean: -20.717756\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.720578\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.723372\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.726139\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.728877\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.731588\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.734273\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.736930\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.739561\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -20.722165\n",
            "resetting env. episode 67.000000, reward total was -18.000000. running mean: -20.694943\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.677994\n",
            "resetting env. episode 69.000000, reward total was -18.000000. running mean: -20.651214\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.644702\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.638255\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.631872\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.635554\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.619198\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.613006\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.606876\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.600807\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.594799\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.588851\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.592963\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.597033\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.591063\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.585152\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.569300\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.563607\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.557971\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.562392\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.556768\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.561200\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.565588\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.569932\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -20.554233\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.558691\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.563104\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.557473\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.561898\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.566279\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.570616\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.574910\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.569161\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.553469\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.557935\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.552355\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.546832\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.541363\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.535950\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.540590\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.535184\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.519832\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.524634\n",
            "resetting env. episode 111.000000, reward total was -18.000000. running mean: -20.499388\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.484394\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.489550\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.484654\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.479808\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.485010\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.490160\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.475258\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.470506\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.475801\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.471043\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.466332\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.461669\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.467052\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.472382\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.477658\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.472881\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.458152\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.463571\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.448935\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.454446\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.439901\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.435502\n",
            "resetting env. episode 134.000000, reward total was -15.000000. running mean: -20.381147\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.387336\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.383462\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.389628\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.395732\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.391774\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.387857\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.393978\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.400038\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.406038\n",
            "resetting env. episode 144.000000, reward total was -18.000000. running mean: -20.381977\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.368158\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.374476\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.370731\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.377024\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.383254\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.389421\n",
            "resetting env. episode 151.000000, reward total was -18.000000. running mean: -20.365527\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.371872\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.368153\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.374471\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.370727\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.377019\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.363249\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.369617\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.375921\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.382161\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -20.358340\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.364756\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.371109\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.377398\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.383624\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.389788\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.375890\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.372131\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.378409\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.374625\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.380879\n",
            "resetting env. episode 172.000000, reward total was -18.000000. running mean: -20.357070\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.353500\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.359965\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.356365\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.362801\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.349173\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.355682\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.362125\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.368504\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.374818\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.381070\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.387260\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.393387\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.389453\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.395559\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.401603\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.407587\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.413511\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.409376\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.415282\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.401129\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.407118\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.403047\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.409016\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.404926\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.400877\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.406868\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -20.392800\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.388872\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.384983\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.391133\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.377222\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.363450\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.359815\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.356217\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.362655\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.369028\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.375338\n",
            "resetting env. episode 210.000000, reward total was -17.000000. running mean: -20.341584\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.348169\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.344687\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.341240\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.347828\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.354349\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.360806\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.357198\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.363626\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.369990\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.356290\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.362727\n",
            "resetting env. episode 222.000000, reward total was -18.000000. running mean: -20.339100\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.325709\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.312451\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.319327\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.326134\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.322872\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.319644\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.306447\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.313383\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.310249\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.307146\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.304075\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.311034\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.307924\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.304845\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.291796\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.288878\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.295989\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.283030\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.290199\n",
            "resetting env. episode 242.000000, reward total was -18.000000. running mean: -20.267297\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.274624\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.271878\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.259159\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.256568\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.264002\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.271362\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.278648\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.275862\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.273103\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.260372\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.267768\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.265091\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.272440\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.279715\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.276918\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.284149\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.281308\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.288495\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.295610\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.302654\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.309627\n",
            "resetting env. episode 264.000000, reward total was -18.000000. running mean: -20.286531\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.293665\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.290729\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.287821\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.274943\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.282194\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.279372\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.276578\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.283812\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.290974\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.288065\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.295184\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.302232\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.309210\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.306118\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.313056\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.319926\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.326727\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.333459\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.340125\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.346724\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.343256\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.349824\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.356325\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.362762\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.369135\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.365443\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.371789\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.368071\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.364390\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.370746\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.377039\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.383268\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.379436\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.375641\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.371885\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.378166\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.384385\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.390541\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.386635\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.382769\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.378941\n",
            "resetting env. episode 306.000000, reward total was -18.000000. running mean: -20.355152\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.351600\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.338084\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.344703\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.351256\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.337744\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.324366\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.321123\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.307912\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.294832\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.281884\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.269065\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.276375\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.283611\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.290775\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.297867\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.294888\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.301939\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.308920\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.295831\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.302873\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.299844\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.296845\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.303877\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.290838\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.287930\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.285050\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.292200\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.289278\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.296385\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.303421\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.310387\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.317283\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.324110\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.330869\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.327561\n",
            "resetting env. episode 342.000000, reward total was -18.000000. running mean: -20.304285\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.311242\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.318130\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.304948\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.311899\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.298780\n",
            "resetting env. episode 348.000000, reward total was -19.000000. running mean: -20.285792\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.292934\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.290005\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.277105\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.274334\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.271590\n",
            "resetting env. episode 354.000000, reward total was -18.000000. running mean: -20.248875\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.236386\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.224022\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.231782\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.239464\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.247069\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.244599\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.252153\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.259631\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.267035\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.264364\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.261721\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.259104\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.266513\n",
            "resetting env. episode 368.000000, reward total was -18.000000. running mean: -20.243847\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.241409\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.248995\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.256505\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.253940\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.261400\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.268786\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.266099\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.273438\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.270703\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.257996\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.255416\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.252862\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.260333\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.267730\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.265053\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.262402\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.259778\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.267180\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.264509\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.261864\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.269245\n",
            "resetting env. episode 390.000000, reward total was -18.000000. running mean: -20.246552\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.254087\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.251546\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.249031\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.236540\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.224175\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.221933\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.229714\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.227417\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.235143\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.222791\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.230563\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.228258\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.225975\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.233715\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.241378\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.248964\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.236475\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.234110\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.241769\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.229351\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.237058\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.234687\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.232340\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.230017\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.227717\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.225439\n",
            "resetting env. episode 417.000000, reward total was -17.000000. running mean: -20.193185\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.201253\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.189241\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.177348\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.185575\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.183719\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.191882\n",
            "resetting env. episode 424.000000, reward total was -17.000000. running mean: -20.159963\n",
            "resetting env. episode 425.000000, reward total was -18.000000. running mean: -20.138363\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.146980\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.155510\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.163955\n",
            "resetting env. episode 429.000000, reward total was -18.000000. running mean: -20.142315\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -20.130892\n",
            "resetting env. episode 431.000000, reward total was -18.000000. running mean: -20.109583\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.118487\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.107303\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.106230\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.105167\n",
            "resetting env. episode 436.000000, reward total was -18.000000. running mean: -20.084116\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.093274\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.082342\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.081518\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.070703\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.069996\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.079296\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.088503\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.097618\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.086642\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.095775\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.104818\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.103770\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.112732\n",
            "resetting env. episode 450.000000, reward total was -17.000000. running mean: -20.081605\n",
            "resetting env. episode 451.000000, reward total was -18.000000. running mean: -20.060788\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.050181\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.059679\n",
            "resetting env. episode 454.000000, reward total was -18.000000. running mean: -20.039082\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.048691\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.048204\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.047722\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.057245\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.056673\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.066106\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.075445\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.064690\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.054043\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.063503\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.072868\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.072139\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.081418\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.090604\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.099698\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -20.088701\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.097814\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.106836\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.095767\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.104810\n",
            "resetting env. episode 475.000000, reward total was -18.000000. running mean: -20.083761\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.072924\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.082195\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.081373\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.090559\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.089653\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.088757\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.087869\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.096991\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.096021\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.085060\n",
            "resetting env. episode 486.000000, reward total was -17.000000. running mean: -20.054210\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.063668\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.073031\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.062301\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.051678\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.051161\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.050649\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.050143\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.059641\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.059045\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.068455\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.077770\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.086992\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.096122\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.095161\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.104210\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.113167\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.112036\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.110915\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.119806\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.118608\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.127422\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.126148\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.134886\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.143538\n",
            "resetting env. episode 511.000000, reward total was -19.000000. running mean: -20.132102\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.140781\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.139373\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.147980\n",
            "resetting env. episode 515.000000, reward total was -18.000000. running mean: -20.126500\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.135235\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.143882\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.152444\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.150919\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.149410\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.147916\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -20.146437\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.144972\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.153523\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.161987\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.170368\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.178664\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.176877\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.185108\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.193257\n",
            "resetting env. episode 531.000000, reward total was -17.000000. running mean: -20.161325\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.169712\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.178014\n",
            "resetting env. episode 534.000000, reward total was -20.000000. running mean: -20.176234\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.184472\n",
            "resetting env. episode 536.000000, reward total was -19.000000. running mean: -20.172627\n",
            "resetting env. episode 537.000000, reward total was -18.000000. running mean: -20.150901\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.159392\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.167798\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.176120\n",
            "resetting env. episode 541.000000, reward total was -19.000000. running mean: -20.164359\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.172715\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.180988\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.179178\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.177386\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.185613\n",
            "resetting env. episode 547.000000, reward total was -19.000000. running mean: -20.173756\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.182019\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.190199\n",
            "resetting env. episode 550.000000, reward total was -19.000000. running mean: -20.178297\n",
            "resetting env. episode 551.000000, reward total was -19.000000. running mean: -20.166514\n",
            "resetting env. episode 552.000000, reward total was -19.000000. running mean: -20.154849\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -20.143300\n",
            "resetting env. episode 554.000000, reward total was -19.000000. running mean: -20.131867\n",
            "resetting env. episode 555.000000, reward total was -19.000000. running mean: -20.120548\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -20.129343\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.138050\n",
            "resetting env. episode 558.000000, reward total was -18.000000. running mean: -20.116669\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.125502\n",
            "resetting env. episode 560.000000, reward total was -19.000000. running mean: -20.114247\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.113105\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.121974\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.130754\n",
            "resetting env. episode 564.000000, reward total was -19.000000. running mean: -20.119447\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.128252\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.126970\n",
            "resetting env. episode 567.000000, reward total was -18.000000. running mean: -20.105700\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.114643\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.123496\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.132261\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.130939\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.129629\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.128333\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -20.127050\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.135779\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.144422\n",
            "resetting env. episode 577.000000, reward total was -19.000000. running mean: -20.132977\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.141648\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.150231\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.148729\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.157241\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.165669\n",
            "resetting env. episode 583.000000, reward total was -19.000000. running mean: -20.154012\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.162472\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.170848\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.169139\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.177448\n",
            "resetting env. episode 588.000000, reward total was -20.000000. running mean: -20.175673\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.173916\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.182177\n",
            "resetting env. episode 591.000000, reward total was -19.000000. running mean: -20.170356\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.178652\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.186865\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.194997\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -20.193047\n",
            "resetting env. episode 596.000000, reward total was -16.000000. running mean: -20.151116\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -20.139605\n",
            "resetting env. episode 598.000000, reward total was -18.000000. running mean: -20.118209\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.127027\n",
            "resetting env. episode 600.000000, reward total was -19.000000. running mean: -20.115757\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.114599\n",
            "resetting env. episode 602.000000, reward total was -19.000000. running mean: -20.103453\n",
            "resetting env. episode 603.000000, reward total was -19.000000. running mean: -20.092419\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.101494\n",
            "resetting env. episode 605.000000, reward total was -20.000000. running mean: -20.100480\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.109475\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.118380\n",
            "resetting env. episode 608.000000, reward total was -18.000000. running mean: -20.097196\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.096224\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.095262\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.094309\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.103366\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.102333\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.111309\n",
            "resetting env. episode 615.000000, reward total was -19.000000. running mean: -20.100196\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.099194\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -20.098202\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.097220\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.096248\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.095286\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.104333\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.103289\n",
            "resetting env. episode 623.000000, reward total was -18.000000. running mean: -20.082257\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -20.071434\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.080720\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.089912\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.099013\n",
            "resetting env. episode 628.000000, reward total was -19.000000. running mean: -20.088023\n",
            "resetting env. episode 629.000000, reward total was -19.000000. running mean: -20.077143\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.086371\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.085508\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -20.084653\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.093806\n",
            "resetting env. episode 634.000000, reward total was -19.000000. running mean: -20.082868\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.082039\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.091219\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.090307\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.099404\n",
            "resetting env. episode 639.000000, reward total was -19.000000. running mean: -20.088410\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.087526\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.086650\n",
            "resetting env. episode 642.000000, reward total was -19.000000. running mean: -20.075784\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.085026\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.094176\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -20.083234\n",
            "resetting env. episode 646.000000, reward total was -15.000000. running mean: -20.032402\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -20.022078\n",
            "resetting env. episode 648.000000, reward total was -20.000000. running mean: -20.021857\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -20.011638\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.021522\n",
            "resetting env. episode 651.000000, reward total was -19.000000. running mean: -20.011307\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.021194\n",
            "resetting env. episode 653.000000, reward total was -19.000000. running mean: -20.010982\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.020872\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -20.010663\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.020557\n",
            "resetting env. episode 657.000000, reward total was -19.000000. running mean: -20.010351\n",
            "resetting env. episode 658.000000, reward total was -19.000000. running mean: -20.000247\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.010245\n",
            "resetting env. episode 660.000000, reward total was -19.000000. running mean: -20.000143\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.000141\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -19.990140\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.000238\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.010236\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.010134\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.010032\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.009932\n",
            "resetting env. episode 668.000000, reward total was -18.000000. running mean: -19.989833\n",
            "resetting env. episode 669.000000, reward total was -17.000000. running mean: -19.959934\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -19.960335\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -19.970732\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -19.971024\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -19.971314\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -19.971601\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -19.971885\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -19.982166\n",
            "resetting env. episode 677.000000, reward total was -19.000000. running mean: -19.972344\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -19.982621\n",
            "resetting env. episode 679.000000, reward total was -19.000000. running mean: -19.972795\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -19.983067\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -19.993236\n",
            "resetting env. episode 682.000000, reward total was -19.000000. running mean: -19.983304\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -19.993471\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.003536\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.013501\n",
            "resetting env. episode 686.000000, reward total was -18.000000. running mean: -19.993366\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.003432\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.013398\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.023264\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.023031\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.022801\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.032573\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.042247\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.051825\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.061306\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.070693\n",
            "resetting env. episode 697.000000, reward total was -19.000000. running mean: -20.059986\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.069386\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.078693\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.087906\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.097027\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.096056\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.105096\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.104045\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.103004\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.091974\n",
            "resetting env. episode 707.000000, reward total was -19.000000. running mean: -20.081055\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.080244\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.089442\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.088547\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.087662\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.096785\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.095817\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.094859\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.093910\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -20.092971\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.102042\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.111021\n",
            "resetting env. episode 719.000000, reward total was -19.000000. running mean: -20.099911\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.108912\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -20.107823\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.106745\n",
            "resetting env. episode 723.000000, reward total was -19.000000. running mean: -20.095677\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.104720\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -20.103673\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.102636\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.111610\n",
            "resetting env. episode 728.000000, reward total was -19.000000. running mean: -20.100494\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.109489\n",
            "resetting env. episode 730.000000, reward total was -19.000000. running mean: -20.098394\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -20.087410\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.096536\n",
            "resetting env. episode 733.000000, reward total was -20.000000. running mean: -20.095571\n",
            "resetting env. episode 734.000000, reward total was -19.000000. running mean: -20.084615\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.083769\n",
            "resetting env. episode 736.000000, reward total was -19.000000. running mean: -20.072931\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -20.072202\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.081480\n",
            "resetting env. episode 739.000000, reward total was -19.000000. running mean: -20.070665\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.079958\n",
            "resetting env. episode 741.000000, reward total was -18.000000. running mean: -20.059159\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.068567\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.067882\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.077203\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.086431\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.095566\n",
            "resetting env. episode 747.000000, reward total was -19.000000. running mean: -20.084611\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -20.083765\n",
            "resetting env. episode 749.000000, reward total was -19.000000. running mean: -20.072927\n",
            "resetting env. episode 750.000000, reward total was -19.000000. running mean: -20.062198\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.061576\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.060960\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.070350\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.079647\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -20.068850\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.068162\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.077480\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.086705\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.095838\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.094880\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.103931\n",
            "resetting env. episode 762.000000, reward total was -19.000000. running mean: -20.092892\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.101963\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.100943\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -20.089934\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -20.079035\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -20.078244\n",
            "resetting env. episode 768.000000, reward total was -19.000000. running mean: -20.067462\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.076787\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.076019\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.085259\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.094407\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.103462\n",
            "resetting env. episode 774.000000, reward total was -19.000000. running mean: -20.092428\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.101504\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.100489\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.109484\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.108389\n",
            "resetting env. episode 779.000000, reward total was -18.000000. running mean: -20.087305\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.096432\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.105468\n",
            "resetting env. episode 782.000000, reward total was -19.000000. running mean: -20.094413\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.093469\n",
            "resetting env. episode 784.000000, reward total was -19.000000. running mean: -20.082534\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.091709\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.100792\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.109784\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.118686\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -20.117499\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.126324\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.135061\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.143710\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.152273\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -20.150750\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.159243\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.167650\n",
            "resetting env. episode 797.000000, reward total was -16.000000. running mean: -20.125974\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.134714\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -20.133367\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.142033\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.140613\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.149207\n",
            "resetting env. episode 803.000000, reward total was -20.000000. running mean: -20.147715\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.156238\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -20.144675\n",
            "resetting env. episode 806.000000, reward total was -19.000000. running mean: -20.133229\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.131896\n",
            "resetting env. episode 808.000000, reward total was -20.000000. running mean: -20.130577\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.139272\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.137879\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.136500\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.135135\n",
            "resetting env. episode 813.000000, reward total was -19.000000. running mean: -20.123784\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.132546\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.131220\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.139908\n",
            "resetting env. episode 817.000000, reward total was -18.000000. running mean: -20.118509\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.117324\n",
            "resetting env. episode 819.000000, reward total was -18.000000. running mean: -20.096151\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.095189\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.104237\n",
            "resetting env. episode 822.000000, reward total was -19.000000. running mean: -20.093195\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.102263\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.101240\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.100228\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.109226\n",
            "resetting env. episode 827.000000, reward total was -19.000000. running mean: -20.098133\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.107152\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.116081\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.124920\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -20.113671\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.112534\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.111409\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.110294\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -20.109192\n",
            "resetting env. episode 836.000000, reward total was -19.000000. running mean: -20.098100\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.107119\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.116047\n",
            "resetting env. episode 839.000000, reward total was -19.000000. running mean: -20.104887\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.103838\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.112800\n",
            "resetting env. episode 842.000000, reward total was -20.000000. running mean: -20.111672\n",
            "resetting env. episode 843.000000, reward total was -19.000000. running mean: -20.100555\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.109549\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -20.108454\n",
            "resetting env. episode 846.000000, reward total was -19.000000. running mean: -20.097369\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.096396\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.095432\n",
            "resetting env. episode 849.000000, reward total was -18.000000. running mean: -20.074477\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.083733\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.092895\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.101966\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -20.100947\n",
            "resetting env. episode 854.000000, reward total was -19.000000. running mean: -20.089937\n",
            "resetting env. episode 855.000000, reward total was -19.000000. running mean: -20.079038\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.088248\n",
            "resetting env. episode 857.000000, reward total was -19.000000. running mean: -20.077365\n",
            "resetting env. episode 858.000000, reward total was -19.000000. running mean: -20.066591\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.065925\n",
            "resetting env. episode 860.000000, reward total was -19.000000. running mean: -20.055266\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.064714\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.074066\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.083326\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.092492\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.101568\n",
            "resetting env. episode 866.000000, reward total was -17.000000. running mean: -20.070552\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.079846\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.079048\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.088257\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.087375\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.086501\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.085636\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.094780\n",
            "resetting env. episode 874.000000, reward total was -18.000000. running mean: -20.073832\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.073094\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.082363\n",
            "resetting env. episode 877.000000, reward total was -20.000000. running mean: -20.081539\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.090724\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.099816\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.108818\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -20.107730\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.106653\n",
            "resetting env. episode 883.000000, reward total was -17.000000. running mean: -20.075586\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.084830\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -20.083982\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.093142\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.092211\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.101289\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -20.090276\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -20.089373\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.098479\n",
            "resetting env. episode 892.000000, reward total was -19.000000. running mean: -20.087495\n",
            "resetting env. episode 893.000000, reward total was -17.000000. running mean: -20.056620\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -20.056053\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.065493\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.074838\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.084090\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.083249\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -20.082416\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.091592\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.090676\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.099769\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.108772\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.107684\n",
            "resetting env. episode 905.000000, reward total was -20.000000. running mean: -20.106607\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.115541\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.124386\n",
            "resetting env. episode 908.000000, reward total was -19.000000. running mean: -20.113142\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.122010\n",
            "resetting env. episode 910.000000, reward total was -19.000000. running mean: -20.110790\n",
            "resetting env. episode 911.000000, reward total was -19.000000. running mean: -20.099682\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.108686\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -20.097599\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.106623\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.105556\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.104501\n",
            "resetting env. episode 917.000000, reward total was -19.000000. running mean: -20.093456\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.092521\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -20.091596\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.100680\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.109673\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.118577\n",
            "resetting env. episode 923.000000, reward total was -19.000000. running mean: -20.107391\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -20.106317\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.105254\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.114201\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.123059\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.131829\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.130510\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.139205\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.137813\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.136435\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -20.125071\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.133820\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.142482\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.141057\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.139646\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.138250\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.136867\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.145499\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.144044\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.142603\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.151177\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.149666\n",
            "resetting env. episode 945.000000, reward total was -18.000000. running mean: -20.128169\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.126887\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.135618\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.134262\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.132920\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.141590\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -20.140174\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.148773\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -20.147285\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -20.145812\n",
            "resetting env. episode 955.000000, reward total was -19.000000. running mean: -20.134354\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.143010\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.141580\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.150165\n",
            "resetting env. episode 959.000000, reward total was -18.000000. running mean: -20.128663\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.137376\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.146002\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.154542\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.152997\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -20.141467\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -20.140052\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.148652\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -20.147165\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.145694\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.154237\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.162694\n",
            "resetting env. episode 971.000000, reward total was -17.000000. running mean: -20.131067\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -20.129757\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -20.128459\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.137175\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.145803\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.154345\n",
            "resetting env. episode 977.000000, reward total was -18.000000. running mean: -20.132801\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.141473\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -20.140059\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.138658\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.147271\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -20.145799\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.154341\n",
            "resetting env. episode 984.000000, reward total was -19.000000. running mean: -20.142797\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.151369\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.159856\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.168257\n",
            "resetting env. episode 988.000000, reward total was -16.000000. running mean: -20.126575\n",
            "resetting env. episode 989.000000, reward total was -19.000000. running mean: -20.115309\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -20.114156\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -20.103014\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.101984\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.110964\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.119855\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.128656\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.137369\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.145996\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.144536\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.153090\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.151560\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.150044\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.158544\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -20.156958\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.145388\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.143935\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.142495\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.151070\n",
            "resetting env. episode 1008.000000, reward total was -19.000000. running mean: -20.139560\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.138164\n",
            "resetting env. episode 1010.000000, reward total was -19.000000. running mean: -20.126782\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -20.125515\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.124259\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.133017\n",
            "resetting env. episode 1014.000000, reward total was -20.000000. running mean: -20.131687\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.130370\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.129066\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.127775\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -20.116498\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -20.115333\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -20.114179\n",
            "resetting env. episode 1021.000000, reward total was -19.000000. running mean: -20.103038\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.112007\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.120887\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.129678\n",
            "resetting env. episode 1025.000000, reward total was -17.000000. running mean: -20.098381\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.097398\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.106424\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -20.095359\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -20.094406\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.103462\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.112427\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.121303\n",
            "resetting env. episode 1033.000000, reward total was -19.000000. running mean: -20.110090\n",
            "resetting env. episode 1034.000000, reward total was -19.000000. running mean: -20.098989\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.097999\n",
            "resetting env. episode 1036.000000, reward total was -19.000000. running mean: -20.087019\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -20.076149\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.075387\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.084634\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -20.083787\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -20.082949\n",
            "resetting env. episode 1042.000000, reward total was -19.000000. running mean: -20.072120\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -20.071399\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.080685\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.079878\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.089079\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.088188\n",
            "resetting env. episode 1048.000000, reward total was -17.000000. running mean: -20.057306\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.066733\n",
            "resetting env. episode 1050.000000, reward total was -19.000000. running mean: -20.056066\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.065505\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.064850\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.074202\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -20.073460\n",
            "resetting env. episode 1055.000000, reward total was -19.000000. running mean: -20.062725\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.062098\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.071477\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -20.070762\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.080054\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -20.079254\n",
            "resetting env. episode 1061.000000, reward total was -17.000000. running mean: -20.048461\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.047977\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.047497\n",
            "resetting env. episode 1064.000000, reward total was -20.000000. running mean: -20.047022\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.056552\n",
            "resetting env. episode 1066.000000, reward total was -18.000000. running mean: -20.035986\n",
            "resetting env. episode 1067.000000, reward total was -19.000000. running mean: -20.025626\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.035370\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.045016\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.054566\n",
            "resetting env. episode 1071.000000, reward total was -19.000000. running mean: -20.044021\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.053580\n",
            "resetting env. episode 1073.000000, reward total was -19.000000. running mean: -20.043045\n",
            "resetting env. episode 1074.000000, reward total was -19.000000. running mean: -20.032614\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.042288\n",
            "resetting env. episode 1076.000000, reward total was -20.000000. running mean: -20.041865\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.051447\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.060932\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -20.060323\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.069720\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -20.069022\n",
            "resetting env. episode 1082.000000, reward total was -19.000000. running mean: -20.058332\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.067749\n",
            "resetting env. episode 1084.000000, reward total was -20.000000. running mean: -20.067071\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -20.066401\n",
            "resetting env. episode 1086.000000, reward total was -18.000000. running mean: -20.045737\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.055279\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.064726\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -20.054079\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -20.053538\n",
            "resetting env. episode 1091.000000, reward total was -18.000000. running mean: -20.033003\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.022673\n",
            "resetting env. episode 1093.000000, reward total was -19.000000. running mean: -20.012446\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -20.012322\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.022199\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.031977\n",
            "resetting env. episode 1097.000000, reward total was -19.000000. running mean: -20.021657\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.031440\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -20.031126\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.030815\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.030506\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.040201\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -20.029799\n",
            "resetting env. episode 1104.000000, reward total was -19.000000. running mean: -20.019501\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.029306\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.039013\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.048623\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.058137\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.067556\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.076880\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.086111\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.085250\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.094398\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.103454\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.112419\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -20.111295\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.120182\n",
            "resetting env. episode 1118.000000, reward total was -17.000000. running mean: -20.088980\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.098090\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.097109\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -20.096138\n",
            "resetting env. episode 1122.000000, reward total was -18.000000. running mean: -20.075177\n",
            "resetting env. episode 1123.000000, reward total was -17.000000. running mean: -20.044425\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.053981\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.053441\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.062907\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.072278\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -20.061555\n",
            "resetting env. episode 1129.000000, reward total was -19.000000. running mean: -20.050939\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -20.040430\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.050026\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -20.049525\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.059030\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.058440\n",
            "resetting env. episode 1135.000000, reward total was -19.000000. running mean: -20.047855\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.047377\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.056903\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.056334\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.055771\n",
            "resetting env. episode 1140.000000, reward total was -19.000000. running mean: -20.045213\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.054761\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -20.054213\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.063671\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.063034\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.072404\n",
            "resetting env. episode 1146.000000, reward total was -18.000000. running mean: -20.051680\n",
            "resetting env. episode 1147.000000, reward total was -17.000000. running mean: -20.021163\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.020952\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.030742\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.030435\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.040130\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.039729\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.039332\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.048938\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.058449\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.067865\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -20.067186\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.076514\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.075749\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -20.074991\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.084241\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.093399\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -20.092465\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -20.081540\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -20.080725\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.079918\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.089119\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -20.088227\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.087345\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.096472\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.105507\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.114452\n",
            "resetting env. episode 1173.000000, reward total was -19.000000. running mean: -20.103307\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.102274\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.111252\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.110139\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.119038\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.127847\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.136569\n",
            "resetting env. episode 1180.000000, reward total was -20.000000. running mean: -20.135203\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.143851\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -20.142413\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.150988\n",
            "resetting env. episode 1184.000000, reward total was -20.000000. running mean: -20.149479\n",
            "resetting env. episode 1185.000000, reward total was -19.000000. running mean: -20.137984\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.146604\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.155138\n",
            "resetting env. episode 1188.000000, reward total was -18.000000. running mean: -20.133587\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.142251\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -20.140828\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.149420\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.157926\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -20.156346\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.154783\n",
            "resetting env. episode 1195.000000, reward total was -20.000000. running mean: -20.153235\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.161703\n",
            "resetting env. episode 1197.000000, reward total was -18.000000. running mean: -20.140086\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.148685\n",
            "resetting env. episode 1199.000000, reward total was -19.000000. running mean: -20.137198\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -20.135826\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.144468\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -20.143023\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -20.131593\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -20.130277\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.138974\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.147584\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.156109\n",
            "resetting env. episode 1208.000000, reward total was -19.000000. running mean: -20.144548\n",
            "resetting env. episode 1209.000000, reward total was -18.000000. running mean: -20.123102\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -20.121871\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.130652\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -20.129346\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -20.128052\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.136772\n",
            "resetting env. episode 1215.000000, reward total was -19.000000. running mean: -20.125404\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -20.134150\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.132809\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.141480\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -20.140066\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.148665\n",
            "resetting env. episode 1221.000000, reward total was -18.000000. running mean: -20.127178\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -20.115907\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -20.104747\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.113700\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.112563\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.111437\n",
            "resetting env. episode 1227.000000, reward total was -19.000000. running mean: -20.100323\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -20.099320\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -20.098327\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.107343\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.116270\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.115107\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.123956\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -20.122717\n",
            "resetting env. episode 1235.000000, reward total was -20.000000. running mean: -20.121489\n",
            "resetting env. episode 1236.000000, reward total was -19.000000. running mean: -20.110274\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -20.109172\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -20.108080\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.116999\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.125829\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.134571\n",
            "resetting env. episode 1242.000000, reward total was -20.000000. running mean: -20.133225\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.141893\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.140474\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.149069\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.157579\n",
            "resetting env. episode 1247.000000, reward total was -19.000000. running mean: -20.146003\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.154543\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.162997\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.171367\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.179654\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.187857\n",
            "resetting env. episode 1253.000000, reward total was -19.000000. running mean: -20.175979\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.184219\n",
            "resetting env. episode 1255.000000, reward total was -19.000000. running mean: -20.172377\n",
            "resetting env. episode 1256.000000, reward total was -17.000000. running mean: -20.140653\n",
            "resetting env. episode 1257.000000, reward total was -19.000000. running mean: -20.129246\n",
            "resetting env. episode 1258.000000, reward total was -20.000000. running mean: -20.127954\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -20.126674\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.125408\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.134154\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -20.132812\n",
            "resetting env. episode 1263.000000, reward total was -18.000000. running mean: -20.111484\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -20.110369\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.119265\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.128073\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.136792\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.145424\n",
            "resetting env. episode 1269.000000, reward total was -19.000000. running mean: -20.133970\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.142630\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.141204\n",
            "resetting env. episode 1272.000000, reward total was -18.000000. running mean: -20.119792\n",
            "resetting env. episode 1273.000000, reward total was -19.000000. running mean: -20.108594\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.117508\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.116333\n",
            "resetting env. episode 1276.000000, reward total was -19.000000. running mean: -20.105170\n",
            "resetting env. episode 1277.000000, reward total was -19.000000. running mean: -20.094118\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.103177\n",
            "resetting env. episode 1279.000000, reward total was -19.000000. running mean: -20.092145\n",
            "resetting env. episode 1280.000000, reward total was -19.000000. running mean: -20.081223\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -20.070411\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -20.069707\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.079010\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.088220\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.087338\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.096464\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -20.095500\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.094545\n",
            "resetting env. episode 1289.000000, reward total was -19.000000. running mean: -20.083599\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.082763\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.091936\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -20.091016\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.100106\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.109105\n",
            "resetting env. episode 1295.000000, reward total was -19.000000. running mean: -20.098014\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.107034\n",
            "resetting env. episode 1297.000000, reward total was -19.000000. running mean: -20.095964\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -20.085004\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.094154\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -20.093212\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -20.092280\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.101357\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -20.100344\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.109340\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -20.108247\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -20.107164\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -20.106093\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.115032\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.123882\n",
            "resetting env. episode 1310.000000, reward total was -19.000000. running mean: -20.112643\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.121516\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -20.120301\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.129098\n",
            "resetting env. episode 1314.000000, reward total was -20.000000. running mean: -20.127807\n",
            "resetting env. episode 1315.000000, reward total was -18.000000. running mean: -20.106529\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.115464\n",
            "resetting env. episode 1317.000000, reward total was -19.000000. running mean: -20.104309\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.113266\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.122133\n",
            "resetting env. episode 1320.000000, reward total was -19.000000. running mean: -20.110912\n",
            "resetting env. episode 1321.000000, reward total was -19.000000. running mean: -20.099803\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.108805\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.117717\n",
            "resetting env. episode 1324.000000, reward total was -18.000000. running mean: -20.096540\n",
            "resetting env. episode 1325.000000, reward total was -18.000000. running mean: -20.075574\n",
            "resetting env. episode 1326.000000, reward total was -17.000000. running mean: -20.044819\n",
            "resetting env. episode 1327.000000, reward total was -19.000000. running mean: -20.034370\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -20.034027\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -20.033686\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -20.033350\n",
            "resetting env. episode 1331.000000, reward total was -19.000000. running mean: -20.023016\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.032786\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -20.032458\n",
            "resetting env. episode 1334.000000, reward total was -18.000000. running mean: -20.012133\n",
            "resetting env. episode 1335.000000, reward total was -19.000000. running mean: -20.002012\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.001992\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.011972\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.021852\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.021634\n",
            "resetting env. episode 1340.000000, reward total was -19.000000. running mean: -20.011418\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.021303\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.031090\n",
            "resetting env. episode 1343.000000, reward total was -20.000000. running mean: -20.030779\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.040472\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.050067\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.059566\n",
            "resetting env. episode 1347.000000, reward total was -19.000000. running mean: -20.048971\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.058481\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.057896\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.067317\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.076644\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.085877\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.095019\n",
            "resetting env. episode 1354.000000, reward total was -19.000000. running mean: -20.084069\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -20.083228\n",
            "resetting env. episode 1356.000000, reward total was -19.000000. running mean: -20.072396\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.071672\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.070955\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.080245\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.089443\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.098548\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.107563\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.106487\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -20.095422\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -20.084468\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.083624\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.092787\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -20.091859\n",
            "resetting env. episode 1369.000000, reward total was -19.000000. running mean: -20.080941\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.090131\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -20.089230\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.088338\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.087454\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.096580\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -20.095614\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.104658\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.113611\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.122475\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -20.121251\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.130038\n",
            "resetting env. episode 1381.000000, reward total was -18.000000. running mean: -20.108738\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.117650\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -20.116474\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.125309\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.124056\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.122815\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.121587\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -20.120371\n",
            "resetting env. episode 1389.000000, reward total was -19.000000. running mean: -20.109168\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.118076\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -20.116895\n",
            "resetting env. episode 1392.000000, reward total was -18.000000. running mean: -20.095726\n",
            "resetting env. episode 1393.000000, reward total was -19.000000. running mean: -20.084769\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.093921\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.102982\n",
            "resetting env. episode 1396.000000, reward total was -20.000000. running mean: -20.101952\n",
            "resetting env. episode 1397.000000, reward total was -19.000000. running mean: -20.090933\n",
            "resetting env. episode 1398.000000, reward total was -19.000000. running mean: -20.080023\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.089223\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.098331\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.097348\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.106374\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.115310\n",
            "resetting env. episode 1404.000000, reward total was -18.000000. running mean: -20.094157\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.103216\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.112184\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.121062\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.119851\n",
            "resetting env. episode 1409.000000, reward total was -18.000000. running mean: -20.098653\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.107666\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.116589\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.125424\n",
            "resetting env. episode 1413.000000, reward total was -19.000000. running mean: -20.114169\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -20.103028\n",
            "resetting env. episode 1415.000000, reward total was -19.000000. running mean: -20.091997\n",
            "resetting env. episode 1416.000000, reward total was -20.000000. running mean: -20.091077\n",
            "resetting env. episode 1417.000000, reward total was -18.000000. running mean: -20.070167\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.079465\n",
            "resetting env. episode 1419.000000, reward total was -19.000000. running mean: -20.068670\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -20.077984\n",
            "resetting env. episode 1421.000000, reward total was -19.000000. running mean: -20.067204\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -20.066532\n",
            "resetting env. episode 1423.000000, reward total was -18.000000. running mean: -20.045866\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.055408\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.064854\n",
            "resetting env. episode 1426.000000, reward total was -19.000000. running mean: -20.054205\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.053663\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.063126\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.072495\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -20.071770\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.071052\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -20.070342\n",
            "resetting env. episode 1433.000000, reward total was -19.000000. running mean: -20.059639\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.069042\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -20.068352\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.067668\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -20.066992\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.076322\n",
            "resetting env. episode 1439.000000, reward total was -18.000000. running mean: -20.055558\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.065003\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.064353\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.073709\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -20.072972\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -20.072242\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.071520\n",
            "resetting env. episode 1446.000000, reward total was -19.000000. running mean: -20.060805\n",
            "resetting env. episode 1447.000000, reward total was -19.000000. running mean: -20.050197\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -20.039695\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.039298\n",
            "resetting env. episode 1450.000000, reward total was -19.000000. running mean: -20.028905\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -20.018616\n",
            "resetting env. episode 1452.000000, reward total was -20.000000. running mean: -20.018430\n",
            "resetting env. episode 1453.000000, reward total was -19.000000. running mean: -20.008245\n",
            "resetting env. episode 1454.000000, reward total was -18.000000. running mean: -19.988163\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -19.988281\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -19.998398\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.008415\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -20.008330\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.018247\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -20.018065\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.027884\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.037605\n",
            "resetting env. episode 1463.000000, reward total was -19.000000. running mean: -20.027229\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -20.026957\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -20.016687\n",
            "resetting env. episode 1466.000000, reward total was -17.000000. running mean: -19.986520\n",
            "resetting env. episode 1467.000000, reward total was -19.000000. running mean: -19.976655\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -19.986889\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -19.977020\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -19.977249\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -19.977477\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -19.977702\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -19.967925\n",
            "resetting env. episode 1474.000000, reward total was -19.000000. running mean: -19.958246\n",
            "resetting env. episode 1475.000000, reward total was -19.000000. running mean: -19.948663\n",
            "resetting env. episode 1476.000000, reward total was -18.000000. running mean: -19.929177\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -19.939885\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.950486\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -19.960981\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -19.961372\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -19.971758\n",
            "resetting env. episode 1482.000000, reward total was -19.000000. running mean: -19.962040\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -19.962420\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -19.962796\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -19.973168\n",
            "resetting env. episode 1486.000000, reward total was -17.000000. running mean: -19.943436\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -19.954002\n",
            "resetting env. episode 1488.000000, reward total was -19.000000. running mean: -19.944462\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -19.945017\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -19.955567\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -19.966011\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -19.956351\n",
            "resetting env. episode 1493.000000, reward total was -18.000000. running mean: -19.936788\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -19.937420\n",
            "resetting env. episode 1495.000000, reward total was -19.000000. running mean: -19.928045\n",
            "resetting env. episode 1496.000000, reward total was -19.000000. running mean: -19.918765\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -19.919577\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -19.930382\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -19.931078\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -19.921767\n",
            "CPU times: user 2h 50min 50s, sys: 45min 29s, total: 3h 36min 19s\n",
            "Wall time: 1h 52min 23s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "w2NblmwDsL3y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "48cc5444-bd8a-4a66-bc90-b783050e5caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Iteration-4 H=400 1e-4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}