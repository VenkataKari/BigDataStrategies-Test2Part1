{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "db297f19-4471-4543-9108-8686e4c59fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 22.9 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=9031dadf778a5ef6f4eed540e0942807e1bc040d6bd3d11bbb6b3e6f1911e0c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "39fdcd2f-c91a-4492-8dfe-3d7ef64db923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "701a0adb-46bd-4090-c78c-d54a225cb887"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "67b0107f-eeb3-43fe-cd24-7c3c373a735a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "3ce0cd8e-fdaf-42bf-9ed5-12901a640709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 600 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "8b719e07-b3b0-437c-accf-f5e1c957e6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -20.970100\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.970399\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.960695\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.961088\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.961477\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.941862\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.942444\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.943019\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.933589\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.934253\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.924911\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.925662\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.906405\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.897341\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.898368\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.889384\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.890490\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.881585\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.882769\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.883942\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.875102\n",
            "resetting env. episode 26.000000, reward total was -19.000000. running mean: -20.856351\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.857788\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.859210\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.860618\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.862011\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.853391\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -20.824857\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.826609\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.828343\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.830059\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.811759\n",
            "resetting env. episode 37.000000, reward total was -17.000000. running mean: -20.773641\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.775905\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.768146\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.760464\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.762860\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.765231\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.767579\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.769903\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.772204\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.774482\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.766737\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.759070\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.741479\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.744064\n",
            "resetting env. episode 51.000000, reward total was -18.000000. running mean: -20.716624\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.719457\n",
            "resetting env. episode 53.000000, reward total was -18.000000. running mean: -20.692263\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.685340\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.688487\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.691602\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -20.674686\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.677939\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.681160\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.684348\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.677505\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -20.660729\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.654122\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.647581\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.651105\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.654594\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.648048\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.651568\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.645052\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.648601\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.642115\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -20.625694\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.629437\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.633143\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -20.616812\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.610643\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.614537\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.618392\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.612208\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.616086\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.619925\n",
            "resetting env. episode 82.000000, reward total was -18.000000. running mean: -20.593726\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.597788\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.591810\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.585892\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.580033\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.584233\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.578391\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.582607\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.586781\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.570913\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.575204\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.569452\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.563757\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.568120\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.562438\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.566814\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.551146\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.535634\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.520278\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.525075\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.519825\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.514626\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.519480\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.514285\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.509142\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.514051\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.508911\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.513821\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.498683\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.503696\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.508659\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.513573\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.518437\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.523253\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.528020\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.532740\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.527413\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.532138\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.536817\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.541449\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.536034\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -20.520674\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.505467\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.500413\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.505409\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.510354\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.515251\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.510098\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.504997\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.509947\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.514848\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.519699\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.514502\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.519357\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.514164\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.509022\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.513932\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.498793\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.503805\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.488767\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.493879\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.498940\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.493951\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.489011\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.484121\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.469280\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.474587\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.479841\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.485043\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.490193\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.485291\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.490438\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.495533\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.500578\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.495572\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.490616\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.495710\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -20.480753\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.485946\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.491086\n",
            "resetting env. episode 162.000000, reward total was -18.000000. running mean: -20.466175\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.471514\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.476798\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.482030\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.487210\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.492338\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.497415\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.502441\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.497416\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.482442\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.487618\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.482741\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.477914\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.483135\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.488303\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.493420\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.498486\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.503501\n",
            "resetting env. episode 180.000000, reward total was -18.000000. running mean: -20.478466\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.473682\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.478945\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.484155\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.489314\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.494421\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.489477\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.494582\n",
            "resetting env. episode 188.000000, reward total was -18.000000. running mean: -20.469636\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.464940\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.470290\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.475587\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.480831\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.486023\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.491163\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.496251\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.481289\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.476476\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.461711\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.457094\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.462523\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.467898\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.473219\n",
            "resetting env. episode 203.000000, reward total was -18.000000. running mean: -20.448487\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.444002\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.449562\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.455066\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.450515\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.446010\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.451550\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.457035\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.462464\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.467840\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -20.443161\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.438730\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.444342\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.429899\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.435600\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.441244\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.436832\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.432463\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.428139\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.433857\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.439519\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.435123\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.440772\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.446365\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.451901\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.457382\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.452808\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.458280\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.443697\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.449260\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.444768\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.440320\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.425917\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.421658\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.427441\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.433167\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.438835\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.444447\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.450002\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.435502\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.441147\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.446736\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.452268\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.447746\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.453268\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.458735\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.454148\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.459607\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.465010\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.460360\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.455757\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.461199\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.466587\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.461921\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.457302\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.462729\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.468102\n",
            "resetting env. episode 260.000000, reward total was -17.000000. running mean: -20.433421\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.439087\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.444696\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.440249\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.445846\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.451388\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.456874\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.462305\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.457682\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.453105\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.458574\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.463989\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.469349\n",
            "resetting env. episode 273.000000, reward total was -18.000000. running mean: -20.444655\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.440209\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.435807\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.441448\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.427034\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.422764\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.428536\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.424251\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.420008\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.425808\n",
            "resetting env. episode 283.000000, reward total was -19.000000. running mean: -20.411550\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.407434\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.413360\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.419227\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.405034\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.410984\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.416874\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.422705\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.428478\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.434194\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.429852\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.435553\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.441198\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.446786\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.452318\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.447795\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.453317\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.448783\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.444296\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.429853\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.435554\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.431199\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.416887\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.412718\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.408591\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.414505\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.420360\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.426156\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.431894\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.437575\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.433200\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.438868\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.444479\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.450034\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.445534\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.451079\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -20.436568\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.442202\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.437780\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.443402\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.428968\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.434679\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.440332\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.425928\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.431669\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.437352\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.432979\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.418649\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.414463\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.410318\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.406215\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.392153\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.398231\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.404249\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.390206\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.396304\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.402341\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.388318\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.394435\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.400490\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.406485\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.402421\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.408396\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.414312\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.410169\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.416068\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.421907\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.407688\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.403611\n",
            "resetting env. episode 352.000000, reward total was -17.000000. running mean: -20.369575\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.375879\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.382120\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.388299\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.394416\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.390472\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.386567\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.382702\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.388875\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.384986\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.371136\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.357425\n",
            "resetting env. episode 364.000000, reward total was -17.000000. running mean: -20.323850\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.330612\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.337306\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.343933\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.350493\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.346988\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.353519\n",
            "resetting env. episode 371.000000, reward total was -17.000000. running mean: -20.319983\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.326784\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.333516\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.340181\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.346779\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.333311\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.329978\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.326678\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.323411\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.320177\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.326975\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.333706\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.340369\n",
            "resetting env. episode 384.000000, reward total was -18.000000. running mean: -20.316965\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.323795\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.330557\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.327252\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.323979\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.320739\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.327532\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.334257\n",
            "resetting env. episode 392.000000, reward total was -18.000000. running mean: -20.310914\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.317805\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.314627\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.311481\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.318366\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.325182\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.331930\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.328611\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.325325\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.332072\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.328751\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.335463\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.342109\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.348688\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.345201\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.351749\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.358231\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.364649\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.371003\n",
            "resetting env. episode 411.000000, reward total was -18.000000. running mean: -20.347293\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.353820\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.360281\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.366679\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.373012\n",
            "resetting env. episode 416.000000, reward total was -18.000000. running mean: -20.349282\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.345789\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.342331\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.348908\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.355419\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.361864\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.358246\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.364663\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.371017\n",
            "resetting env. episode 425.000000, reward total was -18.000000. running mean: -20.347307\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.343833\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.350395\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.356891\n",
            "resetting env. episode 429.000000, reward total was -18.000000. running mean: -20.333322\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.329989\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.326689\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.323422\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.320188\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.326986\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.323716\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.330479\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.337174\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.343803\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.340365\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.336961\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.343591\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.350155\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.356654\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.353087\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.349556\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.356061\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.352500\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.348975\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.345486\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.342031\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.348610\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.355124\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.351573\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.358057\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.364477\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.360832\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.367224\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.373551\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.359816\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.366218\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.372556\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.378830\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.385042\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.391191\n",
            "resetting env. episode 465.000000, reward total was -18.000000. running mean: -20.367279\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.373607\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.379870\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.386072\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.392211\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.398289\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.404306\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.390263\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.396360\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.402397\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.408373\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.414289\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.410146\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.416045\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.421884\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.427665\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.423389\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.429155\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.434863\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.440515\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.446110\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.451648\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.457132\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.442561\n",
            "resetting env. episode 489.000000, reward total was -18.000000. running mean: -20.418135\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.413954\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.419814\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.425616\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.431360\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.427046\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.432776\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.438448\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.434064\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.429723\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.415426\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.411271\n",
            "CPU times: user 51min 47s, sys: 12min 53s, total: 1h 4min 40s\n",
            "Wall time: 33min 39s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "a2ed41c4-4216-466c-8cd0-7923dbb5086d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.019701\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.019504\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.019309\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.029116\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.038825\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.048436\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.057952\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.067373\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.076699\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.085932\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.085073\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.094222\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.103280\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.112247\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.101124\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.100113\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.099112\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.108121\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.117040\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.105869\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.104811\n",
            "resetting env. episode 25.000000, reward total was -18.000000. running mean: -20.083762\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.092925\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.101996\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.100976\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.109966\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.118866\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.117678\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -20.096501\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.095536\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.094580\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.093635\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.092698\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.091771\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.090854\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.099945\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.088946\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.098056\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.097076\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.106105\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.115044\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.123893\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.132654\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.141328\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.149915\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.148415\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.156931\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.165362\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.173708\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.181971\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.190152\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.188250\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.196368\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.204404\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.202360\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.210336\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.218233\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.216051\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.213890\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.221751\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.209534\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.207438\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.215364\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.223210\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.220978\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -20.208768\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.206681\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.214614\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.222468\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.230243\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.227941\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.235661\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.243305\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.250872\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -20.238363\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.245979\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.253519\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.260984\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.268374\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.275691\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.282934\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.280104\n",
            "resetting env. episode 86.000000, reward total was -18.000000. running mean: -20.257303\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.254730\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.262183\n",
            "resetting env. episode 89.000000, reward total was -18.000000. running mean: -20.239561\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.247166\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.244694\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.252247\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.259724\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.257127\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.264556\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.261910\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.249291\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.246798\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.254330\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.241787\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.239369\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.246976\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.254506\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.261961\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.269341\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.266648\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.273981\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.281241\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.278429\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.285645\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.292788\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.289860\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.296962\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.293992\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.301052\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.308042\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.314961\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.311812\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.318694\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.315507\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.322352\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.329128\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.335837\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.332478\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.339154\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.345762\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.352304\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.348781\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.355294\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.351741\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.358223\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.354641\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.361095\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.357484\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.363909\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.360270\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.366667\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.363000\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.369370\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.375677\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.381920\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.388101\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.394220\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.390278\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.396375\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.392411\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.388487\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.384602\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.380756\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.376948\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.383179\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.389347\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.395454\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.391499\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.397584\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.393608\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.399672\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.405676\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.411619\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.407503\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.403428\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.399393\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.405399\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.411345\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.417232\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.413060\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.408929\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.394840\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.400891\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.386882\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.373014\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.379283\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.385491\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.371636\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -20.357919\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.364340\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.370697\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.366990\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.373320\n",
            "resetting env. episode 180.000000, reward total was -17.000000. running mean: -20.339587\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.346191\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.342729\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.349302\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.345809\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.352350\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.358827\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.365239\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.371586\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.377870\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.384092\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.390251\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.386348\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.382485\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.368660\n",
            "resetting env. episode 195.000000, reward total was -18.000000. running mean: -20.344973\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.351524\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.358008\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.354428\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.350884\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.347375\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.353901\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.340362\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.346959\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.343489\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.330054\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.326754\n",
            "resetting env. episode 207.000000, reward total was -18.000000. running mean: -20.303486\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.310451\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.317347\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.324173\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.330932\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.337622\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.334246\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.340904\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.337495\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.324120\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.320879\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.327670\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.324393\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.311149\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.308038\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.294957\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.282008\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.289188\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.286296\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.283433\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.280598\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.267792\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.265115\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.272463\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.269739\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.257041\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.264471\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.261826\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.269208\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.266516\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.263851\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.271212\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.268500\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.275815\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.273057\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.270326\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.277623\n",
            "resetting env. episode 244.000000, reward total was -18.000000. running mean: -20.254847\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.262298\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.269675\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.276979\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.264209\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.271567\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.278851\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.276063\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.273302\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.280569\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.287763\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.294886\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.291937\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.289017\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.296127\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.303166\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.300134\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.297133\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.294162\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.301220\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.298208\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -20.285226\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.292373\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.299450\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.296455\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.293491\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.290556\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.287650\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.294774\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.291826\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.298908\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.295919\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.292959\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.300030\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.297030\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.304059\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.311019\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.307909\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.314829\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.321681\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.318464\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.315280\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.322127\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.328906\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.335617\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.322260\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.319038\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.315847\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -20.292689\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.299762\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.306764\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.303697\n",
            "resetting env. episode 296.000000, reward total was -18.000000. running mean: -20.280660\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.267853\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.275175\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.282423\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.279599\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.286803\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.283935\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.291095\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.288184\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.295303\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.302350\n",
            "resetting env. episode 307.000000, reward total was -18.000000. running mean: -20.279326\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.276533\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.283767\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.280930\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.288120\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.295239\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.302287\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.289264\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.296371\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.293408\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.300474\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.307469\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.304394\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.311350\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.318237\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.325054\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.331804\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.338486\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.345101\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.351650\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.348133\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.354652\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.361106\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.357494\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.363920\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.370280\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.376578\n",
            "resetting env. episode 334.000000, reward total was -17.000000. running mean: -20.342812\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.349384\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.355890\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.362331\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.348708\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.355221\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.341668\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.338252\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.344869\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.351420\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.347906\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.344427\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.340983\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.347573\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.354097\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.360556\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.356951\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.353381\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.339847\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.326449\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.333185\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.319853\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.326654\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.333388\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.340054\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.346653\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.353187\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.349655\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.346158\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.332697\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.329370\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.316076\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.322915\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.329686\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.326389\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.333125\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.339794\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.326396\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.323132\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.329901\n",
            "resetting env. episode 374.000000, reward total was -18.000000. running mean: -20.306602\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.303536\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.310500\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.317395\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.324222\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.330979\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.337669\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.324293\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.331050\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.337739\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.344362\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.350918\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.347409\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.333935\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.340596\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.347190\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.333718\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.330381\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.317077\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.323906\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.330667\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.337360\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.323987\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.310747\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.307639\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.294563\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.291617\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.288701\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.295814\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.302856\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.309828\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.316729\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.323562\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.320326\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.327123\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.323852\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.330613\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.337307\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.343934\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.350495\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.356990\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.353420\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.359886\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.366287\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.362624\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.348998\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.335508\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.342153\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.338731\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.325344\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.332090\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.328770\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.335482\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.342127\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.338706\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.345319\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.351866\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.348347\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.354863\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.351315\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.347802\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.344324\n",
            "resetting env. episode 436.000000, reward total was -18.000000. running mean: -20.320880\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.327672\n",
            "resetting env. episode 438.000000, reward total was -18.000000. running mean: -20.304395\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.311351\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.318237\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.325055\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.331804\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.338486\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.335102\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.321751\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.318533\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.315348\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.312194\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.309072\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.315982\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.322822\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.319594\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.326398\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.333134\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.339802\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.346404\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.332940\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.339611\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.346215\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.352753\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.339225\n",
            "resetting env. episode 462.000000, reward total was -18.000000. running mean: -20.315833\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.322674\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.329448\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.336153\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.342792\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.339364\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.345970\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.342510\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.349085\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -20.325594\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.312339\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.319215\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.326023\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.312763\n",
            "resetting env. episode 476.000000, reward total was -17.000000. running mean: -20.279635\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.266839\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.274170\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.271429\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.278714\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.285927\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.273068\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.280337\n",
            "resetting env. episode 484.000000, reward total was -18.000000. running mean: -20.257534\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.254959\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.262409\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.269785\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.267087\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.274416\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.281672\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.288855\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.285967\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.293107\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.290176\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.287274\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.294402\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -20.281458\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.278643\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.285857\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.282998\n",
            "CPU times: user 52min 32s, sys: 12min 59s, total: 1h 5min 32s\n",
            "Wall time: 33min 56s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "4cb803aa-89a2-438c-8d4c-fc7f891db267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "f87a2589-f9f0-4ab8-dfe7-6c5b596357a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980398\n",
            "resetting env. episode 7.000000, reward total was -18.000000. running mean: -20.950594\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951088\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.951577\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.952061\n",
            "resetting env. episode 11.000000, reward total was -18.000000. running mean: -20.922541\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.923315\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.914082\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.914941\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.915792\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.916634\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.907468\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.898393\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.889409\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.880515\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.861710\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.853093\n",
            "resetting env. episode 23.000000, reward total was -18.000000. running mean: -20.824562\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.826316\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.828053\n",
            "resetting env. episode 26.000000, reward total was -18.000000. running mean: -20.799773\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.791775\n",
            "resetting env. episode 28.000000, reward total was -18.000000. running mean: -20.763857\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.766219\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.768556\n",
            "resetting env. episode 31.000000, reward total was -19.000000. running mean: -20.750871\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.743362\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.745928\n",
            "resetting env. episode 34.000000, reward total was -19.000000. running mean: -20.728469\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.721184\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.713973\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.706833\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.709765\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.702667\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.705640\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.708584\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.701498\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.704483\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.697438\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.700464\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.693459\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.676525\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.669759\n",
            "resetting env. episode 49.000000, reward total was -18.000000. running mean: -20.643062\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.646631\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.650165\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.653663\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.647127\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.630655\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.634349\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.628005\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.621725\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.625508\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.619253\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.623060\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.626830\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -20.610561\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.604456\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.588411\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.592527\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.596602\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.600636\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.594629\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.598683\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -20.582696\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.576869\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.571101\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.565390\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.559736\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.564138\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.568497\n",
            "resetting env. episode 77.000000, reward total was -18.000000. running mean: -20.542812\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.547384\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.541910\n",
            "resetting env. episode 80.000000, reward total was -18.000000. running mean: -20.516491\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.511326\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.516213\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.501051\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -20.476040\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.481280\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.486467\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.491602\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.476686\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.471919\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.467200\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.462528\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.457903\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -20.443324\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -20.428891\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.434602\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.430256\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.425953\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.421694\n",
            "resetting env. episode 99.000000, reward total was -18.000000. running mean: -20.397477\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.403502\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.399467\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -20.385472\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.391618\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.397701\n",
            "resetting env. episode 105.000000, reward total was -18.000000. running mean: -20.373724\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.359987\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.356387\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.362823\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.359195\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.365603\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.371947\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.358228\n",
            "resetting env. episode 113.000000, reward total was -19.000000. running mean: -20.344645\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.331199\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.337887\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.344508\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.351063\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.347552\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.354077\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.360536\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.356931\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.363361\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.369728\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.376031\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.372270\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.378548\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.364762\n",
            "resetting env. episode 128.000000, reward total was -16.000000. running mean: -20.321114\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.327903\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.324624\n",
            "resetting env. episode 131.000000, reward total was -19.000000. running mean: -20.311378\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.318264\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.325082\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -20.311831\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.308712\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.315625\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.322469\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.319244\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.306052\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.312991\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.309862\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.316763\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.323595\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.320359\n",
            "resetting env. episode 145.000000, reward total was -18.000000. running mean: -20.297156\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.284184\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.291342\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.298429\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.305445\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.292390\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.279466\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.286672\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.283805\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.280967\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.278157\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.285376\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.282522\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.289697\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.296800\n",
            "resetting env. episode 160.000000, reward total was -17.000000. running mean: -20.263832\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.261193\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.268581\n",
            "resetting env. episode 163.000000, reward total was -17.000000. running mean: -20.235896\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.233537\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.221201\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.218989\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.216799\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.224631\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.222385\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.220161\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.207960\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.215880\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.213721\n",
            "resetting env. episode 174.000000, reward total was -18.000000. running mean: -20.191584\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.199668\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.207671\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.205595\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.213539\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.221403\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.229189\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.226898\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.234629\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.232282\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.229959\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.237660\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.245283\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.242830\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.240402\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.227998\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.225718\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.233461\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.231126\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.238815\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.226427\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.224163\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.211921\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.219802\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.217604\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.225428\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.233173\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.220842\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.228633\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.236347\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.223983\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.211744\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.209626\n",
            "resetting env. episode 207.000000, reward total was -18.000000. running mean: -20.187530\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.185655\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.193798\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.191860\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.199942\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.187942\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.196063\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.194102\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.192161\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.190239\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.198337\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.196354\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.184390\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.182546\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.180721\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.188914\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.197024\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.185054\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.173204\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.181472\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.169657\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.177960\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.186181\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.184319\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.192476\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.190551\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.198645\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.196659\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.204692\n",
            "resetting env. episode 236.000000, reward total was -18.000000. running mean: -20.182645\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.190819\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.198911\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.196922\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.184953\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.173103\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.181372\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.189558\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.197663\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.195686\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.183729\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.191892\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.199973\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.207973\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.215893\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.203735\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.211697\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.219580\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.227384\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.225111\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.222859\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.220631\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.228425\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.216140\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.223979\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.231739\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.239422\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.247028\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.244557\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.252112\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.239591\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.247195\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.234723\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.242375\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.249952\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.247452\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.234978\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.242628\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.250202\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.247700\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.245223\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.252770\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.250243\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.257740\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.255163\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.262611\n",
            "resetting env. episode 282.000000, reward total was -18.000000. running mean: -20.239985\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.247585\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.255109\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.262558\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.269933\n",
            "resetting env. episode 287.000000, reward total was -18.000000. running mean: -20.247233\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.254761\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.252213\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.249691\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.247194\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.244722\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.252275\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.249753\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.247255\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.234782\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.242435\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.240010\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.247610\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.235134\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.242783\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.230355\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.238051\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.245671\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.233214\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.240882\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.248473\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.245988\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.243529\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.241093\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.228682\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.226396\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.214132\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.211990\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.219870\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.227672\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.235395\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.233041\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.230711\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.228403\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.236119\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.233758\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.241421\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.249006\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.246516\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.254051\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.261511\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.258896\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.246307\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.243844\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.251405\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.258891\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.256302\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.253739\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.251202\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.248690\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.246203\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.233741\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.241403\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.228989\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.216699\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.214532\n",
            "resetting env. episode 343.000000, reward total was -17.000000. running mean: -20.182387\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.190563\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.198658\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -20.176671\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.174904\n",
            "resetting env. episode 348.000000, reward total was -19.000000. running mean: -20.163155\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.171524\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.179809\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.178010\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.176230\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.164468\n",
            "resetting env. episode 354.000000, reward total was -17.000000. running mean: -20.132823\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.141495\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.150080\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.148579\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -20.137094\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.145723\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.154265\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.162723\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.151096\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.159585\n",
            "resetting env. episode 364.000000, reward total was -18.000000. running mean: -20.137989\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.146609\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.155143\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.153591\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.162055\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.170435\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.168731\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.157043\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.165473\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.173818\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.172080\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.170359\n",
            "resetting env. episode 376.000000, reward total was -18.000000. running mean: -20.148655\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.157169\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.165597\n",
            "resetting env. episode 379.000000, reward total was -18.000000. running mean: -20.143941\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.142502\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.141077\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.149666\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.148169\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.146688\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.145221\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.133769\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.122431\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.131207\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.129895\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.138596\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.147210\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.155738\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.154180\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.162638\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.171012\n",
            "resetting env. episode 396.000000, reward total was -18.000000. running mean: -20.149302\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.137809\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -20.126431\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.135166\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.143815\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.142377\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.150953\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.159443\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.167849\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.176170\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.184409\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.182565\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.190739\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.198832\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.186843\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.184975\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.193125\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.201194\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.189182\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.177290\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.175517\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.183762\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.191924\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.190005\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.188105\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.196224\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.194262\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.192319\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.200396\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.208392\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.216308\n",
            "resetting env. episode 427.000000, reward total was -18.000000. running mean: -20.194145\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.192204\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.200282\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.198279\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.206296\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.214233\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.222091\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.219870\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.227671\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.235394\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.243040\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.240610\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.228204\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.235922\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.243563\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.231127\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.238816\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.226428\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.224163\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.221922\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.219702\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.217505\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.225330\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.233077\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.240746\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.238339\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.235955\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.233596\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.231260\n",
            "resetting env. episode 456.000000, reward total was -18.000000. running mean: -20.208947\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.196858\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.204889\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.192840\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.200912\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.208903\n",
            "resetting env. episode 462.000000, reward total was -18.000000. running mean: -20.186814\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.184946\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.183096\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.191265\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -20.179353\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -20.157559\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.145984\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.154524\n",
            "resetting env. episode 470.000000, reward total was -18.000000. running mean: -20.132978\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.131649\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.140332\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.148929\n",
            "resetting env. episode 474.000000, reward total was -17.000000. running mean: -20.117440\n",
            "resetting env. episode 475.000000, reward total was -17.000000. running mean: -20.086265\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.095403\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.104448\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.113404\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.112270\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.121147\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.129936\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.128636\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.137350\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.145977\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.144517\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.143072\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.151641\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.140125\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.148723\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.157236\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.165664\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.164007\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.172367\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.160643\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.169037\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.177346\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.185573\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.193717\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.181780\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.189962\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.198063\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.206082\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.214021\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.221881\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.219662\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.227466\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.235191\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.232839\n",
            "resetting env. episode 509.000000, reward total was -20.000000. running mean: -20.230511\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.228206\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.225923\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.233664\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.241328\n",
            "resetting env. episode 514.000000, reward total was -19.000000. running mean: -20.228914\n",
            "resetting env. episode 515.000000, reward total was -19.000000. running mean: -20.216625\n",
            "resetting env. episode 516.000000, reward total was -19.000000. running mean: -20.204459\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.212414\n",
            "resetting env. episode 518.000000, reward total was -19.000000. running mean: -20.200290\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.198287\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.206304\n",
            "resetting env. episode 521.000000, reward total was -19.000000. running mean: -20.194241\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.202299\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.210276\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.208173\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.216091\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.213931\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.221791\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.229573\n",
            "resetting env. episode 529.000000, reward total was -19.000000. running mean: -20.217278\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.225105\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.222854\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.230625\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.238319\n",
            "resetting env. episode 534.000000, reward total was -18.000000. running mean: -20.215936\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.213776\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.211639\n",
            "resetting env. episode 537.000000, reward total was -20.000000. running mean: -20.209522\n",
            "resetting env. episode 538.000000, reward total was -19.000000. running mean: -20.197427\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.195453\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.203498\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.211463\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.219349\n",
            "resetting env. episode 543.000000, reward total was -19.000000. running mean: -20.207155\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.215084\n",
            "resetting env. episode 545.000000, reward total was -18.000000. running mean: -20.192933\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.201003\n",
            "resetting env. episode 547.000000, reward total was -19.000000. running mean: -20.188993\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.197104\n",
            "resetting env. episode 549.000000, reward total was -19.000000. running mean: -20.185132\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.193281\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.191348\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.199435\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.197441\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.205466\n",
            "resetting env. episode 555.000000, reward total was -18.000000. running mean: -20.183411\n",
            "resetting env. episode 556.000000, reward total was -19.000000. running mean: -20.171577\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -20.169862\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.168163\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.176481\n",
            "resetting env. episode 560.000000, reward total was -19.000000. running mean: -20.164716\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.163069\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.171439\n",
            "resetting env. episode 563.000000, reward total was -19.000000. running mean: -20.159724\n",
            "resetting env. episode 564.000000, reward total was -19.000000. running mean: -20.148127\n",
            "resetting env. episode 565.000000, reward total was -18.000000. running mean: -20.126646\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.125379\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -20.124125\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.132884\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.141555\n",
            "resetting env. episode 570.000000, reward total was -20.000000. running mean: -20.140140\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -20.128738\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.137451\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.136077\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.144716\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.153269\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.151736\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.150219\n",
            "resetting env. episode 578.000000, reward total was -19.000000. running mean: -20.138716\n",
            "resetting env. episode 579.000000, reward total was -18.000000. running mean: -20.117329\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.116156\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -20.104994\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.113944\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.122805\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.131577\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.140261\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.138859\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.137470\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.146095\n",
            "resetting env. episode 589.000000, reward total was -19.000000. running mean: -20.134634\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -20.133288\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.141955\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.150536\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.159030\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.157440\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.165865\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.174207\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.182465\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.180640\n",
            "resetting env. episode 599.000000, reward total was -19.000000. running mean: -20.168834\n",
            "resetting env. episode 600.000000, reward total was -20.000000. running mean: -20.167145\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.165474\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.173819\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.172081\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -20.170360\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.178657\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.186870\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.195001\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.193051\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -20.181121\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.189310\n",
            "resetting env. episode 611.000000, reward total was -19.000000. running mean: -20.177416\n",
            "resetting env. episode 612.000000, reward total was -20.000000. running mean: -20.175642\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.173886\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.182147\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.180326\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.168522\n",
            "resetting env. episode 617.000000, reward total was -19.000000. running mean: -20.156837\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.155269\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.163716\n",
            "resetting env. episode 620.000000, reward total was -19.000000. running mean: -20.152079\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.160558\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.158952\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.167363\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.175689\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.173932\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.182193\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.180371\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.188567\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.196682\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.204715\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.212668\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -20.210541\n",
            "resetting env. episode 633.000000, reward total was -18.000000. running mean: -20.188436\n",
            "resetting env. episode 634.000000, reward total was -19.000000. running mean: -20.176551\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.184786\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.182938\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.181109\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.179298\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.187505\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.185630\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.193773\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.191836\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.189917\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.198018\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -20.186038\n",
            "resetting env. episode 646.000000, reward total was -18.000000. running mean: -20.164177\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.172536\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.180810\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -20.169002\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.177312\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.185539\n",
            "resetting env. episode 652.000000, reward total was -19.000000. running mean: -20.173684\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.171947\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.170227\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.178525\n",
            "resetting env. episode 656.000000, reward total was -19.000000. running mean: -20.166740\n",
            "resetting env. episode 657.000000, reward total was -17.000000. running mean: -20.135072\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -20.133722\n",
            "resetting env. episode 659.000000, reward total was -19.000000. running mean: -20.122384\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.121161\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.119949\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -20.108750\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.117662\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -20.116485\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.125321\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.134067\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.132727\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.141399\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.139985\n",
            "resetting env. episode 670.000000, reward total was -19.000000. running mean: -20.128586\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -20.117300\n",
            "resetting env. episode 672.000000, reward total was -18.000000. running mean: -20.096127\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.105165\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.104114\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.103073\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.102042\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.111022\n",
            "resetting env. episode 678.000000, reward total was -20.000000. running mean: -20.109911\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.118812\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.127624\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -20.126348\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.135084\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.143734\n",
            "resetting env. episode 684.000000, reward total was -18.000000. running mean: -20.122296\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.131073\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.129762\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.138465\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.147080\n",
            "resetting env. episode 689.000000, reward total was -19.000000. running mean: -20.135609\n",
            "resetting env. episode 690.000000, reward total was -18.000000. running mean: -20.114253\n",
            "resetting env. episode 691.000000, reward total was -18.000000. running mean: -20.093111\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.102180\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.111158\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -20.110046\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -20.108946\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.107856\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -20.106778\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.115710\n",
            "resetting env. episode 699.000000, reward total was -19.000000. running mean: -20.104553\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.103507\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.112472\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.111348\n",
            "resetting env. episode 703.000000, reward total was -19.000000. running mean: -20.100234\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.109232\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.108139\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.097058\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.096087\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.095127\n",
            "resetting env. episode 709.000000, reward total was -18.000000. running mean: -20.074175\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.083434\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.082599\n",
            "resetting env. episode 712.000000, reward total was -18.000000. running mean: -20.061773\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.071156\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.070444\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.079740\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.088942\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.088053\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.087172\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.096300\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -20.095337\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.104384\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.113340\n",
            "resetting env. episode 723.000000, reward total was -18.000000. running mean: -20.092207\n",
            "resetting env. episode 724.000000, reward total was -19.000000. running mean: -20.081285\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.090472\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.089567\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.098672\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.097685\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.106708\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.115641\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.124484\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.133240\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.141907\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.140488\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.149083\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.147592\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -20.146117\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.154655\n",
            "resetting env. episode 739.000000, reward total was -15.000000. running mean: -20.103109\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.112078\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -20.110957\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.109847\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.118749\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.117561\n",
            "resetting env. episode 745.000000, reward total was -19.000000. running mean: -20.106386\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.105322\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.114269\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -20.113126\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.121995\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.120775\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.129567\n",
            "resetting env. episode 752.000000, reward total was -19.000000. running mean: -20.118271\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.127089\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.125818\n",
            "resetting env. episode 755.000000, reward total was -18.000000. running mean: -20.104560\n",
            "resetting env. episode 756.000000, reward total was -19.000000. running mean: -20.093514\n",
            "resetting env. episode 757.000000, reward total was -19.000000. running mean: -20.082579\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.091753\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.090836\n",
            "resetting env. episode 760.000000, reward total was -19.000000. running mean: -20.079927\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -20.079128\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.078337\n",
            "resetting env. episode 763.000000, reward total was -18.000000. running mean: -20.057553\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.056978\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.066408\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.075744\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.084986\n",
            "resetting env. episode 768.000000, reward total was -18.000000. running mean: -20.064137\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -20.053495\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.052960\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.062431\n",
            "resetting env. episode 772.000000, reward total was -19.000000. running mean: -20.051806\n",
            "resetting env. episode 773.000000, reward total was -19.000000. running mean: -20.041288\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.050875\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.050367\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.049863\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.049364\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.058871\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.068282\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.067599\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.076923\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.086154\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.095292\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.104340\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.113296\n",
            "resetting env. episode 786.000000, reward total was -19.000000. running mean: -20.102163\n",
            "resetting env. episode 787.000000, reward total was -19.000000. running mean: -20.091142\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.100230\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -20.099228\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.098236\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.097253\n",
            "resetting env. episode 792.000000, reward total was -19.000000. running mean: -20.086281\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.095418\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.104464\n",
            "resetting env. episode 795.000000, reward total was -20.000000. running mean: -20.103419\n",
            "resetting env. episode 796.000000, reward total was -19.000000. running mean: -20.092385\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.091461\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -20.080546\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.089741\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.098844\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.097855\n",
            "resetting env. episode 802.000000, reward total was -19.000000. running mean: -20.086877\n",
            "resetting env. episode 803.000000, reward total was -19.000000. running mean: -20.076008\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.085248\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.094395\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.103451\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.112417\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.121293\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.120080\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.128879\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -20.137590\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.146214\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.154752\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.163204\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.171572\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -20.169857\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.178158\n",
            "resetting env. episode 818.000000, reward total was -19.000000. running mean: -20.166377\n",
            "resetting env. episode 819.000000, reward total was -19.000000. running mean: -20.154713\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.153166\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.161634\n",
            "resetting env. episode 822.000000, reward total was -19.000000. running mean: -20.150018\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -20.148518\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.157032\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.165462\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.173807\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.172069\n",
            "resetting env. episode 828.000000, reward total was -19.000000. running mean: -20.160349\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.168745\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.167058\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -20.165387\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.173733\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -20.161996\n",
            "resetting env. episode 834.000000, reward total was -19.000000. running mean: -20.150376\n",
            "resetting env. episode 835.000000, reward total was -18.000000. running mean: -20.128872\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.127583\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.136308\n",
            "resetting env. episode 838.000000, reward total was -20.000000. running mean: -20.134945\n",
            "resetting env. episode 839.000000, reward total was -19.000000. running mean: -20.123595\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.122359\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.131136\n",
            "resetting env. episode 842.000000, reward total was -18.000000. running mean: -20.109824\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.108726\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.117639\n",
            "resetting env. episode 845.000000, reward total was -19.000000. running mean: -20.106462\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.115398\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.114244\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.113101\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -20.111970\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.110851\n",
            "resetting env. episode 851.000000, reward total was -19.000000. running mean: -20.099742\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.098745\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -20.097757\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.096780\n",
            "resetting env. episode 855.000000, reward total was -17.000000. running mean: -20.065812\n",
            "resetting env. episode 856.000000, reward total was -19.000000. running mean: -20.055154\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.054602\n",
            "resetting env. episode 858.000000, reward total was -19.000000. running mean: -20.044056\n",
            "resetting env. episode 859.000000, reward total was -19.000000. running mean: -20.033616\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.043279\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.052847\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.052318\n",
            "resetting env. episode 863.000000, reward total was -18.000000. running mean: -20.031795\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.041477\n",
            "resetting env. episode 865.000000, reward total was -19.000000. running mean: -20.031062\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -20.030752\n",
            "resetting env. episode 867.000000, reward total was -19.000000. running mean: -20.020444\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.020240\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.030037\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.029737\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.039440\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.049045\n",
            "resetting env. episode 873.000000, reward total was -19.000000. running mean: -20.038555\n",
            "resetting env. episode 874.000000, reward total was -19.000000. running mean: -20.028169\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.037887\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.037509\n",
            "resetting env. episode 877.000000, reward total was -18.000000. running mean: -20.017134\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.026962\n",
            "resetting env. episode 879.000000, reward total was -18.000000. running mean: -20.006693\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.016626\n",
            "resetting env. episode 881.000000, reward total was -19.000000. running mean: -20.006459\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.016395\n",
            "resetting env. episode 883.000000, reward total was -19.000000. running mean: -20.006231\n",
            "resetting env. episode 884.000000, reward total was -19.000000. running mean: -19.996169\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -19.986207\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -19.996345\n",
            "resetting env. episode 887.000000, reward total was -19.000000. running mean: -19.986381\n",
            "resetting env. episode 888.000000, reward total was -19.000000. running mean: -19.976518\n",
            "resetting env. episode 889.000000, reward total was -18.000000. running mean: -19.956752\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -19.957185\n",
            "resetting env. episode 891.000000, reward total was -19.000000. running mean: -19.947613\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -19.958137\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -19.968555\n",
            "resetting env. episode 894.000000, reward total was -18.000000. running mean: -19.948870\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -19.939381\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -19.949987\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -19.960488\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -19.970883\n",
            "resetting env. episode 899.000000, reward total was -19.000000. running mean: -19.961174\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -19.971562\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -19.981846\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -19.972028\n",
            "resetting env. episode 903.000000, reward total was -18.000000. running mean: -19.952308\n",
            "resetting env. episode 904.000000, reward total was -18.000000. running mean: -19.932785\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -19.943457\n",
            "resetting env. episode 906.000000, reward total was -19.000000. running mean: -19.934022\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -19.934682\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -19.935335\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -19.945982\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -19.956522\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -19.956957\n",
            "resetting env. episode 912.000000, reward total was -19.000000. running mean: -19.947387\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -19.957913\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -19.968334\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -19.968651\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -19.978964\n",
            "resetting env. episode 917.000000, reward total was -19.000000. running mean: -19.969175\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -19.979483\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -19.979688\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -19.979891\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -19.980092\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -19.990291\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.000389\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.010385\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.010281\n",
            "resetting env. episode 926.000000, reward total was -18.000000. running mean: -19.990178\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -19.990276\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.000373\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.000370\n",
            "resetting env. episode 930.000000, reward total was -19.000000. running mean: -19.990366\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -19.990462\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -19.990558\n",
            "resetting env. episode 933.000000, reward total was -17.000000. running mean: -19.960652\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -19.951046\n",
            "resetting env. episode 935.000000, reward total was -19.000000. running mean: -19.941535\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -19.942120\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -19.942699\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -19.943272\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -19.943839\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -19.954401\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -19.954857\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -19.955308\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -19.945755\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -19.946297\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -19.956834\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -19.967266\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -19.977593\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -19.987817\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -19.997939\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -19.997960\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.007980\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.007900\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.017821\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.027643\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.027367\n",
            "resetting env. episode 956.000000, reward total was -19.000000. running mean: -20.017093\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.026922\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -20.026653\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.036386\n",
            "resetting env. episode 960.000000, reward total was -18.000000. running mean: -20.016023\n",
            "resetting env. episode 961.000000, reward total was -19.000000. running mean: -20.005862\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.015804\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.025646\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.035389\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.025035\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.034785\n",
            "resetting env. episode 967.000000, reward total was -18.000000. running mean: -20.014437\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.024293\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.034050\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.043709\n",
            "resetting env. episode 971.000000, reward total was -18.000000. running mean: -20.023272\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.033040\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -20.032709\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.032382\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.042058\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.051638\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.051121\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.060610\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.070004\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.079304\n",
            "resetting env. episode 981.000000, reward total was -19.000000. running mean: -20.068511\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.077826\n",
            "resetting env. episode 983.000000, reward total was -20.000000. running mean: -20.077048\n",
            "resetting env. episode 984.000000, reward total was -19.000000. running mean: -20.066277\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.075614\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.084858\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.084010\n",
            "resetting env. episode 988.000000, reward total was -19.000000. running mean: -20.073169\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.072438\n",
            "resetting env. episode 990.000000, reward total was -18.000000. running mean: -20.051713\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -20.041196\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.050784\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.060276\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.069674\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.078977\n",
            "resetting env. episode 996.000000, reward total was -18.000000. running mean: -20.058187\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.067605\n",
            "resetting env. episode 998.000000, reward total was -18.000000. running mean: -20.046929\n",
            "resetting env. episode 999.000000, reward total was -19.000000. running mean: -20.036460\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.036095\n",
            "resetting env. episode 1001.000000, reward total was -19.000000. running mean: -20.025734\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.025477\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.035222\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.024870\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.034621\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.044275\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -20.033832\n",
            "resetting env. episode 1008.000000, reward total was -18.000000. running mean: -20.013494\n",
            "resetting env. episode 1009.000000, reward total was -17.000000. running mean: -19.983359\n",
            "resetting env. episode 1010.000000, reward total was -19.000000. running mean: -19.973526\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -19.973790\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -19.984052\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -19.994212\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.004270\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.014227\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.014085\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.013944\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -20.003804\n",
            "resetting env. episode 1019.000000, reward total was -19.000000. running mean: -19.993766\n",
            "resetting env. episode 1020.000000, reward total was -18.000000. running mean: -19.973829\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -19.984090\n",
            "resetting env. episode 1022.000000, reward total was -19.000000. running mean: -19.974250\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -19.974507\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -19.984762\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -19.994914\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -19.994965\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -19.995016\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.005065\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.015015\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.014865\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.024716\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.034469\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.044124\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.043683\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.053246\n",
            "resetting env. episode 1036.000000, reward total was -19.000000. running mean: -20.042714\n",
            "resetting env. episode 1037.000000, reward total was -20.000000. running mean: -20.042286\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.041864\n",
            "resetting env. episode 1039.000000, reward total was -20.000000. running mean: -20.041445\n",
            "resetting env. episode 1040.000000, reward total was -19.000000. running mean: -20.031031\n",
            "resetting env. episode 1041.000000, reward total was -18.000000. running mean: -20.010720\n",
            "resetting env. episode 1042.000000, reward total was -19.000000. running mean: -20.000613\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.010607\n",
            "resetting env. episode 1044.000000, reward total was -19.000000. running mean: -20.000501\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.000496\n",
            "resetting env. episode 1046.000000, reward total was -18.000000. running mean: -19.980491\n",
            "resetting env. episode 1047.000000, reward total was -18.000000. running mean: -19.960686\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -19.971079\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -19.971368\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -19.981655\n",
            "resetting env. episode 1051.000000, reward total was -17.000000. running mean: -19.951838\n",
            "resetting env. episode 1052.000000, reward total was -19.000000. running mean: -19.942320\n",
            "resetting env. episode 1053.000000, reward total was -19.000000. running mean: -19.932896\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -19.933568\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -19.944232\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -19.944790\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -19.935342\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -19.945988\n",
            "resetting env. episode 1059.000000, reward total was -18.000000. running mean: -19.926528\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -19.927263\n",
            "resetting env. episode 1061.000000, reward total was -19.000000. running mean: -19.917990\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -19.918811\n",
            "resetting env. episode 1063.000000, reward total was -19.000000. running mean: -19.909622\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -19.920526\n",
            "resetting env. episode 1065.000000, reward total was -16.000000. running mean: -19.881321\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -19.892508\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -19.893583\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -19.894647\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -19.905700\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -19.916643\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -19.917477\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -19.928302\n",
            "resetting env. episode 1073.000000, reward total was -17.000000. running mean: -19.899019\n",
            "resetting env. episode 1074.000000, reward total was -19.000000. running mean: -19.890029\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -19.881129\n",
            "resetting env. episode 1076.000000, reward total was -20.000000. running mean: -19.882317\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -19.893494\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -19.904559\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -19.915514\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -19.926359\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -19.937095\n",
            "resetting env. episode 1082.000000, reward total was -18.000000. running mean: -19.917724\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -19.918547\n",
            "resetting env. episode 1084.000000, reward total was -18.000000. running mean: -19.899361\n",
            "resetting env. episode 1085.000000, reward total was -18.000000. running mean: -19.880368\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -19.891564\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -19.902648\n",
            "resetting env. episode 1088.000000, reward total was -18.000000. running mean: -19.883622\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -19.884786\n",
            "resetting env. episode 1090.000000, reward total was -18.000000. running mean: -19.865938\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -19.877278\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -19.878506\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -19.879721\n",
            "resetting env. episode 1094.000000, reward total was -18.000000. running mean: -19.860923\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -19.862314\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -19.873691\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -19.874954\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -19.876205\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -19.887442\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -19.888568\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -19.879682\n",
            "resetting env. episode 1102.000000, reward total was -20.000000. running mean: -19.880886\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -19.892077\n",
            "resetting env. episode 1104.000000, reward total was -19.000000. running mean: -19.883156\n",
            "resetting env. episode 1105.000000, reward total was -17.000000. running mean: -19.854324\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -19.865781\n",
            "resetting env. episode 1107.000000, reward total was -18.000000. running mean: -19.847123\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -19.858652\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -19.860066\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -19.861465\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -19.872850\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -19.884122\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -19.895281\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -19.906328\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -19.907264\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -19.908192\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -19.909110\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -19.920019\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -19.930819\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -19.941510\n",
            "resetting env. episode 1121.000000, reward total was -19.000000. running mean: -19.932095\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -19.932774\n",
            "resetting env. episode 1123.000000, reward total was -20.000000. running mean: -19.933447\n",
            "resetting env. episode 1124.000000, reward total was -19.000000. running mean: -19.924112\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -19.924871\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -19.925622\n",
            "resetting env. episode 1127.000000, reward total was -18.000000. running mean: -19.906366\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -19.907302\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -19.908229\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -19.899147\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -19.900156\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -19.911154\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -19.922043\n",
            "resetting env. episode 1134.000000, reward total was -18.000000. running mean: -19.902822\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -19.903794\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -19.894756\n",
            "resetting env. episode 1137.000000, reward total was -20.000000. running mean: -19.895808\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -19.906850\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -19.907782\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -19.908704\n",
            "resetting env. episode 1141.000000, reward total was -19.000000. running mean: -19.899617\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -19.890621\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -19.891715\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -19.892797\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -19.883869\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -19.895031\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -19.906080\n",
            "resetting env. episode 1148.000000, reward total was -17.000000. running mean: -19.877020\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -19.878249\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -19.889467\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -19.900572\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -19.901567\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -19.912551\n",
            "resetting env. episode 1154.000000, reward total was -20.000000. running mean: -19.913425\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -19.924291\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -19.925048\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -19.925798\n",
            "resetting env. episode 1158.000000, reward total was -18.000000. running mean: -19.906540\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -19.907474\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -19.908400\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -19.909316\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -19.920222\n",
            "resetting env. episode 1163.000000, reward total was -18.000000. running mean: -19.901020\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -19.892010\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -19.893090\n",
            "resetting env. episode 1166.000000, reward total was -18.000000. running mean: -19.874159\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -19.865417\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -19.866763\n",
            "resetting env. episode 1169.000000, reward total was -19.000000. running mean: -19.858096\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -19.869515\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -19.880820\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -19.892011\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.903091\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -19.914060\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -19.914920\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -19.925771\n",
            "resetting env. episode 1177.000000, reward total was -19.000000. running mean: -19.916513\n",
            "resetting env. episode 1178.000000, reward total was -19.000000. running mean: -19.907348\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -19.908274\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -19.919191\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -19.930000\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -19.930700\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -19.931393\n",
            "resetting env. episode 1184.000000, reward total was -20.000000. running mean: -19.932079\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -19.932758\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -19.943430\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -19.943996\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -19.944556\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -19.945110\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -19.945659\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -19.956203\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -19.966641\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -19.966974\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -19.977305\n",
            "resetting env. episode 1195.000000, reward total was -20.000000. running mean: -19.977532\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -19.977756\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -19.977979\n",
            "resetting env. episode 1198.000000, reward total was -19.000000. running mean: -19.968199\n",
            "resetting env. episode 1199.000000, reward total was -18.000000. running mean: -19.948517\n",
            "resetting env. episode 1200.000000, reward total was -19.000000. running mean: -19.939032\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -19.939641\n",
            "resetting env. episode 1202.000000, reward total was -19.000000. running mean: -19.930245\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -19.940943\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -19.951533\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -19.962018\n",
            "resetting env. episode 1206.000000, reward total was -18.000000. running mean: -19.942398\n",
            "resetting env. episode 1207.000000, reward total was -20.000000. running mean: -19.942974\n",
            "resetting env. episode 1208.000000, reward total was -19.000000. running mean: -19.933544\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -19.944208\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -19.954766\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -19.965219\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -19.965567\n",
            "resetting env. episode 1213.000000, reward total was -17.000000. running mean: -19.935911\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -19.936552\n",
            "resetting env. episode 1215.000000, reward total was -18.000000. running mean: -19.917186\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -19.918014\n",
            "resetting env. episode 1217.000000, reward total was -19.000000. running mean: -19.908834\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -19.919746\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -19.910548\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -19.921443\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -19.922228\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -19.923006\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -19.923776\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -19.934538\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -19.945193\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -19.955741\n",
            "resetting env. episode 1227.000000, reward total was -18.000000. running mean: -19.936184\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -19.946822\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -19.947354\n",
            "resetting env. episode 1230.000000, reward total was -19.000000. running mean: -19.937880\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -19.948501\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -19.949016\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.959526\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -19.969931\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -19.960232\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -19.970629\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -19.980923\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -19.991114\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.001203\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.001191\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -20.001179\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.011167\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -20.011055\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.010945\n",
            "resetting env. episode 1245.000000, reward total was -18.000000. running mean: -19.990835\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.000927\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -20.000918\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.010908\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -20.010799\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.010691\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.010584\n",
            "resetting env. episode 1252.000000, reward total was -19.000000. running mean: -20.000479\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.000474\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.010469\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.020364\n",
            "resetting env. episode 1256.000000, reward total was -19.000000. running mean: -20.010161\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.010059\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -19.999958\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -19.999959\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.009959\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -19.999860\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -19.999861\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -19.989863\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -19.989964\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -19.990064\n",
            "resetting env. episode 1266.000000, reward total was -19.000000. running mean: -19.980164\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -19.990362\n",
            "resetting env. episode 1268.000000, reward total was -19.000000. running mean: -19.980458\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -19.990654\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -19.990747\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -19.990840\n",
            "resetting env. episode 1272.000000, reward total was -18.000000. running mean: -19.970931\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -19.971222\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -19.981510\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -19.981695\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -19.981878\n",
            "resetting env. episode 1277.000000, reward total was -19.000000. running mean: -19.972059\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -19.972338\n",
            "resetting env. episode 1279.000000, reward total was -17.000000. running mean: -19.942615\n",
            "resetting env. episode 1280.000000, reward total was -17.000000. running mean: -19.913189\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -19.904057\n",
            "resetting env. episode 1282.000000, reward total was -17.000000. running mean: -19.875016\n",
            "resetting env. episode 1283.000000, reward total was -20.000000. running mean: -19.876266\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -19.887504\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -19.888629\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -19.889742\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -19.890845\n",
            "resetting env. episode 1288.000000, reward total was -19.000000. running mean: -19.881936\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -19.883117\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -19.894286\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -19.905343\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -19.906290\n",
            "resetting env. episode 1293.000000, reward total was -18.000000. running mean: -19.887227\n",
            "resetting env. episode 1294.000000, reward total was -18.000000. running mean: -19.868354\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -19.869671\n",
            "resetting env. episode 1296.000000, reward total was -20.000000. running mean: -19.870974\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -19.882264\n",
            "resetting env. episode 1298.000000, reward total was -17.000000. running mean: -19.853442\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -19.844907\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -19.846458\n",
            "resetting env. episode 1301.000000, reward total was -18.000000. running mean: -19.827994\n",
            "resetting env. episode 1302.000000, reward total was -19.000000. running mean: -19.819714\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -19.821517\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -19.813301\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -19.815168\n",
            "resetting env. episode 1306.000000, reward total was -19.000000. running mean: -19.807017\n",
            "resetting env. episode 1307.000000, reward total was -18.000000. running mean: -19.788947\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -19.791057\n",
            "resetting env. episode 1309.000000, reward total was -18.000000. running mean: -19.773147\n",
            "resetting env. episode 1310.000000, reward total was -19.000000. running mean: -19.765415\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -19.777761\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -19.769983\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -19.762283\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -19.774661\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -19.776914\n",
            "resetting env. episode 1316.000000, reward total was -19.000000. running mean: -19.769145\n",
            "resetting env. episode 1317.000000, reward total was -18.000000. running mean: -19.751453\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -19.763939\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -19.776300\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -19.788537\n",
            "resetting env. episode 1321.000000, reward total was -18.000000. running mean: -19.770651\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -19.772945\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -19.775215\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -19.777463\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -19.779688\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -19.791892\n",
            "resetting env. episode 1327.000000, reward total was -18.000000. running mean: -19.773973\n",
            "resetting env. episode 1328.000000, reward total was -19.000000. running mean: -19.766233\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -19.768571\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -19.780885\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -19.783076\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -19.795245\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -19.807293\n",
            "resetting env. episode 1334.000000, reward total was -18.000000. running mean: -19.789220\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -19.801328\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -19.803314\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -19.805281\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -19.817228\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -19.819056\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -19.830866\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -19.822557\n",
            "resetting env. episode 1342.000000, reward total was -19.000000. running mean: -19.814331\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -19.826188\n",
            "resetting env. episode 1344.000000, reward total was -17.000000. running mean: -19.797926\n",
            "resetting env. episode 1345.000000, reward total was -19.000000. running mean: -19.789947\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -19.802047\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -19.804027\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -19.795987\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -19.798027\n",
            "resetting env. episode 1350.000000, reward total was -20.000000. running mean: -19.800047\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -19.802046\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -19.804026\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -19.815985\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -19.827826\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -19.829547\n",
            "resetting env. episode 1356.000000, reward total was -19.000000. running mean: -19.821252\n",
            "resetting env. episode 1357.000000, reward total was -19.000000. running mean: -19.813039\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -19.824909\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -19.826660\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -19.828393\n",
            "resetting env. episode 1361.000000, reward total was -17.000000. running mean: -19.800109\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -19.802108\n",
            "resetting env. episode 1363.000000, reward total was -18.000000. running mean: -19.784087\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -19.776246\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -19.768484\n",
            "resetting env. episode 1366.000000, reward total was -18.000000. running mean: -19.750799\n",
            "resetting env. episode 1367.000000, reward total was -18.000000. running mean: -19.733291\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -19.735958\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -19.748598\n",
            "resetting env. episode 1370.000000, reward total was -19.000000. running mean: -19.741112\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -19.743701\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -19.756264\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -19.768702\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -19.771015\n",
            "resetting env. episode 1375.000000, reward total was -19.000000. running mean: -19.763305\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -19.765671\n",
            "resetting env. episode 1377.000000, reward total was -19.000000. running mean: -19.758015\n",
            "resetting env. episode 1378.000000, reward total was -19.000000. running mean: -19.750435\n",
            "resetting env. episode 1379.000000, reward total was -18.000000. running mean: -19.732930\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -19.745601\n",
            "resetting env. episode 1381.000000, reward total was -21.000000. running mean: -19.758145\n",
            "resetting env. episode 1382.000000, reward total was -17.000000. running mean: -19.730564\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -19.723258\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -19.726025\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -19.738765\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -19.751377\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -19.753864\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -19.756325\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -19.758762\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -19.771174\n",
            "resetting env. episode 1391.000000, reward total was -18.000000. running mean: -19.753462\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -19.755928\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -19.768368\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -19.770685\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -19.782978\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -19.795148\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -19.807197\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -19.819125\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -19.820933\n",
            "resetting env. episode 1400.000000, reward total was -18.000000. running mean: -19.802724\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -19.804697\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -19.816650\n",
            "resetting env. episode 1403.000000, reward total was -19.000000. running mean: -19.808483\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -19.810399\n",
            "resetting env. episode 1405.000000, reward total was -19.000000. running mean: -19.802295\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -19.814272\n",
            "resetting env. episode 1407.000000, reward total was -18.000000. running mean: -19.796129\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -19.798168\n",
            "resetting env. episode 1409.000000, reward total was -19.000000. running mean: -19.790186\n",
            "resetting env. episode 1410.000000, reward total was -19.000000. running mean: -19.782284\n",
            "resetting env. episode 1411.000000, reward total was -18.000000. running mean: -19.764461\n",
            "resetting env. episode 1412.000000, reward total was -19.000000. running mean: -19.756817\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -19.759248\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -19.771656\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -19.773939\n",
            "resetting env. episode 1416.000000, reward total was -20.000000. running mean: -19.776200\n",
            "resetting env. episode 1417.000000, reward total was -19.000000. running mean: -19.768438\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -19.770754\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -19.783046\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -19.785216\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -19.797364\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -19.809390\n",
            "resetting env. episode 1423.000000, reward total was -16.000000. running mean: -19.771296\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -19.773583\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -19.785847\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -19.787989\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -19.800109\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -19.812108\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -19.823987\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -19.835747\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -19.847389\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -19.848915\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -19.850426\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -19.851922\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -19.853403\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -19.854869\n",
            "resetting env. episode 1437.000000, reward total was -19.000000. running mean: -19.846320\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -19.857857\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -19.859278\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -19.860686\n",
            "resetting env. episode 1441.000000, reward total was -19.000000. running mean: -19.852079\n",
            "resetting env. episode 1442.000000, reward total was -15.000000. running mean: -19.803558\n",
            "resetting env. episode 1443.000000, reward total was -19.000000. running mean: -19.795522\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -19.787567\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -19.789691\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -19.801795\n",
            "resetting env. episode 1447.000000, reward total was -19.000000. running mean: -19.793777\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -19.785839\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -19.787980\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -19.790101\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -19.802200\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -19.814178\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -19.826036\n",
            "resetting env. episode 1454.000000, reward total was -18.000000. running mean: -19.807775\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -19.819698\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -19.831501\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -19.833186\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -19.844854\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -19.856405\n",
            "resetting env. episode 1460.000000, reward total was -18.000000. running mean: -19.837841\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -19.839463\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -19.851068\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -19.862558\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -19.863932\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -19.855293\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -19.856740\n",
            "resetting env. episode 1467.000000, reward total was -19.000000. running mean: -19.848172\n",
            "resetting env. episode 1468.000000, reward total was -19.000000. running mean: -19.839691\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -19.841294\n",
            "resetting env. episode 1470.000000, reward total was -19.000000. running mean: -19.832881\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -19.844552\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -19.856106\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -19.867545\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -19.878870\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -19.880081\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -19.891280\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -19.882368\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.893544\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -19.904608\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -19.915562\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -19.916407\n",
            "resetting env. episode 1482.000000, reward total was -19.000000. running mean: -19.907243\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -19.908170\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -19.919089\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -19.929898\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -19.940599\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -19.951193\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -19.961681\n",
            "resetting env. episode 1489.000000, reward total was -19.000000. running mean: -19.952064\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -19.962543\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -19.972918\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -19.983189\n",
            "resetting env. episode 1493.000000, reward total was -19.000000. running mean: -19.973357\n",
            "resetting env. episode 1494.000000, reward total was -19.000000. running mean: -19.963623\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -19.973987\n",
            "resetting env. episode 1496.000000, reward total was -19.000000. running mean: -19.964247\n",
            "resetting env. episode 1497.000000, reward total was -19.000000. running mean: -19.954605\n",
            "resetting env. episode 1498.000000, reward total was -19.000000. running mean: -19.945059\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -19.955608\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -19.966052\n",
            "CPU times: user 2h 45min 22s, sys: 42min 45s, total: 3h 28min 7s\n",
            "Wall time: 1h 47min 50s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "w2NblmwDsL3y",
        "outputId": "1884286d-2750-4298-8927-45c7a9d491fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Iteration-5_h=600_1e_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}